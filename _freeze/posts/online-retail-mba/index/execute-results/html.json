{
  "hash": "5aae0cb26fcd3446c771d1d1f17634cf",
  "result": {
    "markdown": "---\ntitle: \"\\U0001F6D2 Sales & Market Basket Analysis\"\nsubtitle: Bundled deals for an untapped market\nabstract: 'Through the analysis of one year of sales data from an e-commerce website, I was able to identify for the firm a promising market with untapped potential, and implement a marketing strategy to target it using bundle deals tailored to the specific preferences of its consumers.'\nauthor: Antonio Buzzelli\ndate: '2023-04-02'\nimage: ../../_freeze/posts/online-retail-mba/index/figure-html/cell-30-output-1.png\ncategories:\n  - Online Retail\n  - Market Basket Analysis\n  - Python\n---\n\nThe e-commerce industry has experienced significant growth in recent years, and online sales have become an increasingly important aspect of many businesses. Analyzing sales data can help businesses understand customer behavior and identify trends, which can then be used to improve their overall sales strategies and revenue. In this notebook, we will be analyzing a sales dataset from an e-commerce company to gain insights into their sales patterns and identify profitable opportunities.\n\nOur analysis will cover various aspects of the data, including temporal trends and customer geographical segmentation. We will also be performing a market basket analysis to identify relationships between products and suggest strategies for improving sales. By the end of this notebook, we aim to provide a comprehensive understanding of the sales data, which can then be used to make informed decisions and drive business growth.\n\n# Key findings and achievements\n\n* Even though the majority of the volume of sales is concentrated in the UK, the most performing region in terms of average revenue is Asia. The ANOVA analysis shows that the **mean purchase value in the Asia/Pacific region is consistently and significantly higher** than the mean purchase value in the other regions. We can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales.\n* By conducting a **market basket analysis** with a focus on the Asian market, we have identified groups of products that are commonly bought together. This has helped us uncover the specific preferences and purchasing patterns of this region. The firm could use this information to create bundled offers that combine these item sets and boost sales volume in the Asian market, ultimately leading to an increase in revenue.\n\n___\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\n# dataviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from jupyterthemes import jtplot\n# jtplot.style(theme='monokai', context='notebook', grid=False)\n\n# hypothesis testing\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# market basket analysis\nfrom itertools import permutations\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom pandas.plotting import parallel_coordinates\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n```\n:::\n\n\n# Data\n\nIn this first part of the notebook we will import the data and prepare it for analysis.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('../../data/sales.csv')\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 536350 entries, 0 to 536349\nData columns (total 8 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   TransactionNo  536350 non-null  object \n 1   Date           536350 non-null  object \n 2   ProductNo      536350 non-null  object \n 3   ProductName    536350 non-null  object \n 4   Price          536350 non-null  float64\n 5   Quantity       536350 non-null  int64  \n 6   CustomerNo     536295 non-null  float64\n 7   Country        536350 non-null  object \ndtypes: float64(2), int64(1), object(5)\nmemory usage: 32.7+ MB\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionNo</th>\n      <th>Date</th>\n      <th>ProductNo</th>\n      <th>ProductName</th>\n      <th>Price</th>\n      <th>Quantity</th>\n      <th>CustomerNo</th>\n      <th>Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>581482</td>\n      <td>12/9/2019</td>\n      <td>22485</td>\n      <td>Set Of 2 Wooden Market Crates</td>\n      <td>21.47</td>\n      <td>12</td>\n      <td>17490.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>581475</td>\n      <td>12/9/2019</td>\n      <td>22596</td>\n      <td>Christmas Star Wish List Chalkboard</td>\n      <td>10.65</td>\n      <td>36</td>\n      <td>13069.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>581475</td>\n      <td>12/9/2019</td>\n      <td>23235</td>\n      <td>Storage Tin Vintage Leaf</td>\n      <td>11.53</td>\n      <td>12</td>\n      <td>13069.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>581475</td>\n      <td>12/9/2019</td>\n      <td>23272</td>\n      <td>Tree T-Light Holder Willie Winkie</td>\n      <td>10.65</td>\n      <td>12</td>\n      <td>13069.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>581475</td>\n      <td>12/9/2019</td>\n      <td>23239</td>\n      <td>Set Of 4 Knick Knack Tins Poppies</td>\n      <td>11.94</td>\n      <td>6</td>\n      <td>13069.0</td>\n      <td>United Kingdom</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>536345</th>\n      <td>C536548</td>\n      <td>12/1/2018</td>\n      <td>22168</td>\n      <td>Organiser Wood Antique White</td>\n      <td>18.96</td>\n      <td>-2</td>\n      <td>12472.0</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>536346</th>\n      <td>C536548</td>\n      <td>12/1/2018</td>\n      <td>21218</td>\n      <td>Red Spotty Biscuit Tin</td>\n      <td>14.09</td>\n      <td>-3</td>\n      <td>12472.0</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>536347</th>\n      <td>C536548</td>\n      <td>12/1/2018</td>\n      <td>20957</td>\n      <td>Porcelain Hanging Bell Small</td>\n      <td>11.74</td>\n      <td>-1</td>\n      <td>12472.0</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>536348</th>\n      <td>C536548</td>\n      <td>12/1/2018</td>\n      <td>22580</td>\n      <td>Advent Calendar Gingham Sack</td>\n      <td>16.35</td>\n      <td>-4</td>\n      <td>12472.0</td>\n      <td>Germany</td>\n    </tr>\n    <tr>\n      <th>536349</th>\n      <td>C536548</td>\n      <td>12/1/2018</td>\n      <td>22767</td>\n      <td>Triple Photo Frame Cornice</td>\n      <td>20.45</td>\n      <td>-2</td>\n      <td>12472.0</td>\n      <td>Germany</td>\n    </tr>\n  </tbody>\n</table>\n<p>536350 rows × 8 columns</p>\n</div>\n```\n:::\n:::\n\n\nHere above, a glimpse of the data at our disposal. The dataset is composed by the following original variables:\n\n* TransactionNo (categorical): a six-digit unique number that defines each transaction. The letter “C” in the code indicates a cancellation.\n* Date (numeric): the date when each transaction was generated.\n* ProductNo (categorical): a five or six-digit unique character used to identify a specific product.\n* Product (categorical): product/item name.\n* Price (numeric): the price of each product per unit in pound sterling (£).\n* Quantity (numeric): the quantity of each product per transaction. Negative values related to cancelled transactions.\n* CustomerNo (categorical): a five-digit unique number that defines each customer.\n* Country (categorical): name of the country where the customer resides.\n\nHere below, we perform some operations to validate the type of variable, create an `Amount` column, and check the presence of missing values.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Validating variable types\ndf['Date'] = pd.to_datetime(df['Date'])\ndf[['ProductNo', 'CustomerNo']] = df[['ProductNo', 'CustomerNo']].astype('object')\n\n# Splitting `Date` column\ndf['Month'] = df['Date'].dt.month\ndf['Weekday'] = df['Date'].dt.weekday\ndf['WeekdayName'] = df['Date'].dt.day_name()\ndf['Day'] = df['Date'].dt.day\n\n# Creating a `TotalPrice` colum\ndf['Amount'] = df['Quantity'] * df['Price']\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nCustomerNo       0.000103\nTransactionNo    0.000000\nDate             0.000000\nProductNo        0.000000\nProductName      0.000000\nPrice            0.000000\nQuantity         0.000000\nCountry          0.000000\nMonth            0.000000\nWeekday          0.000000\nWeekdayName      0.000000\nDay              0.000000\nAmount           0.000000\ndtype: float64\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnulls = df[df['CustomerNo'].isnull()]\nnulls.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionNo</th>\n      <th>Date</th>\n      <th>ProductNo</th>\n      <th>ProductName</th>\n      <th>Price</th>\n      <th>Quantity</th>\n      <th>CustomerNo</th>\n      <th>Country</th>\n      <th>Month</th>\n      <th>Weekday</th>\n      <th>WeekdayName</th>\n      <th>Day</th>\n      <th>Amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6511</th>\n      <td>C581406</td>\n      <td>2019-12-08</td>\n      <td>46000M</td>\n      <td>Polyester Filler Pad 45x45cm</td>\n      <td>6.19</td>\n      <td>-240</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>12</td>\n      <td>6</td>\n      <td>Sunday</td>\n      <td>8</td>\n      <td>-1485.60</td>\n    </tr>\n    <tr>\n      <th>6512</th>\n      <td>C581406</td>\n      <td>2019-12-08</td>\n      <td>46000S</td>\n      <td>Polyester Filler Pad 40x40cm</td>\n      <td>6.19</td>\n      <td>-300</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>12</td>\n      <td>6</td>\n      <td>Sunday</td>\n      <td>8</td>\n      <td>-1857.00</td>\n    </tr>\n    <tr>\n      <th>90098</th>\n      <td>C575153</td>\n      <td>2019-11-08</td>\n      <td>22947</td>\n      <td>Wooden Advent Calendar Red</td>\n      <td>44.25</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>11</td>\n      <td>4</td>\n      <td>Friday</td>\n      <td>8</td>\n      <td>-44.25</td>\n    </tr>\n    <tr>\n      <th>102671</th>\n      <td>C574288</td>\n      <td>2019-11-03</td>\n      <td>22178</td>\n      <td>Victorian Glass Hanging T-Light</td>\n      <td>25.37</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>11</td>\n      <td>6</td>\n      <td>Sunday</td>\n      <td>3</td>\n      <td>-25.37</td>\n    </tr>\n    <tr>\n      <th>117263</th>\n      <td>C573180</td>\n      <td>2019-10-28</td>\n      <td>23048</td>\n      <td>Set Of 10 Lanterns Fairy Light Star</td>\n      <td>14.50</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>United Kingdom</td>\n      <td>10</td>\n      <td>0</td>\n      <td>Monday</td>\n      <td>28</td>\n      <td>-14.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe missing values in the dataset are only related to the `CustomerNo` column for a very small part that doesn't impact our analysis.\n\n# Sales analysis\n\nWe will now be analyzing online sales data from a one-year period spanning from 2018-12-01 to 2019-11-30. The first type of analysis will focus on the **temporal aspect** of the data. This analysis aims to understand the sales evolution over time, as well as identify trends within months and weeks. The second type of analysis will center around examining the **regional spread of sales** in order to evaluate the existing market segmentation and gain insights into potential opportunities.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Subsetting for one exact year\ndf = df[df['Date'] <= '2019-11-30']\n```\n:::\n\n\n## Monthly evolution\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmonth_evo = df.groupby(pd.Grouper(key='Date', freq='M')).agg(\n    sold=('Amount','sum'), returned=('Amount', lambda x: sum(x[x < 0])),\n    nunique=('TransactionNo', 'nunique'))\nmonth_evo['sold_moving_avg'] = month_evo['sold'].rolling(window=3).mean()\nmonth_evo['returned'] = month_evo['returned'].abs()\nmonth_evo.index = month_evo.index.date\nmonth_evo\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sold</th>\n      <th>returned</th>\n      <th>nunique</th>\n      <th>sold_moving_avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-12-31</th>\n      <td>4234147.48</td>\n      <td>181268.04</td>\n      <td>1852</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-01-31</th>\n      <td>3649506.42</td>\n      <td>910349.95</td>\n      <td>1327</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2019-02-28</th>\n      <td>3299537.56</td>\n      <td>35479.62</td>\n      <td>1287</td>\n      <td>3.727730e+06</td>\n    </tr>\n    <tr>\n      <th>2019-03-31</th>\n      <td>4353308.78</td>\n      <td>45092.82</td>\n      <td>1722</td>\n      <td>3.767451e+06</td>\n    </tr>\n    <tr>\n      <th>2019-04-30</th>\n      <td>3416109.24</td>\n      <td>173388.64</td>\n      <td>1455</td>\n      <td>3.689652e+06</td>\n    </tr>\n    <tr>\n      <th>2019-05-31</th>\n      <td>4530850.36</td>\n      <td>48114.72</td>\n      <td>1938</td>\n      <td>4.100089e+06</td>\n    </tr>\n    <tr>\n      <th>2019-06-30</th>\n      <td>4410422.29</td>\n      <td>84308.52</td>\n      <td>1826</td>\n      <td>4.119127e+06</td>\n    </tr>\n    <tr>\n      <th>2019-07-31</th>\n      <td>4518347.92</td>\n      <td>75519.14</td>\n      <td>1687</td>\n      <td>4.486540e+06</td>\n    </tr>\n    <tr>\n      <th>2019-08-31</th>\n      <td>4614243.55</td>\n      <td>144112.47</td>\n      <td>1581</td>\n      <td>4.514338e+06</td>\n    </tr>\n    <tr>\n      <th>2019-09-30</th>\n      <td>6542706.30</td>\n      <td>85596.76</td>\n      <td>2117</td>\n      <td>5.225099e+06</td>\n    </tr>\n    <tr>\n      <th>2019-10-31</th>\n      <td>6971407.82</td>\n      <td>266009.54</td>\n      <td>2312</td>\n      <td>6.042786e+06</td>\n    </tr>\n    <tr>\n      <th>2019-11-30</th>\n      <td>7745257.92</td>\n      <td>115939.20</td>\n      <td>3146</td>\n      <td>7.086457e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmonth_evo_sum = month_evo[['sold', 'returned']].sum(axis=1)\nmonth_evo_pct = month_evo[['sold', 'returned']].div(month_evo_sum, axis=0)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfig, ax = plt.subplots(2, 1, figsize=(15,10))\nmonth_evo[['sold', 'returned']].plot.bar(ax=ax[0])\nax[0].set_ylabel('Revenue (GBP)')\nax[0].set_xlabel('Month')\nax[0].set_title(\"Monthly evolution of sales and returns\")\nax[0].grid(axis='y')\n\nmonth_evo_pct.plot.bar(stacked=True, ax=ax[1])\nax[1].set_ylabel('Percentage')\nax[1].set_xlabel('Month')\nax[1].set_title(\"Monthly relative amounts of sold and returned\")\nax[1].grid(axis='y')\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=1184 height=883}\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax1.plot(month_evo.index, month_evo['sold'], label='Revenue')\nax1.plot(month_evo.index, month_evo['sold_moving_avg'], label='3-month revenue moving average')\nax2.bar(month_evo.index, month_evo['nunique'], width=8, label='Volume', alpha=0.25)\n\nax1.set_ylabel('Revenue (GBP)')\nax2.set_ylabel('Volume')\nax1.set_xlabel('Month')\nplt.title(\"Monthly evolution of sales\")\nplt.grid(True)\nax1.legend(loc=(0.025,0.85))\nax2.legend(loc=(0.3,0.85))\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=1233 height=449}\n:::\n:::\n\n\n> An **increased volume of sales and revenue** is clearly visible **during the last months of the year**, from September to December.\n\n## Intra-month analysis\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndf = df[df['Quantity'] > 0]\n\nbydate = df.groupby('Date').agg(\n    UniqueTransactions=('TransactionNo', 'nunique'),\n    UniqueProdSold=('TransactionNo', 'count'),\n    ProdSold=('Quantity', 'sum'),\n    Revenue=('Amount', 'sum')\n    ).reset_index()\nbydate['Day'] = bydate['Date'].dt.day\nbydate['Weekday'] = bydate['Date'].dt.weekday\nbydate['Month'] = bydate['Date'].dt.month\n\nbydate['WeekdayName'] = bydate['Date'].dt.day_name()\nbydate\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>UniqueTransactions</th>\n      <th>UniqueProdSold</th>\n      <th>ProdSold</th>\n      <th>Revenue</th>\n      <th>Day</th>\n      <th>Weekday</th>\n      <th>Month</th>\n      <th>WeekdayName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-12-01</td>\n      <td>127</td>\n      <td>3061</td>\n      <td>26889</td>\n      <td>326820.08</td>\n      <td>1</td>\n      <td>5</td>\n      <td>12</td>\n      <td>Saturday</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-12-02</td>\n      <td>141</td>\n      <td>2057</td>\n      <td>31297</td>\n      <td>367316.62</td>\n      <td>2</td>\n      <td>6</td>\n      <td>12</td>\n      <td>Sunday</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-12-03</td>\n      <td>68</td>\n      <td>2136</td>\n      <td>16164</td>\n      <td>206313.62</td>\n      <td>3</td>\n      <td>0</td>\n      <td>12</td>\n      <td>Monday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-12-05</td>\n      <td>88</td>\n      <td>2694</td>\n      <td>16357</td>\n      <td>197565.27</td>\n      <td>5</td>\n      <td>2</td>\n      <td>12</td>\n      <td>Wednesday</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-12-06</td>\n      <td>103</td>\n      <td>3823</td>\n      <td>21867</td>\n      <td>273420.10</td>\n      <td>6</td>\n      <td>3</td>\n      <td>12</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>2019-11-25</td>\n      <td>84</td>\n      <td>3073</td>\n      <td>31504</td>\n      <td>197883.43</td>\n      <td>25</td>\n      <td>0</td>\n      <td>11</td>\n      <td>Monday</td>\n    </tr>\n    <tr>\n      <th>293</th>\n      <td>2019-11-27</td>\n      <td>57</td>\n      <td>2529</td>\n      <td>11151</td>\n      <td>71166.89</td>\n      <td>27</td>\n      <td>2</td>\n      <td>11</td>\n      <td>Wednesday</td>\n    </tr>\n    <tr>\n      <th>294</th>\n      <td>2019-11-28</td>\n      <td>114</td>\n      <td>3293</td>\n      <td>29440</td>\n      <td>190534.34</td>\n      <td>28</td>\n      <td>3</td>\n      <td>11</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>2019-11-29</td>\n      <td>135</td>\n      <td>4275</td>\n      <td>30872</td>\n      <td>200962.48</td>\n      <td>29</td>\n      <td>4</td>\n      <td>11</td>\n      <td>Friday</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>2019-11-30</td>\n      <td>108</td>\n      <td>3337</td>\n      <td>28369</td>\n      <td>182968.22</td>\n      <td>30</td>\n      <td>5</td>\n      <td>11</td>\n      <td>Saturday</td>\n    </tr>\n  </tbody>\n</table>\n<p>297 rows × 9 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nbyday = bydate.groupby('Day')[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()\nbyday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']\nbyday = byday.sort_index()\nbyday.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DailyAvgUniqueTransactions</th>\n      <th>DailyAvgUniqueProdSold</th>\n      <th>DailyAvgProdSold</th>\n      <th>DailyAvgRev</th>\n    </tr>\n    <tr>\n      <th>Day</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>62.200</td>\n      <td>1425.600000</td>\n      <td>15170.000000</td>\n      <td>176053.439000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66.875</td>\n      <td>1685.625000</td>\n      <td>16904.750000</td>\n      <td>197402.811250</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>54.700</td>\n      <td>1410.100000</td>\n      <td>15707.800000</td>\n      <td>188742.753000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>61.400</td>\n      <td>1713.100000</td>\n      <td>19456.500000</td>\n      <td>226897.317000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>71.000</td>\n      <td>1815.111111</td>\n      <td>20756.666667</td>\n      <td>238868.772222</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nrev_coefficients = np.polyfit(byday.index.values, byday['DailyAvgRev'].values, 5)\nrev_regression_line = np.poly1d(rev_coefficients)\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax2.plot(byday.index, byday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)\nax1.bar(byday.index, byday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)\nax2.plot(rev_regression_line(byday.index.values), label='Regression line')\nax2.axhline(byday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Monthly average')\n\nax1.set_ylabel('N. transactions')\nax2.set_ylabel('Revenue (GBP)')\nplt.title(\"Intra-month sales analysis\")\nplt.grid(True)\nax1.legend(loc='upper left')\nax1.set_xlabel('Day')\nax2.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=1258 height=449}\n:::\n:::\n\n\n> By analyzing the revenue data within a month, we can observe that the daily average revenue varies throughout the month. The revenue reaches its peak at around three-quarters of the month and dips to its lowest point just before the end of the month. However, it starts to increase again just before the last few days. The dip in revenue just before the end of the month is considered normal as it coincides with the time when people typically receive their salaries.\n\n## Intra-week analysis\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nbyweekday = bydate.groupby(['Weekday', 'WeekdayName'])[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()\nbyweekday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']\nbyweekday = byweekday.reset_index().set_index('Weekday')\nbyweekday.index = byweekday.index + 1\nbyweekday\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WeekdayName</th>\n      <th>DailyAvgUniqueTransactions</th>\n      <th>DailyAvgUniqueProdSold</th>\n      <th>DailyAvgProdSold</th>\n      <th>DailyAvgRev</th>\n    </tr>\n    <tr>\n      <th>Weekday</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Monday</td>\n      <td>61.416667</td>\n      <td>1570.854167</td>\n      <td>16290.958333</td>\n      <td>187875.265208</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Wednesday</td>\n      <td>43.612245</td>\n      <td>1259.551020</td>\n      <td>9342.183673</td>\n      <td>107400.393061</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Thursday</td>\n      <td>64.217391</td>\n      <td>1900.000000</td>\n      <td>17979.391304</td>\n      <td>208483.595435</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Friday</td>\n      <td>66.705882</td>\n      <td>1879.764706</td>\n      <td>21235.274510</td>\n      <td>245216.874706</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Saturday</td>\n      <td>68.500000</td>\n      <td>1721.365385</td>\n      <td>18900.384615</td>\n      <td>210759.228077</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Sunday</td>\n      <td>77.843137</td>\n      <td>1820.686275</td>\n      <td>22464.431373</td>\n      <td>257149.160980</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nrev_coefficients = np.polyfit(byweekday.index.values, byweekday['DailyAvgRev'].values, 2)\nrev_regression_line = np.poly1d(rev_coefficients)\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax2.plot(byweekday['WeekdayName'], byweekday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)\nax1.bar(byweekday['WeekdayName'], byweekday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)\nax2.plot(rev_regression_line(byweekday.index.values), label='Regression line')\nax2.axhline(byweekday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Weekly average')\n\nax1.set_ylabel('N. transactions')\nax2.set_ylabel('Revenue(GBP)')\nplt.title(\"Intra-week sales analysis\")\nplt.grid(axis='y')\nax1.legend(loc='lower left')\nax1.set_xlabel('Weekday')\nax2.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=1258 height=449}\n:::\n:::\n\n\nSimilar to the analysis conducted within a month, examining sales patterns within a week can also reveal interesting insights.\n\n> By looking at the graph above, it becomes evident that the sales volume and revenue significantly increase during the latter part of the week. Specifically, revenue exceeds the weekly average starting from Thursday. On the other hand, Wednesday remains the least profitable day of the week with the lowest sales volume and revenue.\n\n## Geographical analysis\n\nWhen conducting a geographical analysis of sales, it is essential to consider both the average purchase value and sales volume to determine if there are any countries that offer promising opportunities. For instance, a country with a high average purchase value but low sales volume may indicate that it has untapped potential and should be targeted for further penetration. The average purchase value gives an indication of the buying power and willingness of customers to spend money, while sales volume reflects the market demand and potential for growth. A country with a high average purchase value and low sales volume could be a potential opportunity for businesses to capitalize on the untapped market potential by increasing their presence and promoting their products or services more effectively.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Mapping regions\nregions = {'Europe': ['Sweden', 'Denmark', 'Norway', 'Finland', 'Iceland', 'Netherlands', 'Belgium', 'France', 'Germany', 'Switzerland', 'Austria',\n                      'Italy', 'Spain', 'Greece', 'Portugal', 'Malta', 'Cyprus', 'Czech Republic', 'Lithuania', 'Poland', 'United Kingdom', 'EIRE',\n                      'Channel Islands', 'European Community'],\n           'North America': ['USA', 'Canada'],\n           'Middle East': ['Bahrain', 'United Arab Emirates', 'Israel', 'Lebanon', 'Saudi Arabia'],\n           'Asia Pacific': ['Japan', 'Australia', 'Singapore', 'Hong Kong'],\n           'RoW': ['Brazil', 'RSA'],\n           'Unspecified': ['Unspecified']}\n\ncountry_to_region = {}\nfor region, countries in regions.items():\n    for country in countries:\n        country_to_region[country] = region\n\ndf['Region'] = df['Country'].map(country_to_region)\n\ndf['UKvsRoW'] = np.where(df['Country'] == 'United Kingdom', 'UK', 'RoW')\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nbycountry = df.groupby('Country').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean')\n).sort_values('tot_amount', ascending=False)\nbycountry.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tot_amount</th>\n      <th>mean_amount</th>\n    </tr>\n    <tr>\n      <th>Country</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>United Kingdom</th>\n      <td>50192562.28</td>\n      <td>110.502027</td>\n    </tr>\n    <tr>\n      <th>Netherlands</th>\n      <td>2101104.07</td>\n      <td>937.992888</td>\n    </tr>\n    <tr>\n      <th>EIRE</th>\n      <td>1687318.68</td>\n      <td>225.607525</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>1346540.40</td>\n      <td>135.494103</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>1306661.68</td>\n      <td>129.564867</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfig, ax = plt.subplots(2, figsize=(15,10))\nax[0].bar(bycountry.index, bycountry['tot_amount'])\nax[1].bar(bycountry.sort_values('mean_amount', ascending=False).index, bycountry.sort_values('mean_amount', ascending=False)['mean_amount'])\nplt.setp(ax, xticks=bycountry.index, xticklabels=bycountry.index)\nplt.setp(ax[0].get_xticklabels(), rotation=90, ha=\"center\")\nplt.setp(ax[1].get_xticklabels(), rotation=90, ha=\"center\")\n\nax[0].set_ylabel(\"Amount (GBP)\")\nax[1].set_ylabel(\"Amount (GBP)\")\nax[0].set_title(\"Countries by total amount sold\")\nax[1].set_title(\"Countries by average amount sold\")\nplt.suptitle(\"Overview on geographical market spread\")\nax[0].grid(axis='y')\nax[1].grid(axis='y')\nplt.subplots_adjust(hspace=0.7)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=1197 height=1008}\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nbyukvsrow = df.groupby('UKvsRoW').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\nbyukvsrow\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tot_amount</th>\n      <th>mean_amount</th>\n      <th>n_inv</th>\n      <th>quantity</th>\n    </tr>\n    <tr>\n      <th>UKvsRoW</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>RoW</th>\n      <td>10258462.78</td>\n      <td>211.383944</td>\n      <td>1809</td>\n      <td>18.478900</td>\n    </tr>\n    <tr>\n      <th>UK</th>\n      <td>50192562.28</td>\n      <td>110.502027</td>\n      <td>17164</td>\n      <td>9.646163</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nplt.pie(byukvsrow['tot_amount'], labels=byukvsrow.index, autopct='%1.1f%%', explode=(0.1,0), shadow=True)\nplt.title('Total revenue by UK vs other countries')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=389 height=409}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nrow_rev = df.loc[df['UKvsRoW'] == 'RoW', 'Amount']\nuk_rev = df.loc[df['UKvsRoW'] == 'UK', 'Amount']\n\nttest_ind(uk_rev, row_rev)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nTtest_indResult(statistic=-16.7279858606087, pvalue=8.524026769071223e-63)\n```\n:::\n:::\n\n\n> Even though the volume of sales of international customers accounts only for the 17.0%, the **average revenue generated abroad is significantly higher than the one generated in the UK**. This means that international markets for this business are potentially more lucrative than the national one and need to be exploited more.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nbyregion = df.groupby('Region').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\nbyregion.sort_values('mean_amount', ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tot_amount</th>\n      <th>mean_amount</th>\n      <th>n_inv</th>\n      <th>quantity</th>\n    </tr>\n    <tr>\n      <th>Region</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Asia Pacific</th>\n      <td>1380079.80</td>\n      <td>590.787586</td>\n      <td>92</td>\n      <td>51.083904</td>\n    </tr>\n    <tr>\n      <th>North America</th>\n      <td>59633.28</td>\n      <td>154.891636</td>\n      <td>11</td>\n      <td>13.503896</td>\n    </tr>\n    <tr>\n      <th>Middle East</th>\n      <td>76798.30</td>\n      <td>154.835282</td>\n      <td>16</td>\n      <td>12.661290</td>\n    </tr>\n    <tr>\n      <th>Europe</th>\n      <td>58892902.53</td>\n      <td>118.021612</td>\n      <td>18839</td>\n      <td>10.308538</td>\n    </tr>\n    <tr>\n      <th>RoW</th>\n      <td>8912.10</td>\n      <td>101.273864</td>\n      <td>2</td>\n      <td>8.000000</td>\n    </tr>\n    <tr>\n      <th>Unspecified</th>\n      <td>32699.05</td>\n      <td>73.152237</td>\n      <td>13</td>\n      <td>6.272931</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nfig, ax1 = plt.subplots(figsize=(15,5))\nax1 = plt.bar(byregion.index, byregion['mean_amount'])\nplt.title(\"Average purchase value by region\")\nplt.ylabel('Amount (GBP)')\nplt.xlabel('Region')\nplt.grid(axis='y')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){width=1188 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nf_value, p_value = f_oneway(\n    df.loc[df['Region'] == 'Asia Pacific', 'Amount'],\n    df.loc[df['Region'] == 'North America', 'Amount'],\n    df.loc[df['Region'] == 'Middle East', 'Amount'],\n    df.loc[df['Region'] == 'Europe', 'Amount'],\n    df.loc[df['Region'] == 'RoW', 'Amount'])\nprint(f'ANOVA F-value: {f_value:.2f}')\nprint(f'ANOVA p-value: {p_value:.4f}')\ntukey_df = df.filter(items=['Amount', 'Region']).dropna()\nprint(pairwise_tukeyhsd(tukey_df['Amount'], tukey_df['Region']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA F-value: 81.58\nANOVA p-value: 0.0000\n          Multiple Comparison of Means - Tukey HSD, FWER=0.05          \n=======================================================================\n    group1        group2     meandiff p-adj    lower     upper   reject\n-----------------------------------------------------------------------\n Asia Pacific        Europe  -472.766    0.0 -547.3921 -398.1398   True\n Asia Pacific   Middle East -435.9523    0.0  -613.855 -258.0496   True\n Asia Pacific North America -435.8959    0.0 -633.8256 -237.9663   True\n Asia Pacific           RoW -489.5137 0.0048 -880.2663  -98.7612   True\n Asia Pacific   Unspecified -517.6353    0.0 -703.4071 -331.8636   True\n       Europe   Middle East   36.8137 0.9872  -124.841  198.4683  False\n       Europe North America     36.87 0.9928 -146.5938  220.3338  False\n       Europe           RoW  -16.7477    1.0 -400.3757  366.8802  False\n       Europe   Unspecified  -44.8694 0.9754 -215.1456  125.4068  False\n  Middle East North America    0.0564    1.0 -244.3599  244.4726  False\n  Middle East           RoW  -53.5614 0.9991 -469.7954  362.6726  False\n  Middle East   Unspecified   -81.683 0.9207 -316.3622  152.9961  False\nNorth America           RoW  -53.6178 0.9992 -478.7971  371.5616  False\nNorth America   Unspecified  -81.7394 0.9386 -331.9414  168.4626  False\n          RoW   Unspecified  -28.1216    1.0 -447.7792   391.536  False\n-----------------------------------------------------------------------\n```\n:::\n:::\n\n\n> We can observe from both the bar plot and the ANOVA analysis that the **mean purchase value in the Asia/Pacific region is consistently and significantly higher** than the mean purchase value in the other regions. Based on this important information, we can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales. The business can consider implementing targeted marketing strategies, such as advertising campaigns and promotions, that cater to the preferences and interests of the Asia/Pacific market. Additionally, it can explore expanding its product offerings to meet the specific demands of this region, or enhancing the quality of existing products to meet their higher standards. It may be useful to conduct further research and analysis to gain deeper insights into the preferences and behavior of customers in the Asia/Pacific region, and tailor sales strategies accordingly.\n\n# Market basket analysis for the Asian market\n\nMarket basket analysis, specifically Apriori and association rules, can provide valuable insights into customer behavior and preferences that can be used to develop effective marketing strategies. By analyzing customer purchase patterns and identifying which products are commonly purchased together, businesses can create product bundles and promotions that cater to specific customer segments. For instance, if the analysis reveals that customers who purchase Product A are highly likely to also purchase Product B, the business can create a bundle that includes both products at a discounted price to increase sales.\n\nThe Asia/Pacific region has a consistently higher average purchase value than other regions, indicating a potential opportunity to increase sales and revenue in that particular market. By conducting basket analysis on this region, the business can gain further insights into the specific product preferences and purchasing habits of customers in this market. This information can then be used to create targeted marketing strategies, such as promotions and advertising campaigns, that appeal to the unique needs and interests of customers in the Asia/Pacific region.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Subsetting for Asia/Pacific transactions\nasian_market = df[df['Region'] == 'Asia Pacific']\n\n# Converting transactions in a list of lists\ntransactions = asian_market.groupby('TransactionNo').apply(lambda x: list(x['ProductName'])).to_list()\nencoder = TransactionEncoder().fit(transactions)\nonehot = encoder.transform(transactions)\nonehot = pd.DataFrame(onehot, columns=encoder.columns_)\n\n# Selecting frequent itemsets with apriori algorythm\nfrequent_itemsets = apriori(onehot,\n                            min_support = 0.05, \n                            max_len = 5, \n                            use_colnames = True)\nprint('Number of itemsets selected by the Apriori algorithm:', len(frequent_itemsets))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of itemsets selected by the Apriori algorithm: 163\n```\n:::\n:::\n\n\nFirst, we subset the dataframe to filter for the transactions happened in the Asian market and encode them in binary features (one-hot encoding). Then, with the Apriori algorithm, we group them together according to a minimum support of 0.05 and we filter them according to a minimum confidence level of 1.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Computing association rules for the frequent itemsets, and filtering by confidence == 1\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=1)\n\n# Adding number of items in the itemsets\nrules['n_antecedents'] = rules['antecedents'].apply(lambda x: len(x))\nrules['n_consequents'] = rules['consequents'].apply(lambda x: len(x))\nrules.sample(15, random_state=42)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antecedents</th>\n      <th>consequents</th>\n      <th>antecedent support</th>\n      <th>consequent support</th>\n      <th>support</th>\n      <th>confidence</th>\n      <th>lift</th>\n      <th>leverage</th>\n      <th>conviction</th>\n      <th>n_antecedents</th>\n      <th>n_consequents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37</th>\n      <td>(Roses Regency Teacup And Saucer, Dolly Girl L...</td>\n      <td>(Spaceboy Lunch Box)</td>\n      <td>0.054348</td>\n      <td>0.108696</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>9.200000</td>\n      <td>0.048440</td>\n      <td>inf</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>(Roses Regency Teacup And Saucer, Dolly Girl L...</td>\n      <td>(Spaceboy Lunch Box)</td>\n      <td>0.054348</td>\n      <td>0.108696</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>9.200000</td>\n      <td>0.048440</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>(Roses Regency Teacup And Saucer, Spaceboy Lun...</td>\n      <td>(Dolly Girl Lunch Box)</td>\n      <td>0.054348</td>\n      <td>0.097826</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>10.222222</td>\n      <td>0.049031</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>(Roses Regency Teacup And Saucer, Regency Cake...</td>\n      <td>(Dolly Girl Lunch Box)</td>\n      <td>0.054348</td>\n      <td>0.097826</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>10.222222</td>\n      <td>0.049031</td>\n      <td>inf</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>(Roses Regency Teacup And Saucer, Dolly Girl L...</td>\n      <td>(Regency Cakestand 3 Tier)</td>\n      <td>0.054348</td>\n      <td>0.086957</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>11.500000</td>\n      <td>0.049622</td>\n      <td>inf</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>(Regency Cakestand 3 Tier, Spaceboy Lunch Box)</td>\n      <td>(Roses Regency Teacup And Saucer, Dolly Girl L...</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(Pink Polkadot Bowl)</td>\n      <td>(Blue Polkadot Bowl)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(Set Of 6 Snack Loaf Baking Cases)</td>\n      <td>(Set Of 6 Tea Time Baking Cases)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(Feltcraft Princess Lola Doll)</td>\n      <td>(Feltcraft Princess Olivia Doll)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Blue Happy Birthday Bunting)</td>\n      <td>(Pink Happy Birthday Bunting)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(Dolly Girl Lunch Box)</td>\n      <td>(Spaceboy Lunch Box)</td>\n      <td>0.097826</td>\n      <td>0.108696</td>\n      <td>0.097826</td>\n      <td>1.0</td>\n      <td>9.200000</td>\n      <td>0.087193</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>(Lunch Bag Dolly Girl Design, Set 3 Retrospot ...</td>\n      <td>(Red Spotty Biscuit Tin)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>(Roses Regency Teacup And Saucer, Regency Cake...</td>\n      <td>(Spaceboy Lunch Box)</td>\n      <td>0.054348</td>\n      <td>0.108696</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>9.200000</td>\n      <td>0.048440</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(Roses Regency Teacup And Saucer, Dolly Girl L...</td>\n      <td>(Regency Cakestand 3 Tier)</td>\n      <td>0.054348</td>\n      <td>0.086957</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>11.500000</td>\n      <td>0.049622</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(Lunch Bag Dolly Girl Design, Spaceboy Lunch Box)</td>\n      <td>(Dolly Girl Lunch Box)</td>\n      <td>0.065217</td>\n      <td>0.097826</td>\n      <td>0.065217</td>\n      <td>1.0</td>\n      <td>10.222222</td>\n      <td>0.058837</td>\n      <td>inf</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe result is a dataframe containing frequently sold itemsets with a set a metrics for market basket analysis. These MBA metrics are commonly used in association rule mining, a data mining technique used to identify relationships and patterns among items in a dataset. Here's a brief explanation of each metric:\n\n* **Antecedent support**: This refers to the proportion of transactions that contain the antecedent (or the \"if\" part of a rule). It is calculated as the number of transactions containing the antecedent divided by the total number of transactions.\n\n* **Consequent support**: This refers to the proportion of transactions that contain the consequent (or the \"then\" part of a rule). It is calculated as the number of transactions containing the consequent divided by the total number of transactions.\n\n* **Support**: This refers to the proportion of transactions that contain both the antecedent and the consequent. It is calculated as the number of transactions containing both the antecedent and the consequent divided by the total number of transactions.\n\n* **Confidence**: This measures the strength of the association between the antecedent and the consequent. It is calculated as the support of the antecedent and consequent divided by the support of the antecedent. Confidence can range from 0 to 1, with higher values indicating stronger associations.\n\n* **Lift**: This measures the degree to which the presence of the antecedent affects the likelihood of the consequent. It is calculated as the support of the antecedent and consequent divided by the product of the support of the antecedent and the support of the consequent. A lift value greater than 1 indicates a positive association between the antecedent and consequent, while a value less than 1 indicates a negative association.\n\n* **Leverage**: This measures the difference between the observed frequency of the antecedent and consequent co-occurring and the frequency expected if they were independent. It is calculated as the support of the antecedent and consequent minus the product of the support of the antecedent and the support of the consequent. A positive leverage value indicates a positive association between the antecedent and consequent, while a negative value indicates a negative association.\n\n* **Conviction**: This measures the degree of implication of the rule. It is calculated as the ratio of the support of the antecedent to the complement of the confidence. Conviction can range from 0 to infinity, with higher values indicating stronger implications.\n\n> Upon examining the frequent itemsets, it becomes evident that most of them consist of identical items that are often purchased together, with only minor variations such as color or pattern. For instance, transactions may include items like Blue Polkadot Bowls and Pink Polkadot Bowls, Dolly Girl Lunch Boxes and Spaceboy Lunch Boxes, or Feltcraft Princess Lola Dolls and Feltcraft Princess Olivia Dolls.\n\n\n## Bundle offers\n\nBased on the observation that these items are frequently bought together, it could be advantageous to offer them as bundles to customers. The firm could offer convenience and value to customers while potentially increasing sales and revenue. For example, a bundle might include both the Blue Polkadot Bowl and the Pink Polkadot Bowl, or the Dolly Girl Lunch Box and the Spaceboy Lunch Box. This strategy can be an effective way to meet Asian customers needs while boosting profits for the retailer.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Since we want to create bundle offers for single products, we filter for single items\nrules = rules[(rules['n_antecedents'] == 1) & (rules['n_consequents'] == 1)]\nrules.sort_values('support', ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>antecedents</th>\n      <th>consequents</th>\n      <th>antecedent support</th>\n      <th>consequent support</th>\n      <th>support</th>\n      <th>confidence</th>\n      <th>lift</th>\n      <th>leverage</th>\n      <th>conviction</th>\n      <th>n_antecedents</th>\n      <th>n_consequents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>(Dolly Girl Lunch Box)</td>\n      <td>(Spaceboy Lunch Box)</td>\n      <td>0.097826</td>\n      <td>0.108696</td>\n      <td>0.097826</td>\n      <td>1.0</td>\n      <td>9.200000</td>\n      <td>0.087193</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>(Alarm Clock Bakelike Green)</td>\n      <td>(Alarm Clock Bakelike Red)</td>\n      <td>0.065217</td>\n      <td>0.065217</td>\n      <td>0.065217</td>\n      <td>1.0</td>\n      <td>15.333333</td>\n      <td>0.060964</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(Alarm Clock Bakelike Red)</td>\n      <td>(Alarm Clock Bakelike Green)</td>\n      <td>0.065217</td>\n      <td>0.065217</td>\n      <td>0.065217</td>\n      <td>1.0</td>\n      <td>15.333333</td>\n      <td>0.060964</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(Basket Of Toadstools)</td>\n      <td>(Set 3 Retrospot Tea/Coffee/Sugar)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(Blue Happy Birthday Bunting)</td>\n      <td>(Pink Happy Birthday Bunting)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(Pink Polkadot Bowl)</td>\n      <td>(Blue Polkadot Bowl)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(Blue Polkadot Bowl)</td>\n      <td>(Pink Polkadot Bowl)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(Fairy Tale Cottage Night Light)</td>\n      <td>(Red Toadstool Led Night Light)</td>\n      <td>0.054348</td>\n      <td>0.119565</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>8.363636</td>\n      <td>0.047850</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(Feltcraft Princess Lola Doll)</td>\n      <td>(Feltcraft Princess Olivia Doll)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(Green Regency Teacup And Saucer)</td>\n      <td>(Roses Regency Teacup And Saucer)</td>\n      <td>0.054348</td>\n      <td>0.097826</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>10.222222</td>\n      <td>0.049031</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(Set Of 4 Knick Knack Tins Leaf)</td>\n      <td>(Set Of 4 Knick Knack Tins Doily)</td>\n      <td>0.054348</td>\n      <td>0.076087</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>13.142857</td>\n      <td>0.050213</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(Set Of 6 Tea Time Baking Cases)</td>\n      <td>(Set Of 6 Snack Loaf Baking Cases)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(Set Of 6 Snack Loaf Baking Cases)</td>\n      <td>(Set Of 6 Tea Time Baking Cases)</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>0.054348</td>\n      <td>1.0</td>\n      <td>18.400000</td>\n      <td>0.051394</td>\n      <td>inf</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nrules['antecedent'] = rules['antecedents'].apply(lambda x: list(x)[0])\nrules['consequent'] = rules['consequents'].apply(lambda x: list(x)[0])\nrules['rule'] = rules.index\n\ncoords = rules[['antecedent', 'consequent', 'rule']]\n\nparallel_coordinates(coords, 'rule', colormap='ocean')\nplt.title('Bundle offers for Asian / Pacific market')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-30-output-1.png){width=821 height=431}\n:::\n:::\n\n\n> The parallel coordinates plot visually highlights the bundles that were put together for the Asian market, and that the firm should offer on their e-commerce.\n\nOffering bundles of products that are already sold together as frequent itemsets can be an effective marketing strategy for several reasons:\n\n* Convenience: Bundling products that are frequently purchased together can provide customers with a convenient and streamlined shopping experience. Instead of having to search for each product individually, customers can purchase them together in a single transaction.\n\n* Value proposition: Bundling products can create a compelling value proposition for customers. By offering a discount or special deal on a bundle of products, customers may be more likely to make a purchase than if they were buying each item individually.\n\n* Increased sales: Bundling can also lead to increased sales by encouraging customers to purchase additional products that they may not have otherwise considered. For example, a customer who only intended to buy coffee may be enticed to buy a bundle that includes coffee, a mug, and a bag of coffee beans.\n\n* Upselling opportunities: Bundling can also provide opportunities for upselling by encouraging customers to purchase a higher-value bundle that includes additional products or features.\n\nIn summary, while some products may already be sold together as frequent itemsets, bundling can provide additional value and convenience for customers, as well as opportunities for increased sales and upselling. By offering bundles, businesses can differentiate themselves from competitors and create a more compelling value proposition for their customers.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}