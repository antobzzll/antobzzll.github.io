{
  "hash": "0b5ae7aeadb0f784da2d68642dd623d6",
  "result": {
    "markdown": "---\ntitle: \"\\U0001F6A2 Understanding Survival on the Titanic\"\nsubtitle: Using machine learning to explain phenomena and develop business personas\nabstract: 'Businesses can utilize machine learning not only to make predictions but also to unveil valuable insights that are deeply embedded in their data. Using one of the most commonly used datasets in the study of data science – the Titanic dataset –, I demonstrate how supervised learning can serve as a data mining technique to achieve a deep understanding of phenomena and generate Personas.'\nauthor: Antonio Buzzelli\ndate: '2023-03-15'\ncode-fold: true\ncategories:\n  - Business Personas\n  - Machine Learning\n---\n\nThe Titanic disaster remains one of the most infamous shipwrecks in history, with over 1500 lives lost. In this notebook, we will explore and analyze a dataset containing information on passengers who were aboard the ship and build a machine learning model to predict which passengers are likely to have survived the tragedy.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nimport shap\n```\n:::\n\n\n## Data\n\nAfter having installed and imported the necessary libraries, we begin our work by accessing the dataset containing the passengers' data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('titanic.csv')\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Montvila, Rev. Juozas</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211536</td>\n      <td>13.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Graham, Miss. Margaret Edith</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112053</td>\n      <td>30.0000</td>\n      <td>B42</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>W./C. 6607</td>\n      <td>23.4500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Behr, Mr. Karl Howell</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>111369</td>\n      <td>30.0000</td>\n      <td>C148</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Dooley, Mr. Patrick</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>370376</td>\n      <td>7.7500</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 12 columns</p>\n</div>\n```\n:::\n:::\n\n\nThe variables included in the dataset are:\n* **PassengerId**: An integer value representing the unique ID of each passenger on board.\n* **Survived**: A binary variable indicating whether the passenger survived the sinking of the Titanic or not, with \"True\" indicating survival and \"False\" indicating death.\n* **Pclass**: An integer variable representing the class of the passenger's ticket, with values 1, 2, or 3 (1 being the highest class).\n* **Name**: A string variable representing the name of the passenger.\n* **Sex**: A string variable representing the gender of the passenger.\n* **Age**: A float variable representing the age of the passenger in years.\n* **SibSp**: An integer variable representing the number of siblings or spouses the passenger had on board.\n* **Parch**: An integer variable representing the number of parents or children the passenger had on board.\n* **Ticket**: A string variable representing the ticket number of the passenger.\n* **Fare**: A float variable representing the fare paid by the passenger for their ticket.\n* **Cabin**: A string variable representing the cabin number of the passenger, if applicable.\n* **Embarked**: A string variable representing the port of embarkation for the passenger, with possible values \"S\" (Southampton), \"C\" (Cherbourg), or \"Q\" (Queenstown).\n\n<!-- ## Exploratory analysis\n\nIn order to predict the survival of Titanic's passengers, we will focus our work on the **target variable `Survived`**. Let's begin with an exploratory analysis of the dataset (focused on the target variable) that will provide insights on the need of potential data cleaning and preprocessing operations, and potential interesting correlations. -->\n\n## Transformations, feature engineering, preprocessing\n\nIn this first section we are going to explore and process the given dataset and prepare it for modeling. Let's first take a general look at the variables from the structural point-of-view.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n```\n:::\n:::\n\n\n`PassengerId` column is a useless variable in terms of prediction capabilities. We can set it as dataframe index (or drop it) to keep it out of the scope.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf = df.set_index('PassengerId')\n# df = df.drop(columns=['PassengerId'])\n```\n:::\n\n\nTo give more weight to first class (and vice-versa) `Pclass` values must be inverted.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nconditions = [df['Pclass'] == 1, df['Pclass'] == 3]\nchoices = [3, 1]\ndf['Pclass'] = np.select(conditions, choices, 2)\n```\n:::\n\n\nLet's assess now the presence of **null values**.\nPercentage of null values per variable:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndisplay((df.isnull().sum() / df.shape[0]).round(3).sort_values(ascending=False))\n```\n\n::: {.cell-output .cell-output-display}\n```\nCabin       0.771\nAge         0.199\nEmbarked    0.002\nSurvived    0.000\nPclass      0.000\nName        0.000\nSex         0.000\nSibSp       0.000\nParch       0.000\nTicket      0.000\nFare        0.000\ndtype: float64\n```\n:::\n:::\n\n\nA part from a negligeable presence of null values in the `Embarked` variable, we can see that important columns such as `Cabin` and `Age` are full of empty observations. Before proceeding with the model development, we'll have to deal with these inconsistencies.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf['Cabin'].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n       'C148'], dtype=object)\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndisplay(df['Cabin'].value_counts().head(30))\n```\n\n::: {.cell-output .cell-output-display}\n```\nCabin\nB96 B98            4\nG6                 4\nC23 C25 C27        4\nC22 C26            3\nF33                3\nF2                 3\nE101               3\nD                  3\nC78                2\nC93                2\nE8                 2\nD36                2\nB77                2\nC123               2\nE121               2\nE44                2\nD35                2\nC125               2\nE67                2\nB35                2\nB18                2\nE24                2\nB49                2\nC65                2\nB20                2\nB5                 2\nB57 B59 B63 B66    2\nC126               2\nB51 B53 B55        2\nF4                 2\nName: count, dtype: int64\n```\n:::\n:::\n\n\nThe majority of the cabins in the dataset seem to follow a pattern of a letter followed by a two or three digit number, suggesting that the letter represents the section or deck where the cabin is situated and the number represents the room number. It is reasonable to assume that knowing the section where a passenger's cabin is located would provide valuable information about their likelihood of survival. Therefore, it would be beneficial to clean up the column containing cabin information to extract useful insights.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntrans_df = df.copy()\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Splitting Cabin variable into Deck and Room variables\ntrans_df['CabinData'] = trans_df['Cabin'].isnull().apply(lambda x: not x)\ntrans_df['Deck'] = trans_df['Cabin'].str.slice(0,1)\ntrans_df['Room'] = trans_df['Cabin'].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ntrans_df[['Survived', 'CabinData']] = trans_df[['Survived', 'CabinData']].astype(int)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Label-encoding target variable\ntrans_df['Female'] = np.where(trans_df['Sex'] == 'female', 1, 0)\n\n# Drop useless variables for modeling\ntrans_df = trans_df.drop(columns=['Name', 'Ticket', 'Cabin', 'Sex', 'Room'])\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Create one-hot encoding of the categorical variable\ndf_encoded = pd.get_dummies(trans_df, columns=['Deck', 'Embarked'])\n\n# Impute missing values with KNN imputer\nimputer = KNNImputer(n_neighbors=8)\nimputed_df = imputer.fit_transform(df_encoded)\nimputed_df = pd.DataFrame(imputed_df, columns=df_encoded.columns)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nimputed_df\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>CabinData</th>\n      <th>Female</th>\n      <th>Deck_A</th>\n      <th>Deck_B</th>\n      <th>Deck_C</th>\n      <th>Deck_D</th>\n      <th>Deck_E</th>\n      <th>Deck_F</th>\n      <th>Deck_G</th>\n      <th>Deck_T</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>22.00</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>7.2500</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>38.00</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>71.2833</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>26.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.9250</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>35.00</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>53.1000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>35.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8.0500</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>27.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.0000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>19.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>30.0000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>23.75</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>23.4500</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>26.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>30.0000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>32.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.7500</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 19 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Exploratory analysis\n\nWe have transformed our dataframe by imputing missing values with a KNNImputer algorythm and one-hot encoded categorical variables. Let's now perform an exploratory analysis to look for potential correlations between our target variable and the others.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nax = sns.countplot(x=\"Pclass\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\nplt.title(\"Perished / Survived per `Pclass`\")\nplt.ylabel(\"Count\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=593 height=432}\n:::\n:::\n\n\n42% of perished people is coming from the third class.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nax = sns.countplot(x=\"Female\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1], ['Men', 'Women'])\nplt.title(\"Perished / Survived per sex\")\nplt.ylabel(\"Count\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=593 height=431}\n:::\n:::\n\n\nThe majority of perished people are men, while the majority of survived are women.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ncatplot = sns.catplot(data=imputed_df, y=\"Age\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Age` and `Pclass`', y=1.05)\nplt.show()\n\ncatplot = sns.catplot(data=imputed_df, y=\"Fare\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Fare` and `Pclass`', y=1.05)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){width=1023 height=490}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-2.png){width=1023 height=490}\n:::\n:::\n\n\nAlready from these exploratory visualizations, we can easily see that the majority of survived people are women and people from the top classes. Will this be confirmed by the model?\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nsurvived_corr = imputed_df.corr()['Survived'].sort_values(ascending=False)[1:]\ndisplay(survived_corr)\n```\n\n::: {.cell-output .cell-output-display}\n```\nFemale        0.543351\nPclass        0.338481\nCabinData     0.316912\nFare          0.257307\nDeck_B        0.175095\nEmbarked_C    0.168240\nDeck_D        0.150716\nDeck_E        0.145321\nDeck_C        0.114652\nParch         0.081629\nDeck_F        0.057935\nDeck_A        0.022287\nDeck_G        0.016040\nEmbarked_Q    0.003650\nDeck_T       -0.026456\nSibSp        -0.035322\nAge          -0.092717\nEmbarked_S   -0.155660\nName: Survived, dtype: float64\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfor i in range(0, len(survived_corr.index), 6):\n    chunk = survived_corr.index[i:i+6]\n    sns.pairplot(imputed_df, y_vars=['Survived'], x_vars=chunk, kind='reg')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=1420 height=241}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-2.png){width=1420 height=241}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-3.png){width=1420 height=241}\n:::\n:::\n\n\n## Model development\n\nWe begin by splitting the dataset into train and test sets and instantiate a KFold object for cross validation to avoid overfitting. Several trial and errors suggested to set the test size to 0.4, giving the best balance between cross validation score and test set accuracy score for the models.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Split the data into training and test sets\nmodel_df = imputed_df.drop(columns=['CabinData'])\nX = model_df.drop(columns=['Survived'])\ny = model_df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n\n# instantiate a KFold object for cross validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n```\n:::\n\n\n### Model selection\n\nThen we compare three classifiers and choose the best one in terms of accuracy.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Define sklearn pipelines for classifier comparison\n\nlogistic_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('logistic', LogisticRegression(max_iter=1000))\n])\n\nrf_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\nsvm_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\npipelines = [logistic_pipeline, rf_pipeline, svm_pipeline]\nfor pipeline in pipelines:\n    scores = cross_val_score(pipeline, X_train, y_train, cv=kf)\n    print(f\"Pipeline: {pipeline.named_steps.keys()} Mean Cross-Validation Score: {scores.mean().round(3)}\")\n    \n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    print(f\"Test accuracy score: {round(accuracy_score(y_pred, y_test), 3)}\")\n    \n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    cmd = ConfusionMatrixDisplay(cm)\n    fig, ax = plt.subplots()\n    cmd.plot(ax=ax, cmap=plt.cm.Blues)\n    ax.set_title(f\"Confusion Matrix\\n{pipeline[0]}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeline: dict_keys(['logistic']) Mean Cross-Validation Score: 0.792\nTest accuracy score: 0.821\nPipeline: dict_keys(['rf']) Mean Cross-Validation Score: 0.8\nTest accuracy score: 0.798\nPipeline: dict_keys(['svm']) Mean Cross-Validation Score: 0.695\nTest accuracy score: 0.65\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-2.png){width=504 height=467}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-3.png){width=504 height=467}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-4.png){width=504 height=467}\n:::\n:::\n\n\nAs we can see, the classification model with the highest accuracy score (mean cross-validation score) is the **Random Forest Classifier**. We are going to use this specific model to predict survival on the Titanic.\n\n### Prediction with Random Forest Classifier\n\nAfter having chosen the right classifier for the job, we train it on the train set with GridSearchCV, that performs an exhaustive search over a specified hyperparameter space for an estimator. It takes as input an estimator, a dictionary of hyperparameters, and a cross-validation strategy. It then fits the estimator on all possible combinations of hyperparameters and evaluates the performance of each combination using cross-validation.\n\nThe GridSearchCV class is useful for finding the best hyperparameters for a machine learning model. By testing all possible combinations of hyperparameters, it can help to identify the combination that results in the best performance on a given dataset.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nparam_grid = {\n    'rf__n_estimators': [10, 50, 100],\n    'rf__max_depth': [None, 5, 10],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid, cv=kf)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\ny_pred = grid_search.predict(X_test)\nprint(\"Test set accuracy:\", grid_search.score(X_test, y_test))\n\nestimator = grid_search.best_estimator_.named_steps['rf']\n\n # Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ncmd = ConfusionMatrixDisplay(cm)\nfig, ax = plt.subplots()\ncmd.plot(ax=ax, cmap=plt.cm.Blues)\nax.set_title(f\"Confusion Matrix\\n{estimator}\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters: {'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 50}\nBest score: 0.8294657026979368\nTest set accuracy: 0.8207282913165266\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){width=688 height=467}\n:::\n:::\n\n\nWe have taken the best performing algorithm and trained it on our dataset with a GridSearchCV and KFold cross validation. The perfomance on the test set suggests that the chosen classification model is able to predict Survival with an 82% of accuracy. As a final step, we are going to explain the variables which have been taken into consideration by the classifier in the prediction, and draw conclusions on the \"Survived Persona\".\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nshap_values = shap.TreeExplainer(estimator).shap_values(X_test)\nshap.summary_plot(shap_values[1], X_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-2.png){width=730 height=785}\n:::\n:::\n\n\nThe SHAP (SHapley Additive exPlanations) summary plot is a powerful tool for interpreting the output of a tree-based model, such as a Random Forest Classifier, using a TreeExplainer. The plot displays the features in order of their importance, as measured by their mean absolute SHAP values. Here's how to interpret the SHAP summary plot for a TreeExplainer:\n\n* Feature importance: The features are listed along the y-axis of the plot, with the most important features at the top. The importance of each feature is represented by its mean absolute SHAP value, which is indicated by the horizontal bars.\n\n* Impact on prediction: The color of the bars represents the direction and magnitude of the feature's impact on the model's prediction. Red bars indicate a positive impact on the prediction, while blue bars indicate a negative impact. The length of the bar represents the magnitude of the impact, with longer bars indicating a greater impact.\n\n* Relationship between feature and prediction: The position of each bar relative to the center line indicates the direction and strength of the relationship between the feature and the prediction. Features that push the prediction towards the top of the plot are associated with higher predicted probabilities, while features that push the prediction towards the bottom are associated with lower predicted probabilities.\n\n* Interactions between features: The spacing between the bars for each feature represents the degree of interaction between that feature and other features in the model. If the bars for two features are close together, it indicates that those features tend to interact with each other in the model.\n\n## Conclusion\n\nThe Random Forest Classifier trained on our dataset was capable of predicting Survival on the Titanic with a 82% of accuracy. The best predictors identified by the SHAP Tree Explainer are:\n* `Female`: whether the person was female or not. The red dots (representing female persons) are all gathered on the right side of the plot, meaning that they have a strong positive impact on survival.\n* `Pclass`: passenger class. 1st class passengers (red dots) are located in the right size of the graph. Being a 1st class passenger would have meant sure survival.\n* `Age`: older persons, represented by red dots are all gathered in the left (death) side of the plot, inversely, younger people were more prone to be saved.\n* `Fare`: similar situation for fare. Higher fares (red dots) are all located in the right side, meaning that people who paid for more expensive tickets had better chances to survive.\n\nAn interesting insights that SHAP gives us is regarding the `Embarked_S`. This dummy variable tells us if the passenger was embarked in Southampton. We can see that these passengers (red dots) had practically no chance of being saved. Maybe because they were all 3rd class?\n\nTo conclude, we can say that the **Survived Persona** was a young girl, coming from the top classes (having paid a higher fare) and not embarked in Southampton.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}