[
  {
    "objectID": "posts/loan_approval/index.html",
    "href": "posts/loan_approval/index.html",
    "title": "üôè Predicting loan approval",
    "section": "",
    "text": "Compared three classification models to predict loan approval (Logistic Regression, Decision Tree, Random Forest).\nSelected the best performing model and carried out predictions on test set.\nInterpreted the prediction by looking at feature importance."
  },
  {
    "objectID": "posts/loan_approval/index.html#key-findings-and-achievements",
    "href": "posts/loan_approval/index.html#key-findings-and-achievements",
    "title": "üôè Predicting loan approval",
    "section": "",
    "text": "Compared three classification models to predict loan approval (Logistic Regression, Decision Tree, Random Forest).\nSelected the best performing model and carried out predictions on test set.\nInterpreted the prediction by looking at feature importance."
  },
  {
    "objectID": "posts/loan_approval/index.html#introduction",
    "href": "posts/loan_approval/index.html#introduction",
    "title": "üôè Predicting loan approval",
    "section": "Introduction",
    "text": "Introduction\nObtaining loans for various purposes has become an integral part of our lives, whether it‚Äôs financing a dream home, pursuing higher education, or launching a business venture. However, the loan approval process can be complex, time-consuming, and sometimes uncertain for both borrowers and lenders. This short notebook aims to explore the world of predictive modeling in the domain of finance, specifically focusing on the task of predicting loan approval.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.float_format = '{:,.2f}'.format"
  },
  {
    "objectID": "posts/loan_approval/index.html#data",
    "href": "posts/loan_approval/index.html#data",
    "title": "üôè Predicting loan approval",
    "section": "Data",
    "text": "Data\n\n\nCode\ndf = pd.read_csv('../../data/loan_approval_dataset.csv')\ndf.columns = [col.strip() for col in df.columns]\ndf = df.drop(columns=['loan_id'])\n\n# Label-encoding categorical variables\ndf['loan_status'] = df['loan_status'].map(\n    {\n        ' Approved': 1,\n        ' Rejected': 0\n    }\n)\n\ndf['education'] = df['education'].map(\n    {\n        ' Graduate': 1,\n        ' Not Graduate': 0\n    }\n)\n\ndf['self_employed'] = df['self_employed'].map(\n    {\n        ' Yes': 1,\n        ' No': 0\n    }\n)\n\n# Given that the dataset's financial values are expressed in INR (Indian rupee), \n# let's convert them to EUR (Euro) for easier interpretation of the amounts.\ndf[\n    ['income_annum', 'loan_amount', 'residential_assets_value', \n     'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']] = df[\n    ['income_annum', 'loan_amount', 'residential_assets_value', \n     'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']] * 0.01115\n     \n     \nprint(df.info())\ndisplay(df)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4269 entries, 0 to 4268\nData columns (total 12 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   no_of_dependents          4269 non-null   int64  \n 1   education                 4269 non-null   int64  \n 2   self_employed             4269 non-null   int64  \n 3   income_annum              4269 non-null   float64\n 4   loan_amount               4269 non-null   float64\n 5   loan_term                 4269 non-null   int64  \n 6   cibil_score               4269 non-null   int64  \n 7   residential_assets_value  4269 non-null   float64\n 8   commercial_assets_value   4269 non-null   float64\n 9   luxury_assets_value       4269 non-null   float64\n 10  bank_asset_value          4269 non-null   float64\n 11  loan_status               4269 non-null   int64  \ndtypes: float64(6), int64(6)\nmemory usage: 400.3 KB\nNone\n\n\n\n\n\n\n\n\n\nno_of_dependents\neducation\nself_employed\nincome_annum\nloan_amount\nloan_term\ncibil_score\nresidential_assets_value\ncommercial_assets_value\nluxury_assets_value\nbank_asset_value\nloan_status\n\n\n\n\n0\n2\n1\n0\n107,040.00\n333,385.00\n12\n778\n26,760.00\n196,240.00\n253,105.00\n89,200.00\n1\n\n\n1\n0\n0\n1\n45,715.00\n136,030.00\n8\n417\n30,105.00\n24,530.00\n98,120.00\n36,795.00\n0\n\n\n2\n3\n1\n0\n101,465.00\n331,155.00\n20\n506\n79,165.00\n50,175.00\n371,295.00\n142,720.00\n0\n\n\n3\n3\n1\n0\n91,430.00\n342,305.00\n8\n467\n202,930.00\n36,795.00\n259,795.00\n88,085.00\n0\n\n\n4\n5\n0\n1\n109,270.00\n269,830.00\n20\n382\n138,260.00\n91,430.00\n327,810.00\n55,750.00\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4264\n5\n1\n1\n11,150.00\n25,645.00\n12\n317\n31,220.00\n5,575.00\n36,795.00\n8,920.00\n0\n\n\n4265\n0\n0\n1\n36,795.00\n125,995.00\n20\n559\n46,830.00\n32,335.00\n122,650.00\n21,185.00\n1\n\n\n4266\n2\n0\n0\n72,475.00\n266,485.00\n18\n457\n13,380.00\n138,260.00\n201,815.00\n81,395.00\n0\n\n\n4267\n1\n0\n0\n45,715.00\n142,720.00\n8\n780\n91,430.00\n7,805.00\n157,215.00\n64,670.00\n1\n\n\n4268\n1\n1\n0\n102,580.00\n331,155.00\n10\n607\n198,470.00\n131,570.00\n398,055.00\n133,800.00\n1\n\n\n\n\n4269 rows √ó 12 columns\n\n\n\nThe dataset that will be used contains financials and demographic information from an Indian sample, where various attributes of individuals are recorded to analyze and predict loan approval or assess credit risk.\n\nno_of_dependents: An integer column denoting the number of dependents the individual has.\neducation: An integer column, likely representing different levels of education or educational qualifications.\nself_employed: An integer column, potentially indicating if the individual is self-employed or not.\nincome_annum: A floating-point column showing the annual income of the individual.\nloan_amount: A floating-point column indicating the amount of the loan.\nloan_term: An integer column representing the duration or term of the loan in some unit (e.g., months).\ncibil_score: An integer column, possibly representing the credit score of the individual.\nresidential_assets_value: A floating-point column denoting the value of residential assets owned by the individual.\ncommercial_assets_value: A floating-point column indicating the value of commercial assets owned by the individual.\nluxury_assets_value: A floating-point column representing the value of luxury assets owned by the individual.\nbank_asset_value: A floating-point column indicating the value of assets held in banks by the individual.\nloan_status: An integer column, likely representing the status of the loan (e.g., approved, rejected, etc.).\n\nAll columns have the same number of non-null entries, suggesting that there are no missing values.\nBefore presenting the final dataset, we have carried out the following transformations: 1. Label-encoded loan_status, education, and self_employed variables. 2. Converted amount from INR (Indian rupee) to EUR (Euro) at an exchange rate of 0.01115."
  },
  {
    "objectID": "posts/loan_approval/index.html#exploratory-analysis",
    "href": "posts/loan_approval/index.html#exploratory-analysis",
    "title": "üôè Predicting loan approval",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nThe first step is to explore the available variables, looking for potential relationships with the target variable loan_status.\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nno_of_dependents\neducation\nself_employed\nincome_annum\nloan_amount\nloan_term\ncibil_score\nresidential_assets_value\ncommercial_assets_value\nluxury_assets_value\nbank_asset_value\nloan_status\n\n\n\n\ncount\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n4,269.00\n\n\nmean\n2.50\n0.50\n0.50\n56,409.23\n168,737.97\n10.90\n599.94\n83,319.67\n55,450.68\n168,658.31\n55,490.12\n0.62\n\n\nstd\n1.70\n0.50\n0.50\n31,296.26\n100,833.50\n5.71\n172.43\n72,515.55\n48,936.97\n101,506.85\n36,239.57\n0.48\n\n\nmin\n0.00\n0.00\n0.00\n2,230.00\n3,345.00\n2.00\n300.00\n-1,115.00\n0.00\n3,345.00\n0.00\n0.00\n\n\n25%\n1.00\n0.00\n0.00\n30,105.00\n85,855.00\n6.00\n453.00\n24,530.00\n14,495.00\n83,625.00\n25,645.00\n0.00\n\n\n50%\n3.00\n1.00\n1.00\n56,865.00\n161,675.00\n10.00\n600.00\n62,440.00\n41,255.00\n162,790.00\n51,290.00\n1.00\n\n\n75%\n4.00\n1.00\n1.00\n83,625.00\n239,725.00\n16.00\n748.00\n125,995.00\n84,740.00\n241,955.00\n79,165.00\n1.00\n\n\nmax\n5.00\n1.00\n1.00\n110,385.00\n440,425.00\n20.00\n900.00\n324,465.00\n216,310.00\n437,080.00\n163,905.00\n1.00\n\n\n\n\n\n\n\nWe discover a very strong correlation between loan_status and cibil_score, suggesting that loan approval is highly dependant on the financial score of the applicant.\n\n\nCode\ndf.corr()[['loan_status']].sort_values('loan_status', ascending=False)[1:]\n\n\n\n\n\n\n\n\n\nloan_status\n\n\n\n\ncibil_score\n0.77\n\n\nloan_amount\n0.02\n\n\ncommercial_assets_value\n0.01\n\n\neducation\n0.00\n\n\nself_employed\n0.00\n\n\nbank_asset_value\n-0.01\n\n\nresidential_assets_value\n-0.01\n\n\nincome_annum\n-0.02\n\n\nluxury_assets_value\n-0.02\n\n\nno_of_dependents\n-0.02\n\n\nloan_term\n-0.11\n\n\n\n\n\n\n\n\n\nCode\nsns.heatmap(df.corr()[['loan_status']].sort_values('loan_status', ascending=False)[1:])\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\nsns.regplot(df, y='loan_status', x='cibil_score')\n\n\n&lt;Axes: xlabel='cibil_score', ylabel='loan_status'&gt;\n\n\n\n\n\nThe positive relationship between the cibil_score and loan_status variables is confirmed by the regression analysis. As the cibil_score increases, the loan_status also increases, suggesting that a higher credit score may be associated with a higher likelihood of a positive loan status (e.g., loan approval)."
  },
  {
    "objectID": "posts/loan_approval/index.html#predicting-loan-approval",
    "href": "posts/loan_approval/index.html#predicting-loan-approval",
    "title": "üôè Predicting loan approval",
    "section": "Predicting loan approval",
    "text": "Predicting loan approval\nSince we aim to predict loan approval, our target variable will be loan_status. We will initially compare three of the most commonly used classification algorithms (Logistic Regression, Decision Tree, and Random Forest) and select the one demonstrating the best performance. We will measure the performance using the F1 score as the indicator.\nThe F1 score is a metric that combines both precision and recall. Precision measures the accuracy of positive predictions, while recall measures the model‚Äôs ability to capture all positive instances. The F1 score considers both false positives and false negatives, providing a balanced assessment of a model‚Äôs performance, especially in situations where there is an imbalance between the classes being predicted. In the context of predicting loan approval, where a balanced evaluation of correctly predicting both approved and rejected loans is crucial, the F1 score is a suitable performance metric.\n\n\nCode\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n# from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier #, BaggingClassifier\n# from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\nX = df.drop(columns=['loan_status'])\ny = df['loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n\nModel selection\n\n\nCode\nclass_weights = 'balanced'\n\nmodels = (\n    (LogisticRegression(random_state=42, class_weight=class_weights, max_iter=1000), 'Logistic Regression'),\n    # (SVC(random_state=42, class_weight=class_weights, probability=True), 'SVC'),\n    # (KNeighborsClassifier(), 'KNN'),\n    (DecisionTreeClassifier(random_state=42, class_weight=class_weights), 'Decision Tree'),\n    (RandomForestClassifier(random_state=42, class_weight=class_weights), 'Random Forest'),\n    # (BaggingClassifier(random_state=42), 'Bagging'),\n    # (XGBClassifier(random_state=42), 'XGBoost'),\n)\n\nmodel_names = []\naccuracies = []\nprecisions = []\nrecalls = []\nf1s = []\naucs = []\n\nfor model in models:\n    clf = model[0]\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n    model_names.append(model[1])\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred))\n    recalls.append(recall_score(y_test, y_pred))\n    f1s.append(f1_score(y_test, y_pred))\n    aucs.append(roc_auc_score(y_test, y_pred_proba))\n\nmodel_comp = pd.DataFrame(\n    {'Model': model_names, 'Accuracy': accuracies, 'Precision': precisions, 'Recall': recalls, 'F1': f1s, 'AUROC': aucs})\n\n\n\n\nCode\nprint('Model comparison (sorted by F1 score):')\nprint('--------------------------------------')\nmodel_comp.sort_values('F1', ascending=False)\n\n\nModel comparison (sorted by F1 score):\n--------------------------------------\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1\nAUROC\n\n\n\n\n2\nRandom Forest\n0.98\n0.98\n0.99\n0.99\n1.00\n\n\n1\nDecision Tree\n0.98\n0.99\n0.98\n0.99\n0.98\n\n\n0\nLogistic Regression\n0.80\n0.82\n0.86\n0.84\n0.87\n\n\n\n\n\n\n\nThe three classification models have been trained on an split dataframe (60% train test, 40% test set), with a StratifiedKFold cross validation technique.\n\nRandom Forest seems to perform consistently well across most metrics: high accuracy, precision, recall, F1 score, and AUROC (Area Under the Receiver Operating Characteristic Curve). It has a balanced F1 score of 0.98, indicating a good balance between precision and recall.\nDecision Tree also performs quite well but slightly lower than the Random Forest in terms of recall (0.98 compared to 0.99). However, it‚Äôs still a strong performer.\nLogistic Regression has a significantly lower performance on various metrics compared to the other two models, especially in accuracy, F1 score, and AUROC.\n\nBased on the results, the Random Forest model appears to be the best choice due to its high F1 score, near-perfect AUROC, and excellent balance between precision and recall. This model is likely the most reliable for predicting loan approval in this scenario.\n\n\nPredicting loan approval with Random Forest\nAfter selecting the model, we proceed to generate predictions and their probabilities.\n\n\nCode\nrf = RandomForestClassifier(random_state=42, class_weight=class_weights)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\ny_pred_proba = rf.predict_proba(X_test)\nprint('RandomForestClassifier fit:')\nprint(classification_report(y_test, y_pred))\n\n\nRandomForestClassifier fit:\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98       645\n           1       0.98      0.99      0.99      1063\n\n    accuracy                           0.98      1708\n   macro avg       0.98      0.98      0.98      1708\nweighted avg       0.98      0.98      0.98      1708\n\n\n\nThe above output shows the evaluation metrics of the RandomForestClassifier after fitting the model to the dataset. Here‚Äôs a breakdown of the metrics:\n\nPrecision: Indicates the proportion of correctly predicted instances of a specific class among all instances that were predicted as that class. For class 0, the precision is 0.98, and for class 1, it‚Äôs 0.98. This means that 98% of the instances predicted as class 0 were actually class 0, and similarly for class 1.\nRecall: Denotes the proportion of correctly predicted instances of a class among all the instances that actually belong to that class. For class 0, the recall is 0.98, and for class 1, it‚Äôs 0.99. This means that the model identified 98% of the actual class 0 instances and 99% of the actual class 1 instances.\nF1-score: The harmonic mean of precision and recall. It combines both precision and recall into a single value. For class 0, the F1-score is 0.98, and for class 1, it‚Äôs 0.99. The F1-score gives an overall idea of a model‚Äôs accuracy, considering both false positives and false negatives.\nSupport: The number of actual occurrences of each class in the test dataset. For class 0, there are 645 instances, and for class 1, there are 1063 instances.\nAccuracy: Represents the overall correct predictions made by the model. It‚Äôs the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances. The overall accuracy of the model is 0.98, indicating that 98% of the predictions made by the model on the test set were correct.\nMacro Average (macro avg): The average of the precision, recall, and F1-score across all classes, giving equal weight to each class. In this case, it‚Äôs 0.98.\nWeighted Average (weighted avg): The weighted average of the precision, recall, and F1-score, where each class‚Äôs contribution is weighted by its presence in the dataset. This is also 0.98.\n\nOverall, the RandomForestClassifier model demonstrates strong performance across all metrics, indicating high precision, recall, and accuracy, suggesting it‚Äôs a reliable model for classifying instances into the two classes represented in the dataset.\n\n\nCode\nfig = plt.figure(figsize=(15, 5), dpi=300)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='.0f')\nplt.title('Confution matrix\\nRandomForestClassifier fit')\nplt.show()\n\n\n\n\n\n\n\nModel interpretation\nAfter evaluating the model, we check what variables were considered ‚Äúimportant features‚Äù in the prediction.\n\n\nCode\nfeature_importances = pd.DataFrame({'feature': X_train.columns, 'score': rf.feature_importances_})\nprint(\"Feature Importances:\")\nfeature_importances.sort_values('score', ascending=False)\n\n\nFeature Importances:\n\n\n\n\n\n\n\n\n\nfeature\nscore\n\n\n\n\n6\ncibil_score\n0.83\n\n\n5\nloan_term\n0.04\n\n\n4\nloan_amount\n0.03\n\n\n7\nresidential_assets_value\n0.02\n\n\n9\nluxury_assets_value\n0.02\n\n\n3\nincome_annum\n0.02\n\n\n8\ncommercial_assets_value\n0.02\n\n\n10\nbank_asset_value\n0.02\n\n\n0\nno_of_dependents\n0.01\n\n\n2\nself_employed\n0.00\n\n\n1\neducation\n0.00\n\n\n\n\n\n\n\nWe can see that the most influencial feature in the prediction was cibil_score (83%), and that‚Äôs in line with what we discovered in the exploratory analysis. Loan approval is highly influenced by the applicant‚Äôs financial score.\n\n\nLoan approval probability score\nFinally, the following table represents a comparison between the actual loan status (y_true) and the predicted probabilities of loan approval (y_pred) for various loan requests.\n\n\nCode\ntest_df = X_test.copy()\ntest_df['loan_status (y_true)'] = y_test\ntest_df['approval_proba (y_pred)'] = y_pred_proba[:, 1]\ntest_df[['loan_status (y_true)', 'approval_proba (y_pred)']]\n\n\n\n\n\n\n\n\n\nloan_status (y_true)\napproval_proba (y_pred)\n\n\n\n\n12\n0\n0.00\n\n\n3806\n0\n0.03\n\n\n883\n1\n0.98\n\n\n392\n1\n0.98\n\n\n2894\n1\n1.00\n\n\n...\n...\n...\n\n\n1115\n1\n1.00\n\n\n210\n1\n0.96\n\n\n4172\n1\n0.97\n\n\n3824\n1\n0.99\n\n\n3974\n1\n0.80\n\n\n\n\n1708 rows √ó 2 columns"
  },
  {
    "objectID": "posts/online-retail-mba/index.html",
    "href": "posts/online-retail-mba/index.html",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "",
    "text": "Even though the majority of the volume of sales is concentrated in the UK, the most performing region in terms of average revenue is Asia. The ANOVA analysis shows that the mean purchase value in the Asia/Pacific region is consistently and significantly higher than the mean purchase value in the other regions. We can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales.\nBy conducting a market basket analysis with a focus on the Asian market, we have identified groups of products that are commonly bought together. This has helped us uncover the specific preferences and purchasing patterns of this region. The firm could use this information to create bundled offers that combine these item sets and boost sales volume in the Asian market, ultimately leading to an increase in revenue."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#key-findings-and-achievements",
    "href": "posts/online-retail-mba/index.html#key-findings-and-achievements",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "",
    "text": "Even though the majority of the volume of sales is concentrated in the UK, the most performing region in terms of average revenue is Asia. The ANOVA analysis shows that the mean purchase value in the Asia/Pacific region is consistently and significantly higher than the mean purchase value in the other regions. We can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales.\nBy conducting a market basket analysis with a focus on the Asian market, we have identified groups of products that are commonly bought together. This has helped us uncover the specific preferences and purchasing patterns of this region. The firm could use this information to create bundled offers that combine these item sets and boost sales volume in the Asian market, ultimately leading to an increase in revenue."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#introduction",
    "href": "posts/online-retail-mba/index.html#introduction",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "Introduction",
    "text": "Introduction\nThe e-commerce industry has experienced significant growth in recent years, and online sales have become an increasingly important aspect of many businesses. Analyzing sales data can help businesses understand customer behavior and identify trends, which can then be used to improve their overall sales strategies and revenue. In this notebook, we will be analyzing a sales dataset from an e-commerce company to gain insights into their sales patterns and identify profitable opportunities.\nFirstly, we will delve into analyzing the regional distribution of sales to evaluate the existence of untapped markets. By assessing the sales data across different regions, we will determine the areas where our products are in high demand and identify potential opportunities for expanding our market presence.\nSecondly, we will conduct a market basket analysis to uncover correlations between products and provide recommendations for boosting sales. This will involve examining the patterns of product combinations that are frequently purchased together by customers. By identifying these relationships, we can optimize our marketing strategies to better meet the needs of our customers and ultimately increase revenue.\n\n\nCode\n# dataframes\nimport numpy as np\nimport pandas as pd\n\n# dataviz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from jupyterthemes import jtplot\n# jtplot.style(theme='monokai', context='notebook', grid=False)\n\n# hypothesis testing\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# market basket analysis\nfrom itertools import permutations\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\nfrom pandas.plotting import parallel_coordinates\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
  },
  {
    "objectID": "posts/online-retail-mba/index.html#data",
    "href": "posts/online-retail-mba/index.html#data",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "Data",
    "text": "Data\nLet‚Äôs begin by taking a look at the data at our disposal. The dataset is composed by the following original variables:\n\nTransactionNo (categorical): a six-digit unique number that defines each transaction. The letter ‚ÄúC‚Äù in the code indicates a cancellation.\nDate (numeric): the date when each transaction was generated.\nProductNo (categorical): a five or six-digit unique character used to identify a specific product.\nProduct (categorical): product/item name.\nPrice (numeric): the price of each product per unit in pound sterling (¬£).\nQuantity (numeric): the quantity of each product per transaction. Negative values related to cancelled transactions.\nCustomerNo (categorical): a five-digit unique number that defines each customer.\nCountry (categorical): name of the country where the customer resides.\n\n\n\nCode\ndf = pd.read_csv('../../data/sales.csv')\n\n# We perform some operations to validate the type of variable, create an `Amount` column, \n# and subset the dataframe for an exact year of sales.\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf[['ProductNo', 'CustomerNo']] = df[['ProductNo', 'CustomerNo']].astype('object')\n\ndf['Amount'] = df['Quantity'] * df['Price']\n\ndf = df[(df['Date'] &lt;= '2019-11-30') & (df['Quantity'] &gt; 0)].copy()\n\ndisplay(df.info())\ndisplay(df)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 502753 entries, 25361 to 536324\nData columns (total 9 columns):\n #   Column         Non-Null Count   Dtype         \n---  ------         --------------   -----         \n 0   TransactionNo  502753 non-null  object        \n 1   Date           502753 non-null  datetime64[ns]\n 2   ProductNo      502753 non-null  object        \n 3   ProductName    502753 non-null  object        \n 4   Price          502753 non-null  float64       \n 5   Quantity       502753 non-null  int64         \n 6   CustomerNo     502752 non-null  object        \n 7   Country        502753 non-null  object        \n 8   Amount         502753 non-null  float64       \ndtypes: datetime64[ns](1), float64(2), int64(1), object(5)\nmemory usage: 38.4+ MB\n\n\nNone\n\n\n\n\n\n\n\n\n\nTransactionNo\nDate\nProductNo\nProductName\nPrice\nQuantity\nCustomerNo\nCountry\nAmount\n\n\n\n\n25361\n579522\n2019-11-30\n84879\nAssorted Colour Bird Ornament\n6.04\n8\n14432.0\nUnited Kingdom\n48.32\n\n\n25362\n579522\n2019-11-30\n82486\n3 Drawer Antique White Wood Cabinet\n6.04\n4\n14432.0\nUnited Kingdom\n24.16\n\n\n25363\n579522\n2019-11-30\n82483\nWood 2 Drawer Cabinet White Finish\n6.04\n8\n14432.0\nUnited Kingdom\n48.32\n\n\n25364\n579522\n2019-11-30\n23493\nVintage Doily Travel Sewing Kit\n6.04\n10\n14432.0\nUnited Kingdom\n60.40\n\n\n25365\n579522\n2019-11-30\n23240\nSet Of 4 Knick Knack Tins Doily\n6.19\n6\n14432.0\nUnited Kingdom\n37.14\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n536320\n536585\n2018-12-01\n37449\nCeramic Cake Stand + Hanging Cakes\n20.45\n2\n17460.0\nUnited Kingdom\n40.90\n\n\n536321\n536590\n2018-12-01\n22776\nSweetheart 3 Tier Cake Stand\n20.45\n1\n13065.0\nUnited Kingdom\n20.45\n\n\n536322\n536590\n2018-12-01\n22622\nBox Of Vintage Alphabet Blocks\n20.45\n2\n13065.0\nUnited Kingdom\n40.90\n\n\n536323\n536591\n2018-12-01\n37449\nCeramic Cake Stand + Hanging Cakes\n20.45\n1\n14606.0\nUnited Kingdom\n20.45\n\n\n536324\n536597\n2018-12-01\n22220\nCake Stand Lovebird 2 Tier White\n20.45\n1\n18011.0\nUnited Kingdom\n20.45\n\n\n\n\n502753 rows √ó 9 columns"
  },
  {
    "objectID": "posts/online-retail-mba/index.html#geographical-sales-analysis",
    "href": "posts/online-retail-mba/index.html#geographical-sales-analysis",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "Geographical sales analysis",
    "text": "Geographical sales analysis\nWhen conducting a geographical analysis of sales, it is essential to consider both the average purchase value and sales volume to determine if there are any countries that offer promising opportunities. For instance, a country with a high average purchase value but low sales volume may indicate that it has untapped potential and should be targeted for further penetration. The average purchase value gives an indication of the buying power and willingness of customers to spend money, while sales volume reflects the market demand and potential for growth. A country with a high average purchase value and low sales volume could be a potential opportunity for businesses to capitalize on the untapped market potential by increasing their presence and promoting their products or services more effectively.\nThe analysis of the geographical spread of sales will be carried out at country level, national vs.¬†international level, and finally regional level.\n\nCountry level\n\n\nCode\nbycountry = df.groupby('Country').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean')\n).sort_values('tot_amount', ascending=False)\n\n# display(bycountry.head())\n\nfig, ax = plt.subplots(2, figsize=(15,10))\nax[0].bar(bycountry.index, bycountry['tot_amount'])\nax[1].bar(bycountry.sort_values('mean_amount', ascending=False).index, bycountry.sort_values('mean_amount', ascending=False)['mean_amount'])\nplt.setp(ax, xticks=bycountry.index, xticklabels=bycountry.index)\nplt.setp(ax[0].get_xticklabels(), rotation=90, ha=\"center\")\nplt.setp(ax[1].get_xticklabels(), rotation=90, ha=\"center\")\n\nax[0].set_ylabel(\"Amount (GBP)\")\nax[1].set_ylabel(\"Amount (GBP)\")\nax[0].set_title(\"Countries by total amount sold\")\nax[1].set_title(\"Countries by average amount sold\")\nplt.suptitle(\"Overview on geographical market spread\")\nax[0].grid(axis='y')\nax[1].grid(axis='y')\nplt.subplots_adjust(hspace=0.7)\n\nplt.show()\n\n\n\n\n\n\n\nNational vs.¬†international level\n\n\nCode\n# Creating a column to differentiation between national vs. international sales\ndf['UKvsRoW'] = np.where(df['Country'] == 'United Kingdom', 'UK', 'RoW')\n\nbyukvsrow = df.groupby('UKvsRoW').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\n\ndisplay(byukvsrow)\n\nplt.pie(byukvsrow['tot_amount'], labels=byukvsrow.index, autopct='%1.1f%%', explode=(0.1,0), shadow=True)\nplt.title('Total revenue by UK vs other countries')\nplt.show()\n\n\n\n\n\n\n\n\n\ntot_amount\nmean_amount\nn_inv\nquantity\n\n\nUKvsRoW\n\n\n\n\n\n\n\n\nRoW\n10258462.78\n211.383944\n1809\n18.478900\n\n\nUK\n50192562.28\n110.502027\n17164\n9.646163\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrow_rev = df.loc[df['UKvsRoW'] == 'RoW', 'Amount']\nuk_rev = df.loc[df['UKvsRoW'] == 'UK', 'Amount']\n\n# We perform a two-sample ttest to check for statistically significant difference in average sales between national and international markets\nttest_ind(uk_rev, row_rev)\n\n\nTtest_indResult(statistic=-16.7279858606087, pvalue=8.524026769071223e-63)\n\n\n\nEven though the volume of sales of international customers accounts only for the 17.0%, a two-sample ttest demonstrates that the average revenue generated abroad is significantly higher than the one generated in the UK. This means that international markets for this business are potentially more lucrative than the national one and need to be exploited more.\n\n\n\nRegional level\n\n\nCode\n# Mapping regions\nregions = {'Europe': ['Sweden', 'Denmark', 'Norway', 'Finland', 'Iceland', 'Netherlands', 'Belgium', 'France', 'Germany', 'Switzerland', 'Austria',\n                      'Italy', 'Spain', 'Greece', 'Portugal', 'Malta', 'Cyprus', 'Czech Republic', 'Lithuania', 'Poland', 'United Kingdom', 'EIRE',\n                      'Channel Islands', 'European Community'],\n           'North America': ['USA', 'Canada'],\n           'Middle East': ['Bahrain', 'United Arab Emirates', 'Israel', 'Lebanon', 'Saudi Arabia'],\n           'Asia Pacific': ['Japan', 'Australia', 'Singapore', 'Hong Kong'],\n           'RoW': ['Brazil', 'RSA'],\n           'Unspecified': ['Unspecified']}\n\ncountry_to_region = {}\nfor region, countries in regions.items():\n    for country in countries:\n        country_to_region[country] = region\n\ndf['Region'] = df['Country'].map(country_to_region)\n\nbyregion = df.groupby('Region').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\n\ndisplay(byregion.sort_values('mean_amount', ascending=False))\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax1 = plt.bar(byregion.index, byregion['mean_amount'])\nplt.title(\"Average purchase value by region\")\nplt.ylabel('Amount (GBP)')\nplt.xlabel('Region')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\n\n\n\n\n\ntot_amount\nmean_amount\nn_inv\nquantity\n\n\nRegion\n\n\n\n\n\n\n\n\nAsia Pacific\n1380079.80\n590.787586\n92\n51.083904\n\n\nNorth America\n59633.28\n154.891636\n11\n13.503896\n\n\nMiddle East\n76798.30\n154.835282\n16\n12.661290\n\n\nEurope\n58892902.53\n118.021612\n18839\n10.308538\n\n\nRoW\n8912.10\n101.273864\n2\n8.000000\n\n\nUnspecified\n32699.05\n73.152237\n13\n6.272931\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nf_value, p_value = f_oneway(\n    df.loc[df['Region'] == 'Asia Pacific', 'Amount'],\n    df.loc[df['Region'] == 'North America', 'Amount'],\n    df.loc[df['Region'] == 'Middle East', 'Amount'],\n    df.loc[df['Region'] == 'Europe', 'Amount'],\n    df.loc[df['Region'] == 'RoW', 'Amount'])\nprint(f'ANOVA F-value: {f_value:.2f}')\nprint(f'ANOVA p-value: {p_value:.4f}')\ntukey_df = df.filter(items=['Amount', 'Region']).dropna()\nprint(pairwise_tukeyhsd(tukey_df['Amount'], tukey_df['Region']))\n\n\nANOVA F-value: 81.58\nANOVA p-value: 0.0000\n          Multiple Comparison of Means - Tukey HSD, FWER=0.05          \n=======================================================================\n    group1        group2     meandiff p-adj    lower     upper   reject\n-----------------------------------------------------------------------\n Asia Pacific        Europe  -472.766    0.0 -547.3921 -398.1398   True\n Asia Pacific   Middle East -435.9523    0.0  -613.855 -258.0496   True\n Asia Pacific North America -435.8959    0.0 -633.8256 -237.9663   True\n Asia Pacific           RoW -489.5137 0.0048 -880.2663  -98.7612   True\n Asia Pacific   Unspecified -517.6353    0.0 -703.4071 -331.8636   True\n       Europe   Middle East   36.8137 0.9872  -124.841  198.4683  False\n       Europe North America     36.87 0.9928 -146.5938  220.3338  False\n       Europe           RoW  -16.7477    1.0 -400.3757  366.8802  False\n       Europe   Unspecified  -44.8694 0.9754 -215.1456  125.4068  False\n  Middle East North America    0.0564    1.0 -244.3599  244.4726  False\n  Middle East           RoW  -53.5614 0.9991 -469.7954  362.6726  False\n  Middle East   Unspecified   -81.683 0.9207 -316.3622  152.9961  False\nNorth America           RoW  -53.6178 0.9992 -478.7971  371.5616  False\nNorth America   Unspecified  -81.7394 0.9386 -331.9414  168.4626  False\n          RoW   Unspecified  -28.1216    1.0 -447.7792   391.536  False\n-----------------------------------------------------------------------\n\n\n\nWe can observe from both the bar plot and the ANOVA analysis that the mean purchase value in the Asia/Pacific region is consistently and significantly higher than the mean purchase value in the other regions. Based on this important information, we can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales. The business can consider implementing targeted marketing strategies, such as advertising campaigns and promotions, that cater to the preferences and interests of the Asia/Pacific market. Additionally, it can explore expanding its product offerings to meet the specific demands of this region, or enhancing the quality of existing products to meet their higher standards. It may be useful to conduct further research and analysis to gain deeper insights into the preferences and behavior of customers in the Asia/Pacific region, and tailor sales strategies accordingly."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#market-basket-analysis-for-the-asian-market",
    "href": "posts/online-retail-mba/index.html#market-basket-analysis-for-the-asian-market",
    "title": "üõí Using Market Basket Analysis to penetrate an untapped regional market",
    "section": "Market basket analysis for the Asian market",
    "text": "Market basket analysis for the Asian market\nMarket basket analysis, specifically Apriori and association rules, can provide valuable insights into customer behavior and preferences that can be used to develop effective marketing strategies. By analyzing customer purchase patterns and identifying which products are commonly purchased together, businesses can create product bundles and promotions that cater to specific customer segments. For instance, if the analysis reveals that customers who purchase Product A are highly likely to also purchase Product B, the business can create a bundle that includes both products at a discounted price to increase sales.\nThe Asia/Pacific region has a consistently higher average purchase value than other regions, indicating a potential opportunity to increase sales and revenue in that particular market. By conducting basket analysis on this region, the business can gain further insights into the specific product preferences and purchasing habits of customers in this market. This information can then be used to create targeted marketing strategies, such as promotions and advertising campaigns, that appeal to the unique needs and interests of customers in the Asia/Pacific region.\nFirst, we subset the dataframe to filter for the transactions happened in the Asian market and encode them in binary features (one-hot encoding). Then, with the Apriori algorithm, we group them together according to a minimum support of 0.05 and we filter them according to a minimum confidence level of 1.\nThe result is a dataframe containing frequently sold itemsets with a set a metrics for market basket analysis. These MBA metrics are commonly used in association rule mining, a data mining technique used to identify relationships and patterns among items in a dataset. Here‚Äôs a brief explanation of each metric:\n\nAntecedent support: This refers to the proportion of transactions that contain the antecedent (or the ‚Äúif‚Äù part of a rule). It is calculated as the number of transactions containing the antecedent divided by the total number of transactions.\nConsequent support: This refers to the proportion of transactions that contain the consequent (or the ‚Äúthen‚Äù part of a rule). It is calculated as the number of transactions containing the consequent divided by the total number of transactions.\nSupport: This refers to the proportion of transactions that contain both the antecedent and the consequent. It is calculated as the number of transactions containing both the antecedent and the consequent divided by the total number of transactions.\nConfidence: This measures the strength of the association between the antecedent and the consequent. It is calculated as the support of the antecedent and consequent divided by the support of the antecedent. Confidence can range from 0 to 1, with higher values indicating stronger associations.\nLift: This measures the degree to which the presence of the antecedent affects the likelihood of the consequent. It is calculated as the support of the antecedent and consequent divided by the product of the support of the antecedent and the support of the consequent. A lift value greater than 1 indicates a positive association between the antecedent and consequent, while a value less than 1 indicates a negative association.\nLeverage: This measures the difference between the observed frequency of the antecedent and consequent co-occurring and the frequency expected if they were independent. It is calculated as the support of the antecedent and consequent minus the product of the support of the antecedent and the support of the consequent. A positive leverage value indicates a positive association between the antecedent and consequent, while a negative value indicates a negative association.\nConviction: This measures the degree of implication of the rule. It is calculated as the ratio of the support of the antecedent to the complement of the confidence. Conviction can range from 0 to infinity, with higher values indicating stronger implications.\n\n\n\nCode\n# Subsetting for Asia/Pacific transactions\nasian_market = df[df['Region'] == 'Asia Pacific']\n\n# Converting transactions in a list of lists\ntransactions = asian_market.groupby('TransactionNo').apply(lambda x: list(x['ProductName'])).to_list()\nencoder = TransactionEncoder().fit(transactions)\nonehot = encoder.transform(transactions)\nonehot = pd.DataFrame(onehot, columns=encoder.columns_)\n\n# Selecting frequent itemsets with apriori algorythm\nfrequent_itemsets = apriori(onehot,\n                            min_support = 0.05, \n                            max_len = 5, \n                            use_colnames = True)\n\nprint('Number of itemsets selected by the Apriori algorithm:', len(frequent_itemsets))\n\n# Computing association rules for the frequent itemsets, and filtering by confidence == 1\nrules = association_rules(frequent_itemsets, metric='confidence', min_threshold=1)\n\n# Adding number of items in the itemsets\nrules['n_antecedents'] = rules['antecedents'].apply(lambda x: len(x))\nrules['n_consequents'] = rules['consequents'].apply(lambda x: len(x))\nrules.sample(15, random_state=42)\n\n\nNumber of itemsets selected by the Apriori algorithm: 163\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nn_antecedents\nn_consequents\n\n\n\n\n37\n(Roses Regency Teacup And Saucer, Regency Cake...\n(Spaceboy Lunch Box)\n0.054348\n0.108696\n0.054348\n1.0\n9.200000\n0.048440\ninf\n3\n1\n\n\n24\n(Spaceboy Lunch Box, Roses Regency Teacup And ...\n(Dolly Girl Lunch Box)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n2\n1\n\n\n25\n(Roses Regency Teacup And Saucer, Dolly Girl L...\n(Spaceboy Lunch Box)\n0.054348\n0.108696\n0.054348\n1.0\n9.200000\n0.048440\ninf\n2\n1\n\n\n36\n(Spaceboy Lunch Box, Roses Regency Teacup And ...\n(Regency Cakestand 3 Tier)\n0.054348\n0.086957\n0.054348\n1.0\n11.500000\n0.049622\ninf\n3\n1\n\n\n34\n(Spaceboy Lunch Box, Regency Cakestand 3 Tier,...\n(Roses Regency Teacup And Saucer)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n3\n1\n\n\n39\n(Spaceboy Lunch Box, Roses Regency Teacup And ...\n(Regency Cakestand 3 Tier, Dolly Girl Lunch Box)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n2\n2\n\n\n4\n(Blue Polkadot Bowl)\n(Pink Polkadot Bowl)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n12\n(Set Of 6 Snack Loaf Baking Cases)\n(Set Of 6 Tea Time Baking Cases)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n8\n(Feltcraft Princess Lola Doll)\n(Feltcraft Princess Olivia Doll)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n3\n(Blue Happy Birthday Bunting)\n(Pink Happy Birthday Bunting)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n6\n(Dolly Girl Lunch Box)\n(Spaceboy Lunch Box)\n0.097826\n0.108696\n0.097826\n1.0\n9.200000\n0.087193\ninf\n1\n1\n\n\n27\n(Set 3 Retrospot Tea/Coffee/Sugar, Lunch Bag D...\n(Red Spotty Biscuit Tin)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n2\n1\n\n\n31\n(Spaceboy Lunch Box, Regency Cakestand 3 Tier)\n(Roses Regency Teacup And Saucer)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n2\n1\n\n\n19\n(Roses Regency Teacup And Saucer, Regency Cake...\n(Dolly Girl Lunch Box)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n2\n1\n\n\n17\n(Dolly Girl Lunch Box, Lunch Bag Dolly Girl De...\n(Spaceboy Lunch Box)\n0.065217\n0.108696\n0.065217\n1.0\n9.200000\n0.058129\ninf\n2\n1\n\n\n\n\n\n\n\n\nUpon examining the frequent itemsets, it becomes evident that most of them consist of identical items that are often purchased together, with only minor variations such as color or pattern. For instance, transactions may include items like Blue Polkadot Bowls and Pink Polkadot Bowls, Dolly Girl Lunch Boxes and Spaceboy Lunch Boxes, or Feltcraft Princess Lola Dolls and Feltcraft Princess Olivia Dolls.\n\n\nBundle offers\nBased on the observation that these items are frequently bought together, it could be advantageous to offer them as bundles to customers. The firm could offer convenience and value to customers while potentially increasing sales and revenue. For example, a bundle might include both the Blue Polkadot Bowl and the Pink Polkadot Bowl, or the Dolly Girl Lunch Box and the Spaceboy Lunch Box. This strategy can be an effective way to meet Asian customers needs while boosting profits for the retailer.\n\n\nCode\n# Since we want to create bundle offers for single products, we filter for single items\nrules = rules[(rules['n_antecedents'] == 1) & (rules['n_consequents'] == 1)]\ndisplay(rules.sort_values('support', ascending=False))\n\nrules['antecedent'] = rules['antecedents'].apply(lambda x: list(x)[0])\nrules['consequent'] = rules['consequents'].apply(lambda x: list(x)[0])\nrules['rule'] = rules.index\n\ncoords = rules[['antecedent', 'consequent', 'rule']]\n\nparallel_coordinates(coords, 'rule', colormap='ocean')\nplt.title('Bundle offers for the Asian / Pacific market')\nplt.show()\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nn_antecedents\nn_consequents\n\n\n\n\n6\n(Dolly Girl Lunch Box)\n(Spaceboy Lunch Box)\n0.097826\n0.108696\n0.097826\n1.0\n9.200000\n0.087193\ninf\n1\n1\n\n\n0\n(Alarm Clock Bakelike Red)\n(Alarm Clock Bakelike Green)\n0.065217\n0.065217\n0.065217\n1.0\n15.333333\n0.060964\ninf\n1\n1\n\n\n1\n(Alarm Clock Bakelike Green)\n(Alarm Clock Bakelike Red)\n0.065217\n0.065217\n0.065217\n1.0\n15.333333\n0.060964\ninf\n1\n1\n\n\n2\n(Basket Of Toadstools)\n(Set 3 Retrospot Tea/Coffee/Sugar)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n3\n(Blue Happy Birthday Bunting)\n(Pink Happy Birthday Bunting)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n4\n(Blue Polkadot Bowl)\n(Pink Polkadot Bowl)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n5\n(Pink Polkadot Bowl)\n(Blue Polkadot Bowl)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n7\n(Fairy Tale Cottage Night Light)\n(Red Toadstool Led Night Light)\n0.054348\n0.119565\n0.054348\n1.0\n8.363636\n0.047850\ninf\n1\n1\n\n\n8\n(Feltcraft Princess Lola Doll)\n(Feltcraft Princess Olivia Doll)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n9\n(Green Regency Teacup And Saucer)\n(Roses Regency Teacup And Saucer)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n1\n1\n\n\n10\n(Set Of 4 Knick Knack Tins Leaf)\n(Set Of 4 Knick Knack Tins Doily)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n11\n(Set Of 6 Tea Time Baking Cases)\n(Set Of 6 Snack Loaf Baking Cases)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n12\n(Set Of 6 Snack Loaf Baking Cases)\n(Set Of 6 Tea Time Baking Cases)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n\n\n\n\n\n\n\n\n\nThe parallel coordinates plot visually highlights the bundles that were put together for the Asian market, and that the firm should offer on their e-commerce.\n\nOffering bundles of products that are already sold together as frequent itemsets can be an effective marketing strategy for several reasons:\n\nConvenience: Bundling products that are frequently purchased together can provide customers with a convenient and streamlined shopping experience. Instead of having to search for each product individually, customers can purchase them together in a single transaction.\nValue proposition: Bundling products can create a compelling value proposition for customers. By offering a discount or special deal on a bundle of products, customers may be more likely to make a purchase than if they were buying each item individually.\nIncreased sales: Bundling can also lead to increased sales by encouraging customers to purchase additional products that they may not have otherwise considered. For example, a customer who only intended to buy coffee may be enticed to buy a bundle that includes coffee, a mug, and a bag of coffee beans.\nUpselling opportunities: Bundling can also provide opportunities for upselling by encouraging customers to purchase a higher-value bundle that includes additional products or features.\n\nIn summary, while some products may already be sold together as frequent itemsets, bundling can provide additional value and convenience for customers, as well as opportunities for increased sales and upselling. By offering bundles, businesses can differentiate themselves from competitors and create a more compelling value proposition for their customers."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html",
    "href": "posts/online-retail-clustering/index.html",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "",
    "text": "Computed RFMT features from a retail purchase dataset, for customer segmentation analysis.\nPreprocessed and normalized the dataset to enable optimal clustering, and determined the most suitable number of clusters (k) using the Elbow Method.\nExecuted a comparative analysis of two KMeans clustering solutions, providing a clear distinction between the customer groups for k=n and k=n+1.\nInterpreted each cluster to define distinct business personas, leading to the identification of strategic, customized actions aimed at enhancing customer engagement and business growth."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html#key-findings-and-achievements",
    "href": "posts/online-retail-clustering/index.html#key-findings-and-achievements",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "",
    "text": "Computed RFMT features from a retail purchase dataset, for customer segmentation analysis.\nPreprocessed and normalized the dataset to enable optimal clustering, and determined the most suitable number of clusters (k) using the Elbow Method.\nExecuted a comparative analysis of two KMeans clustering solutions, providing a clear distinction between the customer groups for k=n and k=n+1.\nInterpreted each cluster to define distinct business personas, leading to the identification of strategic, customized actions aimed at enhancing customer engagement and business growth."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html#introduction",
    "href": "posts/online-retail-clustering/index.html#introduction",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "Introduction",
    "text": "Introduction\nIn today‚Äôs competitive marketplace, leveraging data to understand and predict customer behavior is paramount for sustaining growth and securing a strategic advantage. Customer clustering analysis stands at the forefront of this endeavor, offering a powerful suite of techniques for unraveling the complexities of customer data. By partitioning customers into distinct groups, or clusters, based on shared attributes, businesses can tailor their marketing strategies, optimize resource allocation, and ultimately drive a more personalized customer experience.\nThe core aim of this analytical notebook is to perform an in-depth clustering analysis of retail customers, with a focus on RFMT attributes. By dissecting the nuanced behaviors captured within these features, we aspire to unearth distinct customer segments that can reveal targeted opportunities for enhanced marketing engagement, optimized product placement, and improved customer lifetime value."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html#loading-the-data",
    "href": "posts/online-retail-clustering/index.html#loading-the-data",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "Loading the data",
    "text": "Loading the data\nThe dataset we are utilizing for this exercise is a classic example of transactional data commonly encountered across various business sectors. It represents purchase data from an online retail business based in the UK. Each record in this dataset corresponds to a single purchase transaction made by a customer. This type of dataset is ubiquitous in the business world, providing invaluable insights into customer behavior.\nOur dataset is structured as follows:\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1041671 entries, 0 to 1067370\nData columns (total 10 columns):\n #   Column       Non-Null Count    Dtype         \n---  ------       --------------    -----         \n 0   Invoice      1041671 non-null  string        \n 1   StockCode    1041671 non-null  string        \n 2   Description  1041671 non-null  string        \n 3   Quantity     1041671 non-null  int64         \n 4   InvoiceDate  1041671 non-null  datetime64[us]\n 5   Price        1041671 non-null  float64       \n 6   CustomerID   805549 non-null   float64       \n 7   Country      1041671 non-null  string        \n 8   Year         1041671 non-null  int64         \n 9   Amount       1041671 non-null  float64       \ndtypes: datetime64[us](1), float64(3), int64(2), string(4)\nmemory usage: 87.4 MB\n\n\nNone\n\n\n\n\n\n\n\n\n\nQuantity\nInvoiceDate\nPrice\nCustomerID\nYear\nAmount\n\n\n\n\ncount\n1.041671e+06\n1041671\n1.041671e+06\n805549.000000\n1.041671e+06\n1.041671e+06\n\n\nmean\n1.096345e+01\n2011-01-03 16:31:26.403269\n4.077038e+00\n15331.954970\n1.508899e+00\n2.013397e+01\n\n\nmin\n1.000000e+00\n2009-12-01 07:45:00\n1.000000e-03\n12346.000000\n1.000000e+00\n1.000000e-03\n\n\n25%\n1.000000e+00\n2010-07-12 10:26:00\n1.250000e+00\n13982.000000\n1.000000e+00\n3.900000e+00\n\n\n50%\n3.000000e+00\n2010-12-07 15:33:00\n2.100000e+00\n15271.000000\n2.000000e+00\n9.960000e+00\n\n\n75%\n1.000000e+01\n2011-07-24 12:05:00\n4.130000e+00\n16805.000000\n2.000000e+00\n1.770000e+01\n\n\nmax\n8.099500e+04\n2011-12-09 12:50:00\n2.511109e+04\n18287.000000\n2.000000e+00\n1.684696e+05\n\n\nstd\n1.265149e+02\nNaN\n5.144898e+01\n1696.737039\n4.999210e-01\n2.031167e+02\n\n\n\n\n\n\n\nThe columns include basic transactional elements like Invoice number, StockCode, product Description, Quantity, and Amount of items purchased, InvoiceDate, Price of the items, CustomerID, Country of purchase, and the Year of the transaction."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html#computing-recency-frequency-monetary-value-and-tenure",
    "href": "posts/online-retail-clustering/index.html#computing-recency-frequency-monetary-value-and-tenure",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "Computing Recency, Frequency, Monetary Value, and Tenure",
    "text": "Computing Recency, Frequency, Monetary Value, and Tenure\nThe RFMT model is a cornerstone of customer value analysis, providing a multifaceted framework to quantify and understand customer behavior. In this phase of our analysis, we will compute the four key metrics that comprise the RFMT model:\n\nRecency (R): The freshness of customer engagement, measured by the time since the last transaction. This metric helps us understand which customers have interacted with the business recently, indicating an ongoing relationship.\nFrequency (F): The rate of transactions, determined by the number of purchases made within a specific period. This dimension allows us to identify the most engaged customers who transact with the business regularly.\nMonetary Value (M): The total spend of a customer, calculated by summing up the value of all purchases. This metric highlights the customers who contribute the most to revenue and can indicate potential for future profitability.\nTenure (T): The length of time a customer has been with the business, defined from their first purchase to the present. Tenure provides insight into customer loyalty and the long-term value of customer relationships.\n\nBy computing these RFMT metrics, we will transform our raw transactional data into actionable insights, segmenting customers into groups that reflect varying levels of engagement and value to the organization. This stratification enables targeted marketing efforts, efficient allocation of resources, and the development of tailored customer retention strategies. In practice, we will calculate these metrics by aggregating transaction data at the customer level, ensuring that each customer‚Äôs profile is accurately represented by their shopping habits and interactions with the business.\nThe recency and tenure calculations will require us to establish a reference point ‚Äî typically the date of analysis or the latest date in the dataset ‚Äî to gauge the time elapsed since the last transaction and the duration of the customer relationship, respectively. Frequency will be assessed by counting the number of transactions per customer, and monetary value will be ascertained by summing the total spend per customer. The process will involve data manipulation and the use of aggregate functions to ensure precise and meaningful calculations.\nUltimately, the RFMT analysis will not only differentiate customers by their transactional behavior but also serve as the basis for applying clustering algorithms. The outcome will be a set of clearly defined customer segments, each with its own distinct characteristics based on these four critical dimensions. These segments will then inform a range of strategic initiatives designed to enhance customer engagement and optimize business outcomes.\nFirst, we define a snapshot date ‚Äì a simulated point in time when we run the analysis. In our case it will be represented by the last date available in the dataset.\nThen we group the data by customer ID and aggregate by:\n\nRecency as the number of days since the last purchase for each customer.\nFrequency as the number of purchases made by each customer.\nMonetaryValue as the total amount spent by each customer.\nTenure as the number of days each customer has been active.\n\nHere below a preview of the transformed dataset:\n\nrfmt = df.groupby('CustomerID').agg(\n    Recency = ('InvoiceDate', lambda x: (snapshot_date - x.max()).days),\n    Frequency = ('Amount', 'count'),\n    MonetaryValue = ('Amount', 'sum'),\n    Tenure = ('InvoiceDate', lambda x: (x.max() - x.min()).days)\n)\nrfmt\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\nTenure\n\n\nCustomerID\n\n\n\n\n\n\n\n\n12346.0\n325\n34\n77556.46\n400\n\n\n12347.0\n1\n253\n5633.32\n402\n\n\n12348.0\n74\n51\n2019.40\n362\n\n\n12349.0\n18\n175\n4428.69\n570\n\n\n12350.0\n309\n17\n334.40\n0\n\n\n...\n...\n...\n...\n...\n\n\n18283.0\n3\n986\n2736.65\n654\n\n\n18284.0\n431\n28\n461.68\n0\n\n\n18285.0\n660\n12\n427.00\n0\n\n\n18286.0\n476\n67\n1296.43\n247\n\n\n18287.0\n42\n155\n4182.99\n528\n\n\n\n\n5878 rows √ó 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\nTenure\n\n\n\n\ncount\n5878.000000\n5878.000000\n5878.000000\n5878.000000\n\n\nmean\n200.331916\n137.044743\n3018.616737\n273.022457\n\n\nstd\n209.338707\n353.818629\n14737.731040\n258.807591\n\n\nmin\n0.000000\n1.000000\n2.950000\n0.000000\n\n\n25%\n25.000000\n21.000000\n348.762500\n0.000000\n\n\n50%\n95.000000\n53.000000\n898.915000\n220.500000\n\n\n75%\n379.000000\n142.000000\n2307.090000\n511.000000\n\n\nmax\n738.000000\n12890.000000\n608821.650000\n738.000000\n\n\n\n\n\n\n\nA quick analysis of the resulting features shows us that:\n\nThe average time since the last purchase across all customers is roughly 200 days, suggesting a moderate level of recent engagement. The standard deviation is substantial at approximately 209 days, indicating variability in customer engagement. The quickest repeat purchase occurred on the same day, while the longest time since a previous purchase extends to 738 days.\nCustomers have made an average of 137 transactions, which is quite diverse as reflected by a high standard deviation of around 354. The frequency of purchases ranges from a single transaction to a high of 12,890, showing that there are both one-time and extremely frequent shoppers in the dataset.\nThe mean monetary value spent by customers is $3,018.62, but with a very high standard deviation of $14,737.73, this suggests a significant spread in the total spend per customer. The minimum spend is $2.95, indicating at least one very low-value purchase, and the maximum spend is an outlier at $608,821.65, which points to high spending customers.\nThe average length of the relationship between the customers and the company is 273 days, with a standard deviation of about 259 days, reflecting a wide range in the duration of customer relationships. The tenure ranges from new customers (0 days) to those with relationships as long as the oldest transaction in the dataset (738 days).\nThe first quartile (25%) indicates that 25% of customers have interacted with the company within the last 25 days, have a transaction count of 21 or fewer, have spent no more than $348.76, and at least 25% are either new customers or have no recorded tenure.\nThe median (50%) shows that half of the customers have made a purchase within the last 95 days, have a transaction count of 53 or less, and have spent no more than $898.91, with a tenure of up to 220.5 days.\nBy the third quartile (75%), 75% of customers have recency scores as high as 379 days, have made up to 142 transactions, have spent as much as $2,307.09, and have a tenure of up to 511 days.\n\nThe high standard deviations for Monetary Value and Frequency suggest a significant variance in customer spending and engagement, which could indicate a diverse customer base with varying levels of loyalty and value to the company."
  },
  {
    "objectID": "posts/online-retail-clustering/index.html#clustering",
    "href": "posts/online-retail-clustering/index.html#clustering",
    "title": "üë• Data-driven Customer Segmentation",
    "section": "Clustering",
    "text": "Clustering\nWith the RFMT metrics serving as a multi-dimensional profile for each customer, we are now equipped to apply clustering algorithms to segment our customer base into meaningful groups. This subsequent clustering is pivotal, as it aims to reveal the intrinsic patterns and similarities between customers that are not immediately apparent.\nThe clustering process will employ the K-means algorithm, a widely recognized method for its simplicity and effectiveness in partitioning data into k distinct clusters. The algorithm iterates through the dataset, optimizing the positions of the centroids and assigning each data point to the nearest cluster, based on the minimum distance principle. The selection of the optimal number of clusters, k, is of paramount importance and will be informed by the Elbow Method and the Silhouette Score. These metrics will guide us to a value of k that balances within-cluster homogeneity and between-cluster separation, ensuring that our clusters are both statistically significant and practically interpretable.\n\nPreprocessing\nIn preparation for clustering, our dataset requires preprocessing to shape the data into a form amenable to the algorithms we intend to use. Two critical steps in this preprocessing pipeline are log transformation and standardization, particularly relevant when dealing with RFMT data, which often contains skewed distributions and features of varying scales (as we see here below).\n\n\n\n\n\nLog Transformation The RFMT metrics‚Äîespecially monetary value and frequency‚Äîcan exhibit right-skewed distributions, where a small number of high-value customers distort the overall picture. To mitigate this and to normalize the distribution of these attributes, we apply a log transformation. Log transformation dampens the impact of outliers and reduces skewness, making the underlying patterns more apparent and suitable for clustering.\nBefore applying the log transformation, we must ensure that there are no zero or negative values in the dataset, as the log function is not defined for these numbers. In such cases, we can add a constant (like 1) to all values to make the transformation feasible.\nStandardization Post log transformation, we perform standardization of the features. Standardization involves rescaling the data so that it has a mean of zero and a standard deviation of one. This is particularly important for distance-based clustering algorithms like K-means, which are sensitive to the scale of the data. If the features are on different scales, one feature might dominate the distance metric, leading to biased clusters.\nBy standardizing the features, we give each one equal weight in the clustering process. For RFMT data, this ensures that recency, which might be on the scale of days, is just as important as monetary value, which could be several orders of magnitude larger in raw units.\n\nfrom sklearn.preprocessing import StandardScaler\nlog_rfmt = np.log1p(rfmt)\nscaler = StandardScaler()\nscaled_rfmt = pd.DataFrame(scaler.fit_transform(log_rfmt), columns=rfmt.columns, index=rfmt.index)\n\n\n\n\n\n\n\n\nSelection of optimal k\nTo identify the optimal k ‚Äì meaning the number of clusters in which we want to group our customers ‚Äì, we use two primary methods: the Elbow Method with inertia as the metric, and Silhouette Scores.\nElbow Method: In this approach, we plot the inertia against the number of clusters. Inertia, which is a measure of the total distance of points from their respective cluster centroids, typically decreases as we increase the number of clusters. However, the rate of decrease sharply changes at a certain point, forming an ‚Äòelbow‚Äô in the plot. The optimal number of clusters is often considered to be at this ‚Äòelbow‚Äô point, where adding more clusters doesn‚Äôt give much better modeling of the data. This method is intuitive and helps in visualizing the trade-off between the number of clusters and the compactness of the clusters.\nSilhouette Scores: To complement the Elbow Method, we also use Silhouette Scores, which provide a quantitative measure of how well each object lies within its cluster. A high Silhouette Score suggests that the object is well matched to its own cluster and poorly matched to neighboring clusters. By evaluating the average Silhouette Score for different values of k, we can more accurately determine the number of clusters that optimally represents our data.\nThe combination of the Elbow Method using inertia and Silhouette Scores allows for a more comprehensive and reliable determination of the optimal number of clusters in our RFM customer segmentation. This dual approach ensures that we achieve a balance in our clustering model, avoiding both over-segmentation and under-segmentation.\n\n\n\n\n\nFrom the Elbow criterion graph, the optimal number of clusters k is typically chosen at the point where the inertia (sum of squared distances within clusters) begins to decrease more slowly, indicating diminishing returns on the distinctness of additional clusters. In the graph, there is no pronounced ‚Äòelbow,‚Äô but there seems to be a slight leveling off after k=4, suggesting that increasing the number of clusters beyond 4 yields smaller gains in compactness.\nThe Silhouette score provides a measure of how similar an object is to its own cluster compared to other clusters, with higher values indicating better-defined clusters. From the Silhouette scores graph, the score tends to decrease as k increases, which is expected as more clusters can lead to less cohesion within clusters. There is a noticeable uptick at k=5, but the score is still lower than for k=2, k=3, and k=4.\nCombining insights from both the Elbow method and the Silhouette scores, k=4 seems to be a reasonable choice for the optimal number of clusters. At this point, the inertia has not yet flattened out completely, and the Silhouette score, while not the highest, is relatively strong compared to higher k values. Therefore, k=4 could be considered a good balance between cluster distinctness and cohesion for this dataset.\nBut for the purpose of thorough analysis, we will conduct a comparison between k=4 and k=3. This comparison will help us to critically evaluate both statistical and practical aspects of the clustering results. By thoroughly investigating these two potential solutions, we will be able to determine which clustering solution is more appropriate for our customer segmentation.\n\n\nPredictions\nWe are going now to fit the KMeans algorithm with k=3 and k=4, and append the cluster predictions both to the original and scaled rfmt datasets.\n\nrfmt_k = rfmt.copy()\nk_cols = []\nfor k in [3, 4]:\n    model = KMeans(n_clusters=k, random_state=1,  n_init='auto')\n    col_name = f'k{k}'\n    rfmt_k[f'k{k}'] = model.fit_predict(scaled_rfmt)\n    k_cols.append(col_name)\nscaled_rfmt_k = pd.concat([scaled_rfmt, rfmt_k.iloc[:, 4:]], axis=1)\nrfmt_k\n\n\n\n\n\n\n\n\nRecency\nFrequency\nMonetaryValue\nTenure\nk3\nk4\n\n\nCustomerID\n\n\n\n\n\n\n\n\n\n\n12346.0\n325\n34\n77556.46\n400\n1\n1\n\n\n12347.0\n1\n253\n5633.32\n402\n1\n1\n\n\n12348.0\n74\n51\n2019.40\n362\n2\n2\n\n\n12349.0\n18\n175\n4428.69\n570\n1\n1\n\n\n12350.0\n309\n17\n334.40\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n18283.0\n3\n986\n2736.65\n654\n1\n1\n\n\n18284.0\n431\n28\n461.68\n0\n0\n0\n\n\n18285.0\n660\n12\n427.00\n0\n0\n0\n\n\n18286.0\n476\n67\n1296.43\n247\n2\n2\n\n\n18287.0\n42\n155\n4182.99\n528\n1\n1\n\n\n\n\n5878 rows √ó 6 columns\n\n\n\n\n\nModel comparison and interpretation of the clusters\nIn the subsequent analysis, we will delve into the interpretation of customer clusters as delineated by the KMeans clustering algorithm for k=3 and k=4, leveraging both heatmap and snakeplot visualizations. These visual tools are instrumental in uncovering the nuances of customer behavior across different segments. The heatmaps provide a clear, immediate visual representation of the mean values of the RFM (Recency, Frequency, Monetary value) metrics and Tenure for each cluster, highlighting patterns through color intensity ‚Äî darker shades indicate higher mean values within a cluster relative to others. In contrast, the snakeplots will allow us to understand the relative positioning of each cluster along the RFM and Tenure dimensions through a line plot, offering a more detailed and comparative view of the clusters‚Äô profiles. This juxtaposition of visual techniques will enable us to draw a comprehensive picture of the clusters, identify key characteristics that define each segment, and infer the strategic implications for targeted marketing actions.\n\nModel comparison\n\n\n\n\n\n\n\n\n\n\nWhen comparing the clusters between k=3 and k=4, it‚Äôs evident that the introduction of a fourth cluster (the red cluster in k=4) provides a finer segmentation of the customer base, specifically related to the green cluster (of k=3).\n\n\nInterpretation of clusters\nHaving established that the k=4 model offers a more granular and insightful segmentation of the customer base, let‚Äôs proceed with a detailed interpretation of each cluster and propose targeted actions for each.\n\n\n\n\n\nThe following table summarizes the clusters‚Äô features and proposes related business personas.\n\n\n\n\n\n\n\n\n\n\n\n\nCluster\nRecency\nFrequency\nMonetaryValue\nTenure\nSize\nBusiness Persona\n\n\n\n\n0\nHigh\nLow\nLow\nLow\nLarge\nNew Potential Customers\n\n\n1\nLow\nHigh\nHigh\nHigh\nMedium\nLoyal High-Value/Engagement Customers\n\n\n2\nHigh\nMedium\nMedium\nMedium\nLarge\nAt-Risk Medium-Value/Engagement Customers\n\n\n3\nLow\nMedium\nMedium\nMedium\nMedium\nLoyal Medium-Value/Engagement Customers\n\n\n\nNew Potential Customers (Cluster 0)\n\nDescription: This large cluster consists of recently acquired customers with low frequency and monetary value, and with no recent interactions.\nActions: Implement a nurturing strategy to foster these new relationships. Introduce onboarding programs, provide educational content, and send tailored promotions to encourage repeat purchases and increase monetary value.\n\nLoyal High-Value/Engagement Customers (Cluster 1)\n\nDescription: A medium-sized group of customers with high frequency, monetary value, and tenure, showing strong loyalty and engagement.\nActions: Develop retention programs with exclusive rewards, recognize loyalty milestones, and offer referral incentives. Engage them with personalized communication and consider involving them in product development feedback loops to maintain their high engagement.\n\nAt-Risk Medium-Value/Engagement Customers (Cluster 2)\n\nDescription: Another large cluster, these customers have medium values for frequency, monetary value, and tenure but have engaged less recently. They might be at-risk of churn.\nActions: Reactivate these customers with ‚Äúwe miss you‚Äù messages, offering updates on what they‚Äôve missed. Provide incentives that encourage them to revisit and transact, such as limited-time offers or exclusive previews of new products.\n\nLoyal Medium-Value/Engagement Customers (Cluster 3)\n\nDescription: A medium-sized cluster with low recency but medium frequency, monetary value, and tenure, indicating constant engagement.\nActions: Design engagement campaigns highlighting new offerings or loyalty rewards. Conduct surveys to understand their recent interaction and potential risks. Offer incentives for increased frequency and spending.\n\nEach cluster requires a tailored approach that aligns with the behavior patterns identified through the RFM and Tenure analysis. The goal is to enhance customer value across all segments, whether by nurturing new relationships, retaining valuable customers, or re-engaging those showing signs of decreasing engagement. By taking these specific, data-driven actions, the business can optimize its marketing efforts and improve the overall customer lifecycle value."
  },
  {
    "objectID": "posts/titanic/index.html",
    "href": "posts/titanic/index.html",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "",
    "text": "Using a machine learning model to predict the probability of a passenger‚Äôs survival on the Titanic, a Persona has been defined to characterize those who survived."
  },
  {
    "objectID": "posts/titanic/index.html#key-findings-and-achievements",
    "href": "posts/titanic/index.html#key-findings-and-achievements",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "",
    "text": "Using a machine learning model to predict the probability of a passenger‚Äôs survival on the Titanic, a Persona has been defined to characterize those who survived."
  },
  {
    "objectID": "posts/titanic/index.html#introduction",
    "href": "posts/titanic/index.html#introduction",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "Introduction",
    "text": "Introduction\nThe Titanic disaster remains one of the most infamous shipwrecks in history, with over 1500 lives lost. In this notebook, we will explore and analyze a dataset containing information on passengers who were aboard the ship and build a machine learning model to predict which passengers were likely to have survived the tragedy.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nimport shap"
  },
  {
    "objectID": "posts/titanic/index.html#data",
    "href": "posts/titanic/index.html#data",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "Data",
    "text": "Data\nThe variables included in the dataset are:\n\nPassengerId: An integer value representing the unique ID of each passenger on board.\nSurvived: A binary variable indicating whether the passenger survived the sinking of the Titanic or not, with ‚ÄúTrue‚Äù indicating survival and ‚ÄúFalse‚Äù indicating death.\nPclass: An integer variable representing the class of the passenger‚Äôs ticket, with values 1, 2, or 3 (1 being the highest class).\nName: A string variable representing the name of the passenger.\nSex: A string variable representing the gender of the passenger.\nAge: A float variable representing the age of the passenger in years.\nSibSp: An integer variable representing the number of siblings or spouses the passenger had on board.\nParch: An integer variable representing the number of parents or children the passenger had on board.\nTicket: A string variable representing the ticket number of the passenger.\nFare: A float variable representing the fare paid by the passenger for their ticket.\nCabin: A string variable representing the cabin number of the passenger, if applicable.\nEmbarked: A string variable representing the port of embarkation for the passenger, with possible values ‚ÄúS‚Äù (Southampton), ‚ÄúC‚Äù (Cherbourg), or ‚ÄúQ‚Äù (Queenstown).\n\nHere below a glimpse of the dataset:\n\n\nCode\ndf = pd.read_csv('../../data/titanic.csv')\ndf\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows √ó 12 columns\n\n\n\n\nTransformations, feature engineering, preprocessing\nIn this first section we are going to explore and process the given dataset and prepare it for modeling. Let‚Äôs first take a general look at the variables from a structural point-of-view.\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerId column is a useless variable in terms of prediction capabilities. We can set it as dataframe index (or drop it) to keep it out of scope.\n\ndf = df.set_index('PassengerId')\n# df = df.drop(columns=['PassengerId'])\n\nTo give more weight to first class (and vice-versa) Pclass values must be inverted.\n\nconditions = [df['Pclass'] == 1, df['Pclass'] == 3]\nchoices = [3, 1]\ndf['Pclass'] = np.select(conditions, choices, 2)\n\nLet‚Äôs assess now the presence of null values. Percentage of null values per variable:\n\n\nCode\ndisplay((df.isnull().sum() / df.shape[0]).round(3).sort_values(ascending=False))\n\n\nCabin       0.771\nAge         0.199\nEmbarked    0.002\nSurvived    0.000\nPclass      0.000\nName        0.000\nSex         0.000\nSibSp       0.000\nParch       0.000\nTicket      0.000\nFare        0.000\ndtype: float64\n\n\nA part from a negligeable presence of null values in the Embarked variable, we can see that important columns such as Cabin and Age are full of empty observations. Before proceeding with the model development, we‚Äôll have to deal with these inconsistencies.\n\n\nCode\ndf['Cabin'].unique()\n\n\narray([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n       'C148'], dtype=object)\n\n\n\n\nCode\ndisplay(df['Cabin'].value_counts().head(30))\n\n\nCabin\nB96 B98            4\nG6                 4\nC23 C25 C27        4\nC22 C26            3\nF33                3\nF2                 3\nE101               3\nD                  3\nC78                2\nC93                2\nE8                 2\nD36                2\nB77                2\nC123               2\nE121               2\nE44                2\nD35                2\nC125               2\nE67                2\nB35                2\nB18                2\nE24                2\nB49                2\nC65                2\nB20                2\nB5                 2\nB57 B59 B63 B66    2\nC126               2\nB51 B53 B55        2\nF4                 2\nName: count, dtype: int64\n\n\nWe can see that the majority of the cabins in the dataset seem to follow a pattern of a letter followed by a two or three digit number, suggesting that the letter represents the section or deck where the cabin is situated and the number represents the room number. It is reasonable to assume that knowing the section where a passenger‚Äôs cabin is located would provide valuable information about their likelihood of survival. Therefore, it would be beneficial to clean up the column containing cabin information to extract useful insights.\n\ntrans_df = df.copy()\n\n# Splitting Cabin variable into Deck and Room variables\ntrans_df['CabinData'] = trans_df['Cabin'].isnull().apply(lambda x: not x)\ntrans_df['Deck'] = trans_df['Cabin'].str.slice(0,1)\ntrans_df['Room'] = trans_df['Cabin'].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\n\ntrans_df[['Survived', 'CabinData']] = trans_df[['Survived', 'CabinData']].astype(int)\n\n# Label-encoding target variable\ntrans_df['Female'] = np.where(trans_df['Sex'] == 'female', 1, 0)\n\n# Drop useless variables for modeling\ntrans_df = trans_df.drop(columns=['Name', 'Ticket', 'Cabin', 'Sex', 'Room'])\n\n# Create one-hot encoding of the categorical variable\ndf_encoded = pd.get_dummies(trans_df, columns=['Deck', 'Embarked'])\n\n# Impute missing values with KNN imputer\nimputer = KNNImputer(n_neighbors=8)\nimputed_df = imputer.fit_transform(df_encoded)\nimputed_df = pd.DataFrame(imputed_df, columns=df_encoded.columns)\n\nimputed_df\n\n\n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nCabinData\nFemale\nDeck_A\nDeck_B\nDeck_C\nDeck_D\nDeck_E\nDeck_F\nDeck_G\nDeck_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n0.0\n1.0\n22.00\n1.0\n0.0\n7.2500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n3.0\n38.00\n1.0\n0.0\n71.2833\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n1.0\n1.0\n26.00\n0.0\n0.0\n7.9250\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n1.0\n3.0\n35.00\n1.0\n0.0\n53.1000\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n0.0\n1.0\n35.00\n0.0\n0.0\n8.0500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n2.0\n27.00\n0.0\n0.0\n13.0000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n887\n1.0\n3.0\n19.00\n0.0\n0.0\n30.0000\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n888\n0.0\n1.0\n23.75\n1.0\n2.0\n23.4500\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n889\n1.0\n3.0\n26.00\n0.0\n0.0\n30.0000\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n890\n0.0\n1.0\n32.00\n0.0\n0.0\n7.7500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n891 rows √ó 19 columns"
  },
  {
    "objectID": "posts/titanic/index.html#exploratory-analysis",
    "href": "posts/titanic/index.html#exploratory-analysis",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nWe have transformed our dataframe by imputing missing values with a KNNImputer algorythm and one-hot encoded categorical variables. Let‚Äôs now perform an exploratory analysis to look for potential correlations between our target variable and the others.\n\n\nCode\nax = sns.countplot(x=\"Pclass\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\nplt.title(\"Perished / Survived per `Pclass`\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n42% of perished people is coming from the third class.\n\n\nCode\nax = sns.countplot(x=\"Female\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1], ['Men', 'Women'])\nplt.title(\"Perished / Survived per sex\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\nThe majority of perished people are men, while the majority of survived are women.\n\n\nCode\ncatplot = sns.catplot(data=imputed_df, y=\"Age\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Age` and `Pclass`', y=1.05)\nplt.show()\n\ncatplot = sns.catplot(data=imputed_df, y=\"Fare\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Fare` and `Pclass`', y=1.05)\nplt.show()\n\n\n\n\n\n\n\n\nAlready from these exploratory visualizations, we can easily see that the majority of survived people are women and people from the top classes. Will this be confirmed by the model?\n\n\nCode\nsurvived_corr = imputed_df.corr()['Survived'].sort_values(ascending=False)[1:]\ndisplay(survived_corr)\n\n\nFemale        0.543351\nPclass        0.338481\nCabinData     0.316912\nFare          0.257307\nDeck_B        0.175095\nEmbarked_C    0.168240\nDeck_D        0.150716\nDeck_E        0.145321\nDeck_C        0.114652\nParch         0.081629\nDeck_F        0.057935\nDeck_A        0.022287\nDeck_G        0.016040\nEmbarked_Q    0.003650\nDeck_T       -0.026456\nSibSp        -0.035322\nAge          -0.092717\nEmbarked_S   -0.155660\nName: Survived, dtype: float64\n\n\n\n\nCode\nfor i in range(0, len(survived_corr.index), 6):\n    chunk = survived_corr.index[i:i+6]\n    sns.pairplot(imputed_df, y_vars=['Survived'], x_vars=chunk, kind='reg')"
  },
  {
    "objectID": "posts/titanic/index.html#model-development",
    "href": "posts/titanic/index.html#model-development",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "Model development",
    "text": "Model development\nWe begin by splitting the dataset into train and test sets and instantiate a KFold object for cross validation to avoid overfitting. Several trial and errors suggested to set the test size to 0.4, giving the best balance between cross validation score and test set accuracy score for the models.\n\n# Split the data into training and test sets\nmodel_df = imputed_df.drop(columns=['CabinData'])\nX = model_df.drop(columns=['Survived'])\ny = model_df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n\n# instantiate a KFold object for cross validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\nModel selection\nThen we compare three classifiers and choose the best one in terms of accuracy.\n\n# Define sklearn pipelines for classifier comparison\n\nlogistic_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('logistic', LogisticRegression(max_iter=1000))\n])\n\nrf_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\nsvm_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\npipelines = [logistic_pipeline, rf_pipeline, svm_pipeline]\nfor pipeline in pipelines:\n    scores = cross_val_score(pipeline, X_train, y_train, cv=kf)\n    print(f\"Pipeline: {pipeline.named_steps.keys()} Mean Cross-Validation Score: {scores.mean().round(3)}\")\n    \n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    print(f\"Test accuracy score: {round(accuracy_score(y_pred, y_test), 3)}\")\n    \n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    cmd = ConfusionMatrixDisplay(cm)\n    fig, ax = plt.subplots()\n    cmd.plot(ax=ax, cmap=plt.cm.Blues)\n    ax.set_title(f\"Confusion Matrix\\n{pipeline[0]}\")\n    plt.show()\n\nPipeline: dict_keys(['logistic']) Mean Cross-Validation Score: 0.792\nTest accuracy score: 0.821\nPipeline: dict_keys(['rf']) Mean Cross-Validation Score: 0.8\nTest accuracy score: 0.798\nPipeline: dict_keys(['svm']) Mean Cross-Validation Score: 0.695\nTest accuracy score: 0.65\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the classification model with the highest accuracy score (mean cross-validation score) is the Random Forest Classifier. We are going to use this specific model to predict survival on the Titanic.\n\n\nPrediction with Random Forest Classifier\nAfter having chosen the right classifier for the job, we train it on the train set with GridSearchCV, that performs an exhaustive search over a specified hyperparameter space for an estimator. It takes as input an estimator, a dictionary of hyperparameters, and a cross-validation strategy. It then fits the estimator on all possible combinations of hyperparameters and evaluates the performance of each combination using cross-validation.\nThe GridSearchCV class is useful for finding the best hyperparameters for a machine learning model. By testing all possible combinations of hyperparameters, it can help to identify the combination that results in the best performance on a given dataset.\n\nparam_grid = {\n    'rf__n_estimators': [10, 50, 100],\n    'rf__max_depth': [None, 5, 10],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid, cv=kf)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\ny_pred = grid_search.predict(X_test)\nprint(\"Test set accuracy:\", grid_search.score(X_test, y_test))\n\nestimator = grid_search.best_estimator_.named_steps['rf']\n\n # Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ncmd = ConfusionMatrixDisplay(cm)\nfig, ax = plt.subplots()\ncmd.plot(ax=ax, cmap=plt.cm.Blues)\nax.set_title(f\"Confusion Matrix\\n{estimator}\")\nplt.show()\n\nBest parameters: {'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 50}\nBest score: 0.8294657026979368\nTest set accuracy: 0.8207282913165266\n\n\n\n\n\nWe have taken the best performing algorithm and trained it on our dataset with a GridSearchCV and KFold cross validation. The perfomance on the test set suggests that the chosen classification model is able to predict Survival with an 82% of accuracy. As a final step, we are going to explain the variables which have been taken into consideration by the classifier in the prediction, and draw conclusions on the ‚ÄúSurvived Persona‚Äù.\n\nshap_values = shap.TreeExplainer(estimator).shap_values(X_test)\nshap.summary_plot(shap_values[1], X_test)\n\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\nThe SHAP (SHapley Additive exPlanations) summary plot is a powerful tool for interpreting the output of a tree-based model, such as a Random Forest Classifier, using a TreeExplainer. The plot displays the features in order of their importance, as measured by their mean absolute SHAP values. Here‚Äôs how to interpret the SHAP summary plot for a TreeExplainer:\n\nFeature importance: The features are listed along the y-axis of the plot, with the most important features at the top. The importance of each feature is represented by its mean absolute SHAP value, which is indicated by the horizontal bars.\nImpact on prediction: The color of the bars represents the direction and magnitude of the feature‚Äôs impact on the model‚Äôs prediction. Red bars indicate a positive impact on the prediction, while blue bars indicate a negative impact. The length of the bar represents the magnitude of the impact, with longer bars indicating a greater impact.\nRelationship between feature and prediction: The position of each bar relative to the center line indicates the direction and strength of the relationship between the feature and the prediction. Features that push the prediction towards the top of the plot are associated with higher predicted probabilities, while features that push the prediction towards the bottom are associated with lower predicted probabilities.\nInteractions between features: The spacing between the bars for each feature represents the degree of interaction between that feature and other features in the model. If the bars for two features are close together, it indicates that those features tend to interact with each other in the model."
  },
  {
    "objectID": "posts/titanic/index.html#conclusion",
    "href": "posts/titanic/index.html#conclusion",
    "title": "üö¢ Understanding Survival on the Titanic",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest Classifier trained on our dataset was capable of predicting Survival on the Titanic with a 82% of accuracy. The best predictors identified by the SHAP Tree Explainer are:\n\nFemale: whether the person was female or not. The red dots (representing female persons) are all gathered on the right side of the plot, meaning that they have a strong positive impact on survival.\nPclass: passenger class. 1st class passengers (red dots) are located in the right size of the graph. Being a 1st class passenger would have meant sure survival.\nAge: older persons, represented by red dots are all gathered in the left (death) side of the plot, inversely, younger people were more prone to be saved.\nFare: similar situation for fare. Higher fares (red dots) are all located in the right side, meaning that people who paid for more expensive tickets had better chances to survive.\n\nAn interesting insights that SHAP gives us is regarding the Embarked_S. This dummy variable tells us if the passenger was embarked in Southampton. We can see that these passengers (red dots) had practically no chance of being saved. Maybe because they were all 3rd class?\nTo conclude, we can say that the Survived Persona was a young girl, coming from the top classes (having paid a higher fare) and not embarked in Southampton."
  },
  {
    "objectID": "posts/motogp-performance/index.html",
    "href": "posts/motogp-performance/index.html",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "",
    "text": "For the purpose of providing more technical insights to fans, we demonstrate how public race data can be used to extract important information on race and rider performance.\nWe demonstrate how to quantify the performances of riders, teams, and constructors through quantitative indicators that take into account relative speed and consistency, the two fundamental variables in a theoretical definition of race performance."
  },
  {
    "objectID": "posts/motogp-performance/index.html#key-findings-and-achievements",
    "href": "posts/motogp-performance/index.html#key-findings-and-achievements",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "",
    "text": "For the purpose of providing more technical insights to fans, we demonstrate how public race data can be used to extract important information on race and rider performance.\nWe demonstrate how to quantify the performances of riders, teams, and constructors through quantitative indicators that take into account relative speed and consistency, the two fundamental variables in a theoretical definition of race performance."
  },
  {
    "objectID": "posts/motogp-performance/index.html#introduction",
    "href": "posts/motogp-performance/index.html#introduction",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "Introduction",
    "text": "Introduction\nSports are being radically transformed by data collection and analysis, benefiting both professionals and fans. Soccer, baseball, and American football are some examples where player performance data is utilized to acquire talent and provide in-depth statistics for the public. In contrast, data analysis for motorsports fans is not yet widespread, despite being fundamental to the sport‚Äôs operation.\nIn this notebook, we will explore the realm of MotoGP racing and analyze publicly available race data with the aim of comprehending how it can be used to provide fans with essential insights on riders‚Äô performance. By employing statistical methodologies, we will extract as much insights as possible from the openly accessible data, contending that it is possible to gauge the riders‚Äô objective performances within a race and across the championship. At the same time, we will propose potential data analyses ‚Äì presented as a ‚Äúpost-GP‚Äù information product ‚Äì that can be used by the media to interpret Grand Prix outcomes and which can spark engaging discussions on the competitors‚Äô performance levels and progress.\n\nData is provided thanks to the motogpdata project.\n\n\n\nCode\nimport motogpdata.handler as mgphandler\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/motogp-performance/index.html#race-results",
    "href": "posts/motogp-performance/index.html#race-results",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "Race Results",
    "text": "Race Results\nFor the scope of this study, we will take into account and examine two races from the 2022 MotoGP season, namely the Qatar Grand Prix and the Italian Grand Prix. Let‚Äôs first refresh our memory of the results of these races.\nFor the Qatar Grand Prix:\n\n\nCode\nseason = mgphandler.Season(2022, 'MotoGP')\nqat2022 = mgphandler.Event(season, 'QAT')\nita2022 = mgphandler.Event(season, 'ITA')\n\nresults_subset = ['rider.full_name', 'position', 'average_speed', 'total_laps', 'time', 'points']\nqat2022.results()[results_subset].set_index('rider.full_name')\n\n\n\n\n\n\n\n\n\nposition\naverage_speed\ntotal_laps\ntime\npoints\n\n\nrider.full_name\n\n\n\n\n\n\n\n\n\nEnea Bastianini\n1.0\n168.2\n22\n42:13.1980\n25\n\n\nBrad Binder\n2.0\n168.1\n22\n42:13.5440\n20\n\n\nPol Espargaro\n3.0\n168.1\n22\n42:14.5490\n16\n\n\nAleix Espargaro\n4.0\n168.0\n22\n42:15.4400\n13\n\n\nMarc Marquez\n5.0\n167.9\n22\n42:17.2970\n11\n\n\nJoan Mir\n6.0\n167.8\n22\n42:18.0410\n10\n\n\nAlex Rins\n7.0\n167.6\n22\n42:22.0080\n9\n\n\nJohann Zarco\n8.0\n167.5\n22\n42:23.7340\n8\n\n\nFabio Quartararo\n9.0\n167.5\n22\n42:23.7410\n7\n\n\nTakaaki Nakagami\n10.0\n167.2\n22\n42:28.1650\n6\n\n\nFranco Morbidelli\n11.0\n167.1\n22\n42:29.9100\n5\n\n\nMaverick Vi√±ales\n12.0\n166.6\n22\n42:36.4140\n4\n\n\nLuca Marini\n13.0\n166.4\n22\n42:40.4810\n3\n\n\nAndrea Dovizioso\n14.0\n166.4\n22\n42:40.5720\n2\n\n\nRemy Gardner\n15.0\n165.5\n22\n42:54.3050\n1\n\n\nDarryn Binder\n16.0\n165.5\n22\n42:54.3170\n0\n\n\nFabio Di Giannantonio\n17.0\n165.5\n22\n42:54.5470\n0\n\n\nRaul Fernandez\n18.0\n165.4\n22\n42:55.5550\n0\n\n\nFrancesco Bagnaia\nNaN\n167.2\n11\n21:13.8350\n0\n\n\nJorge Martin\nNaN\n167.2\n11\n21:13.8000\n0\n\n\nMiguel Oliveira\nNaN\n166.9\n10\n19:19.8860\n0\n\n\nAlex Marquez\nNaN\n166.6\n9\n17:25.7820\n0\n\n\nJack Miller\nNaN\n165.4\n6\n11:42.3060\n0\n\n\nMarco Bezzecchi\nNaN\n165.6\n6\n11:41.3210\n0\n\n\n\n\n\n\n\nFor the Italian Grand Prix:\n\n\nCode\nita2022.results()[results_subset].set_index('rider.full_name')\n\n\n\n\n\n\n\n\n\nposition\naverage_speed\ntotal_laps\ntime\npoints\n\n\nrider.full_name\n\n\n\n\n\n\n\n\n\nFrancesco Bagnaia\n1.0\n175.1\n23\n41:18.9230\n25\n\n\nFabio Quartararo\n2.0\n175.1\n23\n41:19.5580\n20\n\n\nAleix Espargaro\n3.0\n175.0\n23\n41:20.9060\n16\n\n\nJohann Zarco\n4.0\n175.0\n23\n41:21.5130\n13\n\n\nMarco Bezzecchi\n5.0\n174.9\n23\n41:21.9900\n11\n\n\nLuca Marini\n6.0\n174.9\n23\n41:22.7980\n10\n\n\nBrad Binder\n7.0\n174.9\n23\n41:22.9900\n9\n\n\nTakaaki Nakagami\n8.0\n174.4\n23\n41:29.8670\n8\n\n\nMiguel Oliveira\n9.0\n174.3\n23\n41:30.1790\n7\n\n\nMarc Marquez\n10.0\n174.3\n23\n41:30.7230\n6\n\n\nFabio Di Giannantonio\n11.0\n174.2\n23\n41:31.8390\n5\n\n\nMaverick Vi√±ales\n12.0\n174.2\n23\n41:31.8400\n4\n\n\nJorge Martin\n13.0\n173.9\n23\n41:36.1630\n3\n\n\nAlex Marquez\n14.0\n173.9\n23\n41:36.4910\n2\n\n\nJack Miller\n15.0\n173.9\n23\n41:36.6100\n1\n\n\nDarryn Binder\n16.0\n173.7\n23\n41:39.1880\n0\n\n\nFranco Morbidelli\n17.0\n173.7\n23\n41:39.2190\n0\n\n\nMichele Pirro\n18.0\n173.6\n23\n41:40.2280\n0\n\n\nRemy Gardner\n19.0\n173.0\n23\n41:49.4710\n0\n\n\nAndrea Dovizioso\n20.0\n173.0\n23\n41:49.9340\n0\n\n\nRaul Fernandez\n21.0\n172.2\n23\n42:01.6460\n0\n\n\nLorenzo Savadori\n22.0\n164.9\n22\n41:58.6110\n0\n\n\nEnea Bastianini\nNaN\n174.8\n13\n23:24.0940\n0\n\n\nJoan Mir\nNaN\n172.8\n7\n12:44.7980\n0\n\n\nAlex Rins\nNaN\n173.5\n7\n12:41.6070\n0\n\n\nPol Espargaro\nNaN\n171.2\n4\n07:21.0900\n0\n\n\n\n\n\n\n\nThe traditional race classification tables (which represent the only type of data considered by most of the media when convering races) do not provide many analytical insights nor objective indicators of riders performances. They merely offer a snapshot of the race outcome; and the final result does not provide a comprehensive evaluation of the competitors‚Äô achievements in the race.\nIt is in fact possible for a rider to finish a race in a certain position thanks to the mistakes of others, even if he has maintained an anonymous performance throughout the competition. In such situations, the most common question that arises is whether or not the rider ‚Äúdeserved‚Äù or not the position he achieved at the end of the race. In other words, we ask: what was the rider‚Äôs performance after accounting for the race events? How can we measure and evaluate his performance regardless of the result obtained?"
  },
  {
    "objectID": "posts/motogp-performance/index.html#laptime-analysis",
    "href": "posts/motogp-performance/index.html#laptime-analysis",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "Laptime analysis",
    "text": "Laptime analysis\nA statistical assessment of race performance is required to address these inquiries, which entails an evaluation based on factual data rather than subjective sentiments or judgments. To accomplish this, the data on lap times must be taken into account, scrutinizing the race‚Äôs overall progress. An illustration of such data for the Qatar race is presented below.\n\n\nCode\n# Extract race analysis data for the Qatar Grand Prix\ntimesheetQAT, laptimes_ridersQAT, laptimes_teamsQAT, laptimes_constructorsQAT, performanceQAT = qat2022.race_analysis(save_pdf=False, performance=True)\nlaptimes_ridersQAT_melted = laptimes_ridersQAT.melt(id_vars='lap', var_name='rider', value_name='laptime')\n\nlaptimes_ridersQAT.head()\n\n\n\n\n\n\n\n\nrider\nAleix Espargaro\nAlex Marquez\nAlex Rins\nAndrea Dovizioso\nBrad Binder\nDarryn Binder\nEnea Bastianini\nFabio Di Giannantonio\nFabio Quartararo\nFrancesco Bagnaia\n...\nMarc Marquez\nMarco Bezzecchi\nMaverick Vi√±ales\nMiguel Oliveira\nPol Espargaro\nRaul Fernandez\nRemy Gardner\nTakaaki Nakagami\navg_laptime\nlap\n\n\nlap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n121.559\n122.479\n122.363\n123.815\n121.021\n124.531\n121.438\n125.299\n122.042\n121.817\n...\n120.713\n122.419\n124.207\n122.558\n120.669\n125.229\n125.051\n122.755\n122.863542\n1\n\n\n2\n115.237\n116.054\n115.487\n116.887\n115.086\n116.721\n114.953\n116.269\n115.415\n115.135\n...\n115.265\n116.229\n116.618\n116.259\n115.181\n116.272\n116.173\n115.975\n115.839625\n2\n\n\n3\n114.797\n115.223\n114.610\n115.858\n114.846\n115.720\n114.918\n115.982\n115.007\n114.771\n...\n114.875\n115.636\n116.134\n115.488\n114.797\n116.531\n115.974\n115.909\n115.377417\n3\n\n\n4\n115.000\n115.329\n114.706\n115.860\n115.069\n115.591\n114.991\n116.212\n115.171\n115.223\n...\n115.032\n116.318\n116.044\n115.057\n115.076\n116.106\n116.167\n116.029\n115.478208\n4\n\n\n5\n114.944\n115.328\n115.013\n117.290\n114.867\n116.474\n114.956\n116.228\n115.236\n115.105\n...\n114.867\n115.520\n116.015\n115.097\n114.753\n116.326\n116.210\n115.380\n115.526708\n5\n\n\n\n\n5 rows √ó 26 columns\n\n\n\nThe above table (in its comprehensive form) shows the lap times of each rider (expressed in seconds), and provides already a good starting point for deepening the understanding of the time trends and the general race progression. From such data, it is possible to generate some of the following visualizations.\n\n\nCode\nsns.relplot(data=laptimes_ridersQAT_melted[laptimes_ridersQAT_melted['rider'].isin(\n    ['avg_laptime', 'Enea Bastianini', 'Aleix Espargaro', 'Raul Fernandez'])], \n            x='lap', y='laptime', hue='rider', kind='line', \n            palette=['green', 'red', 'darkorange', 'grey'], aspect=1.5)\n\nplt.title(\"Qatar 2022\\nComparative Lap Time Analysis - Riders\")\nplt.xlabel(\"Laps\")\nplt.ylabel(\"Lap times (in seconds)\")\nplt.show()\n\n\n\n\n\nBy projecting the average lap time (gray line), we can appreciate the general trend of the race. For example, in the case of Qatar, there is a decline in overall performance starting from the seventeenth lap. Similarly, there is a significant performance drop in the Italian Grand Prix race starting from the sixteenth lap. This analysis helps to evaluate tire performance.\nRegarding the personal lap times of the riders, we can notice that both Enea Bastianini (who arrived in 1st place) and Aleix Espargaro (who arrived in 4th place) performed well below the average (gray line), unlike Raul Fernandez, for example (orange line), who consistently raced above the general average of the competition. Similar considerations can also be applied to the Italian Grand Prix, as shown below.\n\n\nCode\ntimesheetITA, laptimes_ridersITA, laptimes_teamsITA, laptimes_constructorsITA, performanceITA = ita2022.race_analysis(save_pdf=False, performance=True)\nlaptimes_ridersITA_melted = laptimes_ridersITA.melt(id_vars='lap', var_name='rider', value_name='laptime')\n\nsns.relplot(data=laptimes_ridersITA_melted[laptimes_ridersITA_melted['rider'].isin(\n    ['avg_laptime', 'Francesco Bagnaia', 'Fabio Quartararo', 'Andrea Dovizioso'])], \n            x='lap', y='laptime', hue='rider', kind='line', \n            palette=['aqua', 'blue', 'red', 'grey'], aspect=1.5)\n\nplt.title(\"Mugello 2022\\nComparative Lap Time Analysis - Riders\")\nplt.xlabel(\"Laps\")\nplt.ylabel(\"Lap times (in seconds)\")\nplt.show()\n\n\n\n\n\nThese graphs provide a visual and intuitive way to determine if a particular rider, team, or manufacturer has performed above, below, or at an average level. (It‚Äôs important to note that the more a rider‚Äôs lap times are below the average race‚Äôs lap time, the faster they‚Äôve gone). Using the same visualization concept for the constructors, we can see how Yamaha and KTM consistently performed above average in the Qatar 2022 race, thus indicating that they ‚Äúunderperformed‚Äù compared to the other manufacturers.\n\n\nCode\nlaptimes_constructorsQAT_melted = laptimes_constructorsQAT.melt(id_vars='lap', var_name='constructor', value_name='laptime')\n\nsns.relplot(data=laptimes_constructorsQAT_melted[laptimes_constructorsQAT_melted['constructor'].isin(\n    ['avg_laptime', 'Yamaha', 'KTM'])], \n            x='lap', y='laptime', hue='constructor', kind='line', \n            palette=['darkorange', 'blue', 'grey'], aspect=1.5)\n\nplt.title(\"Qatar 2022\\nComparative Lap Time Analysis - Constructors\")\nplt.xlabel(\"Laps\")\nplt.ylabel(\"Lap times (in seconds)\")\nplt.show()\n\n\n\n\n\n\nSector times\nIn an aggregate manner, it is also possible to determine variation indicators for each sector of the circuit, thereby appreciating the overall level of consistency of the riders in the various sections of the track. For example, we can observe that in the case of the Qatar Grand Prix, the consistency of lap times in different sectors is almost identical, while at Mugello, the sector in which the riders were on average more consistent is T2.\n\n\nCode\nstd_race = pd.DataFrame(timesheetQAT[timesheetQAT['lap'] != 1][['t1', 't2', 't3', 't4']].std())\nstd_race = std_race.pivot_table(columns=std_race.index).round(2)\nstd_race.index = ['œÉ']\nprint(\"üá∂üá¶ Qatar Grand Prix 2022\\nSector Pace:\")\nstd_race\n\n\nüá∂üá¶ Qatar Grand Prix 2022\nSector Pace:\n\n\n\n\n\n\n\n\n\nt1\nt2\nt3\nt4\n\n\n\n\nœÉ\n0.21\n0.23\n0.21\n0.21\n\n\n\n\n\n\n\n\n\nCode\nstd_race = pd.DataFrame(timesheetITA[timesheetITA['lap'] != 1][['t1', 't2', 't3', 't4']].std())\nstd_race = std_race.pivot_table(columns=std_race.index).round(2)\nstd_race.index = ['œÉ']\nprint(\"üáÆüáπ Italian Grand Prix 2022\\nSector pace:\")\nstd_race\n\n\nüáÆüáπ Italian Grand Prix 2022\nSector pace:\n\n\n\n\n\n\n\n\n\nt1\nt2\nt3\nt4\n\n\n\n\nœÉ\n0.46\n0.24\n0.74\n0.45\n\n\n\n\n\n\n\nThe same principle of analysis can be applied to the fastest times set by riders in the different sectors of the track. At the Mugello Grand Prix, we can see, for example, how Yamaha, with Fabio Quartararo, was the fastest in T2 and T3. In contrast, the Ducatis of Bagnaia and Bastianini expressed the best times in the sectors with higher top speeds (T1 and T4) due to the long straights.\n\n\nCode\nby_rider = timesheetITA[timesheetITA['lap'] != 1].groupby(['constructor', 'rider'])[['t1', 't2', 't3', 't4']].min().round(3)\nrider_with_min = by_rider.idxmin()\nconst_t = pd.DataFrame({\n    'rider': rider_with_min, \n    'fastest_time': by_rider.min()})\nconst_t[['constructor', 'rider']] = pd.DataFrame(const_t['rider'].tolist(), index=const_t.index)\nprint(\"üáÆüáπ Italian Grand Prix 2022\\nFastest sector times:\\n\")\nprint(const_t[['constructor', 'rider', 'fastest_time']])\n\n\nüáÆüáπ Italian Grand Prix 2022\nFastest sector times:\n\n   constructor              rider  fastest_time\nt1      Ducati  Francesco Bagnaia        24.868\nt2      Yamaha   Fabio Quartararo        22.499\nt3      Yamaha   Fabio Quartararo        34.842\nt4      Ducati    Enea Bastianini        23.998\n\n\nIn general, analyzing sector times under different measures of aggregation (average time, minimum, maximum, consistency in terms of standard deviation, etc.) can reveal interesting points of comparison to evaluate the adaptation to the track of riders, teams, and constructors."
  },
  {
    "objectID": "posts/motogp-performance/index.html#performance-analysis",
    "href": "posts/motogp-performance/index.html#performance-analysis",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "Performance analysis",
    "text": "Performance analysis\nAlthough an analysis of times like this already gives a clearer idea of the general trend of the race, it alone cannot provide an answer to our initial question. In fact, to quantify riders‚Äô performances, it is necessary to deepen the analysis and develop quantitative indicators.\n\nThe Performance Matrix\nMotorsports performance, can be defined as ‚Äúhow consistently fast a rider/driver has been‚Äù - relative to the number of laps completed. Therefore, at a conceptual level, the so-called ‚Äúpace‚Äù can be theorized as the aggregation of two fundamental variables: consistency and speed. To achieve high positions in the rankings, a rider must in fact be both fast and consistent.\nVisually, the relationship between these two variables can be demonstrated with the help of a matrix - shown below - which we will call ‚Äúperformance matrix‚Äù. The graph represents the ratio between the two indicators (relative speed on the y-axis and relative consistency on the x-axis) for each participant in the race, in relation to the average located at the axes \\(x,y=0\\) (black lines). This means that the performance of the riders located in the upper-right quadrant of the graph are above average, both in terms of speed and consistency. Conversely, those located in the lower-left quadrant have ‚Äúunderperformed‚Äù below average (for both variables).\n\n\nCode\nperf_cols = ['rider', 'constructor', 'grid', 'position', 'pos_delta', 'points', 'avg_laptime', 'avg_speed', 'delta_avg_laptime', 'pace_speed_index', 'delta_std_laptime', 'pace_consistency_index', 'performance_index']\nperformanceQAT = performanceQAT[perf_cols].set_index('rider').round(2)\nperformanceITA = performanceITA[perf_cols].set_index('rider').round(2)\nperformanceITA.loc[performanceITA.index == 'Lorenzo Savadori', 'constructor'] = 'Aprilia'\n\nsns.relplot(data=performanceQAT, \n            x='pace_consistency_index', \n            y='pace_speed_index', hue='constructor', \n            palette=['red', 'darkorange', 'orange', 'green', 'aqua', 'blue'], \n            # size='points', \n            # sizes=[30, 80], \n            # alpha=0.8, \n            legend='brief', aspect=1.5)\n\nplt.title(\"Qatar 2022\\nPerformance Matrix - Riders\")\nplt.xlabel(\"Relative Consistency Indicator (in seconds)\")\nplt.ylabel(\"Relative Speed Indicator (in seconds)\")\n\nplt.axhline(y=0, color='black', linestyle='-', linewidth=2)\nplt.axvline(x=0, color='black', linestyle='-', linewidth=2)\n\nplt.show()\n\n\n\n\n\nIn the case of Qatar, we can see how this graph helps us clearly visualize the struggle of the Ducati teams in having a consistent pace (with the exception of the winner, Bastianini). The Suzuki team had a good overall performance, both in terms of speed and consistency.\n\n\nCode\nsns.relplot(data=performanceITA[performanceITA['performance_index'] &gt; -3], \n            x='pace_consistency_index', \n            y='pace_speed_index', hue='constructor', \n            palette=['red', 'blue', 'green', 'darkorange', 'orange', 'aqua'], \n            # size='points', \n            # sizes=[30, 80], \n            # alpha=0.8, \n            legend='brief', aspect=1.5)\n\nplt.title(\"Mugello 2022\\nPerformance Matrix - Riders\")\nplt.xlabel(\"Relative Consistency Indicator (in seconds)\")\nplt.ylabel(\"Relative Speed Indicator (in seconds)\")\n\nplt.axhline(y=0, color='black', linestyle='-', linewidth=2)\nplt.axvline(x=0, color='black', linestyle='-', linewidth=2)\n\nplt.show()\n\n\n\n\n\nAt Mugello, it‚Äôs interesting to note how the riders who scored points are all grouped in the upper-right quadrant, which means their values for speed and consistency were above average (confirming the demonstrated relationship between the two variables). Only Quartararo (who finished second) demonstrated both consistency and speed among the Yamaha riders, while the other three Yamahas suffered from a noticeable lack of speed that prevented them from scoring points. The Ducati team had a good overall performance, with most of their riders largely confirming their high performance usually demonstrated at their home circuit.\n\n\nA performance-based ranking\nThe two variables quantifying the race pace can be combined to create an index that summarizes a rider‚Äôs performance in a single value. We will call this a ‚Äúperformance indicator‚Äù: an aggregation of the speed and consistency values of a competitor.\nThe following table shows the results of the Qatar race along with these indicators. Specifically, from left to right, it includes the starting position, finishing position, difference in positions at the finish, points earned, speed indicator, consistency indicator, and the aggregated performance indicator. This results table, ordered by the latter variable, presents a ‚Äúrevisited‚Äù ranking based on the performance demonstrated by the riders regardless of the final result and events of the race.\n\n\nCode\nperfQAT = performanceQAT.drop(columns=[\n    'avg_speed', 'avg_laptime', 'delta_avg_laptime',\n    'delta_std_laptime']).sort_values('performance_index', ascending=False).reset_index()\nperfQAT.index += 1\nperfQAT.drop(columns='constructor')\n\n\n\n\n\n\n\n\n\nrider\ngrid\nposition\npos_delta\npoints\npace_speed_index\npace_consistency_index\nperformance_index\n\n\n\n\n1\nBrad Binder\n7\n2.0\n5.0\n20\n0.87\n0.51\n0.69\n\n\n2\nPol Espargaro\n6\n3.0\n3.0\n16\n0.83\n0.52\n0.67\n\n\n3\nEnea Bastianini\n2\n1.0\n1.0\n25\n0.89\n0.39\n0.64\n\n\n4\nMarc Marquez\n3\n5.0\n-2.0\n11\n0.70\n0.59\n0.64\n\n\n5\nAleix Espargaro\n5\n4.0\n1.0\n13\n0.79\n0.39\n0.59\n\n\n6\nJoan Mir\n8\n6.0\n2.0\n10\n0.67\n0.51\n0.59\n\n\n7\nFabio Quartararo\n11\n9.0\n2.0\n7\n0.41\n0.38\n0.40\n\n\n8\nAlex Rins\n10\n7.0\n3.0\n9\n0.49\n0.27\n0.38\n\n\n9\nTakaaki Nakagami\n6\n10.0\n-4.0\n6\n0.21\n0.25\n0.23\n\n\n10\nJohann Zarco\n3\n8.0\n-5.0\n8\n0.41\n-0.06\n0.17\n\n\n11\nFranco Morbidelli\n12\n11.0\n1.0\n5\n0.13\n0.08\n0.10\n\n\n12\nFrancesco Bagnaia\n9\nNaN\n-9.0\n0\n0.12\n-0.09\n0.01\n\n\n13\nJorge Martin\n1\nNaN\n-17.0\n0\n0.12\n-0.28\n-0.08\n\n\n14\nMaverick Vi√±ales\n9\n12.0\n-3.0\n4\n-0.17\n-0.04\n-0.10\n\n\n15\nMiguel Oliveira\n4\nNaN\n-14.0\n0\n0.02\n-0.23\n-0.11\n\n\n16\nAndrea Dovizioso\n10\n14.0\n-4.0\n2\n-0.36\n0.11\n-0.12\n\n\n17\nLuca Marini\n7\n13.0\n-6.0\n3\n-0.35\n0.11\n-0.12\n\n\n18\nAlex Marquez\n8\nNaN\n-10.0\n0\n-0.07\n-0.22\n-0.14\n\n\n19\nMarco Bezzecchi\n5\nNaN\n-13.0\n0\n-0.27\n-0.22\n-0.25\n\n\n20\nJack Miller\n4\nNaN\n-14.0\n0\n-0.23\n-0.35\n-0.29\n\n\n21\nDarryn Binder\n14\n16.0\n-2.0\n0\n-0.98\n0.07\n-0.45\n\n\n22\nRemy Gardner\n12\n15.0\n-3.0\n1\n-0.98\n-0.03\n-0.50\n\n\n23\nFabio Di Giannantonio\n11\n17.0\n-6.0\n0\n-0.99\n-0.06\n-0.53\n\n\n24\nRaul Fernandez\n13\n18.0\n-5.0\n0\n-1.04\n-0.05\n-0.55\n\n\n\n\n\n\n\nIn this ‚Äúnew‚Äù ranking (sorted by the aggregated performance indicator), it is interesting to note that the winner (Enea Bastianini), who dominated the race from the first position since the beginning, is relegated to the third position behind Brad Binder and Pol Espargaro. This is due to the fact that these two riders have shown to have a greater speed and consistency than Enea, who, apart from the victory, has equalled the performance of Marc Marquez.\nThe same concept is applied to the race performances of the constructors in the following table.\n\n\nCode\nperfQAT_constr = perfQAT.groupby('constructor')[['pace_speed_index', 'pace_consistency_index', 'performance_index']].mean().sort_values('performance_index', ascending=False).round(2)\n# perfQAT_constr = perfQAT_constr.sort_values('performance_index', ascending=False).reset_index()\n# perfQAT_constr.index += 1\ndisplay(perfQAT_constr)\n\nsns.relplot(data=perfQAT_constr, \n            x='pace_consistency_index', \n            y='pace_speed_index', hue='constructor', \n            palette=['aqua', 'orange', 'green', 'blue', 'red', 'darkorange'], \n            # size='points', \n            # sizes=[30, 80], \n            # alpha=0.8, \n            legend='brief', aspect=1.5)\n\nplt.title(\"Qatar 2022\\nPerformance Matrix - Constructors\")\nplt.xlabel(\"Relative Consistency Indicator (in seconds)\")\nplt.ylabel(\"Relative Speed Indicator (in seconds)\")\n\nplt.axhline(y=0, color='black', linestyle='-', linewidth=2)\nplt.axvline(x=0, color='black', linestyle='-', linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\n\npace_speed_index\npace_consistency_index\nperformance_index\n\n\nconstructor\n\n\n\n\n\n\n\nSuzuki\n0.58\n0.39\n0.48\n\n\nHonda\n0.42\n0.28\n0.35\n\n\nAprilia\n0.31\n0.18\n0.24\n\n\nYamaha\n-0.20\n0.16\n-0.02\n\n\nDucati\n-0.04\n-0.07\n-0.06\n\n\nKTM\n-0.28\n0.05\n-0.12\n\n\n\n\n\n\n\n\n\n\nAs we can see, thanks in part to the performance matrix applied to the constructors, Suzuki was the most ‚Äúperformant‚Äù motorcycle in Qatar 2022, along with Honda and Aprilia. Ducati, Yamaha, and KTM had inferior performances."
  },
  {
    "objectID": "posts/motogp-performance/index.html#conclusion",
    "href": "posts/motogp-performance/index.html#conclusion",
    "title": "üèçÔ∏è Modelling Riders Performance",
    "section": "Conclusion",
    "text": "Conclusion\nFor the purpose of providing more technical and structured information to motorsports fans, we have demonstrated in this article how public data can be used to extract important insights on race and competitors performance. While the race results remain undoubtedly the reference for assigning race points, it is still crucial to be able to objectively measure and evaluate the pace of each rider, regardless of the outcome at the checkered flag and all the race events. By examining two MotoGP races in the 2022 season, we first established the importance of visually analyzing the race progression in terms of lap times in order to:\n\nExamine the overall evolution of tire performance and degradation;\nCompare the lap time evolution of riders, teams, and constructors (also in relation to the race average).\n\nWe then demonstrated how lap data can be used to analyze the adaptation of riders, teams, and constructors to different track segments, with the help of aggregate metrics for intermediate sector times.\nFinally, we demonstrated the usefulness of quantifying the performances of riders, teams, and constructors through quantitative indicators that take into account relative speed and consistency, the two fundamental variables in a theoretical definition of performance.\nIn conclusion, the research findings confirm the validity of the initial hypothesis and provide a satisfying answer to the questions that were posed at the beginning of the investigation. It is also believed that this study constitutes an important starting point for the creation of an engaging technical information format for motor racing. The ultimate goal is to promote a constructive and informed debate among industry experts and enthusiasts, providing objective information that allows for statistical evaluation of the performances of riders and vehicles. In this way, we could contribute to fostering a more aware and informed motor racing culture, capable of fully appreciating the technical complexity of this type of sport, especially for those more demanding fans."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio & Blog",
    "section": "",
    "text": "Welcome to my personal repository of data analytics notebooks.\nHere, you will find a collection of studies and analyses in which I make use of data visualization, statistical analysis, data mining, and machine learning to extract important insights from data and solve common case studies on various topics such as business management & marketing, social sciences, sports, and more.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nüë• Data-driven Customer Segmentation\n\n\nRFMT Clustering to Decode Customer Behaviors and Define Business Personas\n\n\n\n\nOnline Retail\n\n\nMarketing\n\n\nCustomer Segmentation\n\n\nBusiness Personas\n\n\nUnsupervised Learning\n\n\nPython\n\n\n\n\nIn-depth analysis of customer segmentation using the RFMT model (Recency, Frequency, Monetary Value, Tenure). Employing KMeans clustering, we identify distinct customer personas and propose tailored strategic actions for each group. The study aims to enhance customer engagement, retention, and value generation through precise, data-driven marketing strategies.\n\n\n\n\n\n\nJan 27, 2024\n\n\nAntonio Buzzelli\n\n\n\n\n\n\n  \n\n\n\n\nüôè Predicting loan approval\n\n\nClassification model selection and prediction\n\n\n\n\nFinance\n\n\nClassification\n\n\nSupervised Learning\n\n\nPython\n\n\n\n\nUsing a dataset containing demographic and financial attributes of loan applicants, I make use of a classification model to predict loan approval probability.\n\n\n\n\n\n\nOct 3, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\n  \n\n\n\n\nüèçÔ∏è Modelling Riders Performance\n\n\nLeveraging public race data to boost fans interest in motorsports\n\n\n\n\nmotogpdata\n\n\nMotorsports\n\n\nPublic Data\n\n\nPython\n\n\n\n\nDelving in the world of MotoGP racing, I illustrate how making use of public race data can provide fans with a more structured and comprehensive understanding of the objective performance of race competitors.\n\n\n\n\n\n\nMay 1, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\n  \n\n\n\n\nüõí Using Market Basket Analysis to penetrate an untapped regional market\n\n\nCustomer-centric bundle deals with Apriori and association rules\n\n\n\n\nOnline Retail\n\n\nMarketing\n\n\nMarket Basket Analysis\n\n\nPython\n\n\n\n\nThrough the analysis of one year of sales data from an e-commerce website, I was able to identify for the firm a promising regional market with untapped potential, and implement a marketing strategy to target it using bundle deals tailored to the specific preferences of its consumers.\n\n\n\n\n\n\nApr 2, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\n  \n\n\n\n\nüö¢ Understanding Survival on the Titanic\n\n\nUsing machine learning to explain phenomena and define business personas\n\n\n\n\nBusiness Personas\n\n\nMachine Learning\n\n\nPython\n\n\n\n\nBusinesses can utilize machine learning not only to make predictions but also to unveil valuable insights that are deeply embedded in their data. Using one of the most commonly used datasets in the study of data science ‚Äì the Titanic dataset ‚Äì, I demonstrate how supervised learning can serve as a data mining technique to achieve a deep understanding of phenomena and generate Personas.\n\n\n\n\n\n\nMar 15, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Antonio Buzzelli",
    "section": "",
    "text": "Combining several international experiences in project/business management and a state-of-the-art expertise in data science, my goal is to put at the service of the executive management of a well-established company my capacity to inform data-driven business decisions and strategies."
  }
]