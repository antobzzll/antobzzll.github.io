[
  {
    "objectID": "posts/online-retail-mba/index.html",
    "href": "posts/online-retail-mba/index.html",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "",
    "text": "The e-commerce industry has experienced significant growth in recent years, and online sales have become an increasingly important aspect of many businesses. Analyzing sales data can help businesses understand customer behavior and identify trends, which can then be used to improve their overall sales strategies and revenue. In this notebook, we will be analyzing a sales dataset from an e-commerce company to gain insights into their sales patterns and identify profitable opportunities.\nOur analysis will cover various aspects of the data, including temporal trends and customer geographical segmentation. We will also be performing a market basket analysis to identify relationships between products and suggest strategies for improving sales. By the end of this notebook, we aim to provide a comprehensive understanding of the sales data, which can then be used to make informed decisions and drive business growth."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#monthly-evolution",
    "href": "posts/online-retail-mba/index.html#monthly-evolution",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "Monthly evolution",
    "text": "Monthly evolution\n\nmonth_evo = df.groupby(pd.Grouper(key='Date', freq='M')).agg(\n    sold=('Amount','sum'), returned=('Amount', lambda x: sum(x[x &lt; 0])),\n    nunique=('TransactionNo', 'nunique'))\nmonth_evo['sold_moving_avg'] = month_evo['sold'].rolling(window=3).mean()\nmonth_evo['returned'] = month_evo['returned'].abs()\nmonth_evo.index = month_evo.index.date\nmonth_evo\n\n\n\n\n\n\n\n\nsold\nreturned\nnunique\nsold_moving_avg\n\n\n\n\n2018-12-31\n4234147.48\n181268.04\n1852\nNaN\n\n\n2019-01-31\n3649506.42\n910349.95\n1327\nNaN\n\n\n2019-02-28\n3299537.56\n35479.62\n1287\n3.727730e+06\n\n\n2019-03-31\n4353308.78\n45092.82\n1722\n3.767451e+06\n\n\n2019-04-30\n3416109.24\n173388.64\n1455\n3.689652e+06\n\n\n2019-05-31\n4530850.36\n48114.72\n1938\n4.100089e+06\n\n\n2019-06-30\n4410422.29\n84308.52\n1826\n4.119127e+06\n\n\n2019-07-31\n4518347.92\n75519.14\n1687\n4.486540e+06\n\n\n2019-08-31\n4614243.55\n144112.47\n1581\n4.514338e+06\n\n\n2019-09-30\n6542706.30\n85596.76\n2117\n5.225099e+06\n\n\n2019-10-31\n6971407.82\n266009.54\n2312\n6.042786e+06\n\n\n2019-11-30\n7745257.92\n115939.20\n3146\n7.086457e+06\n\n\n\n\n\n\n\n\nmonth_evo_sum = month_evo[['sold', 'returned']].sum(axis=1)\nmonth_evo_pct = month_evo[['sold', 'returned']].div(month_evo_sum, axis=0)\n\n\nfig, ax = plt.subplots(2, 1, figsize=(15,10))\nmonth_evo[['sold', 'returned']].plot.bar(ax=ax[0])\nax[0].set_ylabel('Revenue (GBP)')\nax[0].set_xlabel('Month')\nax[0].set_title(\"Monthly evolution of sales and returns\")\nax[0].grid(axis='y')\n\nmonth_evo_pct.plot.bar(stacked=True, ax=ax[1])\nax[1].set_ylabel('Percentage')\nax[1].set_xlabel('Month')\nax[1].set_title(\"Monthly relative amounts of sold and returned\")\nax[1].grid(axis='y')\n\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n\n\n\n\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax1.plot(month_evo.index, month_evo['sold'], label='Revenue')\nax1.plot(month_evo.index, month_evo['sold_moving_avg'], label='3-month revenue moving average')\nax2.bar(month_evo.index, month_evo['nunique'], width=8, label='Volume', alpha=0.25)\n\nax1.set_ylabel('Revenue (GBP)')\nax2.set_ylabel('Volume')\nax1.set_xlabel('Month')\nplt.title(\"Monthly evolution of sales\")\nplt.grid(True)\nax1.legend(loc=(0.025,0.85))\nax2.legend(loc=(0.3,0.85))\n\nplt.show()\n\n\n\n\n\nAn increased volume of sales and revenue is clearly visible during the last months of the year, from September to December."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#intra-month-analysis",
    "href": "posts/online-retail-mba/index.html#intra-month-analysis",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "Intra-month analysis",
    "text": "Intra-month analysis\n\ndf = df[df['Quantity'] &gt; 0]\n\nbydate = df.groupby('Date').agg(\n    UniqueTransactions=('TransactionNo', 'nunique'),\n    UniqueProdSold=('TransactionNo', 'count'),\n    ProdSold=('Quantity', 'sum'),\n    Revenue=('Amount', 'sum')\n    ).reset_index()\nbydate['Day'] = bydate['Date'].dt.day\nbydate['Weekday'] = bydate['Date'].dt.weekday\nbydate['Month'] = bydate['Date'].dt.month\n\nbydate['WeekdayName'] = bydate['Date'].dt.day_name()\nbydate\n\n\n\n\n\n\n\n\nDate\nUniqueTransactions\nUniqueProdSold\nProdSold\nRevenue\nDay\nWeekday\nMonth\nWeekdayName\n\n\n\n\n0\n2018-12-01\n127\n3061\n26889\n326820.08\n1\n5\n12\nSaturday\n\n\n1\n2018-12-02\n141\n2057\n31297\n367316.62\n2\n6\n12\nSunday\n\n\n2\n2018-12-03\n68\n2136\n16164\n206313.62\n3\n0\n12\nMonday\n\n\n3\n2018-12-05\n88\n2694\n16357\n197565.27\n5\n2\n12\nWednesday\n\n\n4\n2018-12-06\n103\n3823\n21867\n273420.10\n6\n3\n12\nThursday\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n292\n2019-11-25\n84\n3073\n31504\n197883.43\n25\n0\n11\nMonday\n\n\n293\n2019-11-27\n57\n2529\n11151\n71166.89\n27\n2\n11\nWednesday\n\n\n294\n2019-11-28\n114\n3293\n29440\n190534.34\n28\n3\n11\nThursday\n\n\n295\n2019-11-29\n135\n4275\n30872\n200962.48\n29\n4\n11\nFriday\n\n\n296\n2019-11-30\n108\n3337\n28369\n182968.22\n30\n5\n11\nSaturday\n\n\n\n\n297 rows × 9 columns\n\n\n\n\nbyday = bydate.groupby('Day')[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()\nbyday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']\nbyday = byday.sort_index()\nbyday.head()\n\n\n\n\n\n\n\n\nDailyAvgUniqueTransactions\nDailyAvgUniqueProdSold\nDailyAvgProdSold\nDailyAvgRev\n\n\nDay\n\n\n\n\n\n\n\n\n1\n62.200\n1425.600000\n15170.000000\n176053.439000\n\n\n2\n66.875\n1685.625000\n16904.750000\n197402.811250\n\n\n3\n54.700\n1410.100000\n15707.800000\n188742.753000\n\n\n4\n61.400\n1713.100000\n19456.500000\n226897.317000\n\n\n5\n71.000\n1815.111111\n20756.666667\n238868.772222\n\n\n\n\n\n\n\n\nrev_coefficients = np.polyfit(byday.index.values, byday['DailyAvgRev'].values, 5)\nrev_regression_line = np.poly1d(rev_coefficients)\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax2.plot(byday.index, byday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)\nax1.bar(byday.index, byday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)\nax2.plot(rev_regression_line(byday.index.values), label='Regression line')\nax2.axhline(byday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Monthly average')\n\nax1.set_ylabel('N. transactions')\nax2.set_ylabel('Revenue (GBP)')\nplt.title(\"Intra-month sales analysis\")\nplt.grid(True)\nax1.legend(loc='upper left')\nax1.set_xlabel('Day')\nax2.legend()\n\nplt.show()\n\n\n\n\n\nBy analyzing the revenue data within a month, we can observe that the daily average revenue varies throughout the month. The revenue reaches its peak at around three-quarters of the month and dips to its lowest point just before the end of the month. However, it starts to increase again just before the last few days. The dip in revenue just before the end of the month is considered normal as it coincides with the time when people typically receive their salaries."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#intra-week-analysis",
    "href": "posts/online-retail-mba/index.html#intra-week-analysis",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "Intra-week analysis",
    "text": "Intra-week analysis\n\nbyweekday = bydate.groupby(['Weekday', 'WeekdayName'])[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()\nbyweekday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']\nbyweekday = byweekday.reset_index().set_index('Weekday')\nbyweekday.index = byweekday.index + 1\nbyweekday\n\n\n\n\n\n\n\n\nWeekdayName\nDailyAvgUniqueTransactions\nDailyAvgUniqueProdSold\nDailyAvgProdSold\nDailyAvgRev\n\n\nWeekday\n\n\n\n\n\n\n\n\n\n1\nMonday\n61.416667\n1570.854167\n16290.958333\n187875.265208\n\n\n3\nWednesday\n43.612245\n1259.551020\n9342.183673\n107400.393061\n\n\n4\nThursday\n64.217391\n1900.000000\n17979.391304\n208483.595435\n\n\n5\nFriday\n66.705882\n1879.764706\n21235.274510\n245216.874706\n\n\n6\nSaturday\n68.500000\n1721.365385\n18900.384615\n210759.228077\n\n\n7\nSunday\n77.843137\n1820.686275\n22464.431373\n257149.160980\n\n\n\n\n\n\n\n\nrev_coefficients = np.polyfit(byweekday.index.values, byweekday['DailyAvgRev'].values, 2)\nrev_regression_line = np.poly1d(rev_coefficients)\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax2 = plt.twinx()\nax2.plot(byweekday['WeekdayName'], byweekday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)\nax1.bar(byweekday['WeekdayName'], byweekday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)\nax2.plot(rev_regression_line(byweekday.index.values), label='Regression line')\nax2.axhline(byweekday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Weekly average')\n\nax1.set_ylabel('N. transactions')\nax2.set_ylabel('Revenue(GBP)')\nplt.title(\"Intra-week sales analysis\")\nplt.grid(axis='y')\nax1.legend(loc='lower left')\nax1.set_xlabel('Weekday')\nax2.legend()\n\nplt.show()\n\n\n\n\nSimilar to the analysis conducted within a month, examining sales patterns within a week can also reveal interesting insights.\n\nBy looking at the graph above, it becomes evident that the sales volume and revenue significantly increase during the latter part of the week. Specifically, revenue exceeds the weekly average starting from Thursday. On the other hand, Wednesday remains the least profitable day of the week with the lowest sales volume and revenue."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#geographical-analysis",
    "href": "posts/online-retail-mba/index.html#geographical-analysis",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "Geographical analysis",
    "text": "Geographical analysis\nWhen conducting a geographical analysis of sales, it is essential to consider both the average purchase value and sales volume to determine if there are any countries that offer promising opportunities. For instance, a country with a high average purchase value but low sales volume may indicate that it has untapped potential and should be targeted for further penetration. The average purchase value gives an indication of the buying power and willingness of customers to spend money, while sales volume reflects the market demand and potential for growth. A country with a high average purchase value and low sales volume could be a potential opportunity for businesses to capitalize on the untapped market potential by increasing their presence and promoting their products or services more effectively.\n\n# Mapping regions\nregions = {'Europe': ['Sweden', 'Denmark', 'Norway', 'Finland', 'Iceland', 'Netherlands', 'Belgium', 'France', 'Germany', 'Switzerland', 'Austria',\n                      'Italy', 'Spain', 'Greece', 'Portugal', 'Malta', 'Cyprus', 'Czech Republic', 'Lithuania', 'Poland', 'United Kingdom', 'EIRE',\n                      'Channel Islands', 'European Community'],\n           'North America': ['USA', 'Canada'],\n           'Middle East': ['Bahrain', 'United Arab Emirates', 'Israel', 'Lebanon', 'Saudi Arabia'],\n           'Asia Pacific': ['Japan', 'Australia', 'Singapore', 'Hong Kong'],\n           'RoW': ['Brazil', 'RSA'],\n           'Unspecified': ['Unspecified']}\n\ncountry_to_region = {}\nfor region, countries in regions.items():\n    for country in countries:\n        country_to_region[country] = region\n\ndf['Region'] = df['Country'].map(country_to_region)\n\ndf['UKvsRoW'] = np.where(df['Country'] == 'United Kingdom', 'UK', 'RoW')\n\n\nbycountry = df.groupby('Country').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean')\n).sort_values('tot_amount', ascending=False)\nbycountry.head()\n\n\n\n\n\n\n\n\ntot_amount\nmean_amount\n\n\nCountry\n\n\n\n\n\n\nUnited Kingdom\n50192562.28\n110.502027\n\n\nNetherlands\n2101104.07\n937.992888\n\n\nEIRE\n1687318.68\n225.607525\n\n\nGermany\n1346540.40\n135.494103\n\n\nFrance\n1306661.68\n129.564867\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, figsize=(15,10))\nax[0].bar(bycountry.index, bycountry['tot_amount'])\nax[1].bar(bycountry.sort_values('mean_amount', ascending=False).index, bycountry.sort_values('mean_amount', ascending=False)['mean_amount'])\nplt.setp(ax, xticks=bycountry.index, xticklabels=bycountry.index)\nplt.setp(ax[0].get_xticklabels(), rotation=90, ha=\"center\")\nplt.setp(ax[1].get_xticklabels(), rotation=90, ha=\"center\")\n\nax[0].set_ylabel(\"Amount (GBP)\")\nax[1].set_ylabel(\"Amount (GBP)\")\nax[0].set_title(\"Countries by total amount sold\")\nax[1].set_title(\"Countries by average amount sold\")\nplt.suptitle(\"Overview on geographical market spread\")\nax[0].grid(axis='y')\nax[1].grid(axis='y')\nplt.subplots_adjust(hspace=0.7)\n\nplt.show()\n\n\n\n\n\nbyukvsrow = df.groupby('UKvsRoW').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\nbyukvsrow\n\n\n\n\n\n\n\n\ntot_amount\nmean_amount\nn_inv\nquantity\n\n\nUKvsRoW\n\n\n\n\n\n\n\n\nRoW\n10258462.78\n211.383944\n1809\n18.478900\n\n\nUK\n50192562.28\n110.502027\n17164\n9.646163\n\n\n\n\n\n\n\n\nplt.pie(byukvsrow['tot_amount'], labels=byukvsrow.index, autopct='%1.1f%%', explode=(0.1,0), shadow=True)\nplt.title('Total revenue by UK vs other countries')\nplt.show()\n\n\n\n\n\nrow_rev = df.loc[df['UKvsRoW'] == 'RoW', 'Amount']\nuk_rev = df.loc[df['UKvsRoW'] == 'UK', 'Amount']\n\nttest_ind(uk_rev, row_rev)\n\nTtest_indResult(statistic=-16.7279858606087, pvalue=8.524026769071223e-63)\n\n\n\nEven though the volume of sales of international customers accounts only for the 17.0%, the average revenue generated abroad is significantly higher than the one generated in the UK. This means that international markets for this business are potentially more lucrative than the national one and need to be exploited more.\n\n\nbyregion = df.groupby('Region').agg(\n    tot_amount=('Amount', 'sum'),\n    mean_amount=('Amount', 'mean'),\n    n_inv=('TransactionNo', 'nunique'),\n    quantity=('Quantity', 'mean')\n).sort_values('mean_amount', ascending=False)\nbyregion.sort_values('mean_amount', ascending=False)\n\n\n\n\n\n\n\n\ntot_amount\nmean_amount\nn_inv\nquantity\n\n\nRegion\n\n\n\n\n\n\n\n\nAsia Pacific\n1380079.80\n590.787586\n92\n51.083904\n\n\nNorth America\n59633.28\n154.891636\n11\n13.503896\n\n\nMiddle East\n76798.30\n154.835282\n16\n12.661290\n\n\nEurope\n58892902.53\n118.021612\n18839\n10.308538\n\n\nRoW\n8912.10\n101.273864\n2\n8.000000\n\n\nUnspecified\n32699.05\n73.152237\n13\n6.272931\n\n\n\n\n\n\n\n\nfig, ax1 = plt.subplots(figsize=(15,5))\nax1 = plt.bar(byregion.index, byregion['mean_amount'])\nplt.title(\"Average purchase value by region\")\nplt.ylabel('Amount (GBP)')\nplt.xlabel('Region')\nplt.grid(axis='y')\nplt.show()\n\n\n\n\n\nf_value, p_value = f_oneway(\n    df.loc[df['Region'] == 'Asia Pacific', 'Amount'],\n    df.loc[df['Region'] == 'North America', 'Amount'],\n    df.loc[df['Region'] == 'Middle East', 'Amount'],\n    df.loc[df['Region'] == 'Europe', 'Amount'],\n    df.loc[df['Region'] == 'RoW', 'Amount'])\nprint(f'ANOVA F-value: {f_value:.2f}')\nprint(f'ANOVA p-value: {p_value:.4f}')\ntukey_df = df.filter(items=['Amount', 'Region']).dropna()\nprint(pairwise_tukeyhsd(tukey_df['Amount'], tukey_df['Region']))\n\nANOVA F-value: 81.58\nANOVA p-value: 0.0000\n          Multiple Comparison of Means - Tukey HSD, FWER=0.05          \n=======================================================================\n    group1        group2     meandiff p-adj    lower     upper   reject\n-----------------------------------------------------------------------\n Asia Pacific        Europe  -472.766    0.0 -547.3921 -398.1398   True\n Asia Pacific   Middle East -435.9523    0.0  -613.855 -258.0496   True\n Asia Pacific North America -435.8959    0.0 -633.8256 -237.9663   True\n Asia Pacific           RoW -489.5137 0.0048 -880.2663  -98.7612   True\n Asia Pacific   Unspecified -517.6353    0.0 -703.4071 -331.8636   True\n       Europe   Middle East   36.8137 0.9872  -124.841  198.4683  False\n       Europe North America     36.87 0.9928 -146.5938  220.3338  False\n       Europe           RoW  -16.7477    1.0 -400.3757  366.8802  False\n       Europe   Unspecified  -44.8694 0.9754 -215.1456  125.4068  False\n  Middle East North America    0.0564    1.0 -244.3599  244.4726  False\n  Middle East           RoW  -53.5614 0.9991 -469.7954  362.6726  False\n  Middle East   Unspecified   -81.683 0.9207 -316.3622  152.9961  False\nNorth America           RoW  -53.6178 0.9992 -478.7971  371.5616  False\nNorth America   Unspecified  -81.7394 0.9386 -331.9414  168.4626  False\n          RoW   Unspecified  -28.1216    1.0 -447.7792   391.536  False\n-----------------------------------------------------------------------\n\n\n\nWe can observe from both the bar plot and the ANOVA analysis that the mean purchase value in the Asia/Pacific region is consistently and significantly higher than the mean purchase value in the other regions. Based on this important information, we can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales. The business can consider implementing targeted marketing strategies, such as advertising campaigns and promotions, that cater to the preferences and interests of the Asia/Pacific market. Additionally, it can explore expanding its product offerings to meet the specific demands of this region, or enhancing the quality of existing products to meet their higher standards. It may be useful to conduct further research and analysis to gain deeper insights into the preferences and behavior of customers in the Asia/Pacific region, and tailor sales strategies accordingly."
  },
  {
    "objectID": "posts/online-retail-mba/index.html#bundle-offers",
    "href": "posts/online-retail-mba/index.html#bundle-offers",
    "title": "🛒 Sales & Market Basket Analysis",
    "section": "Bundle offers",
    "text": "Bundle offers\nBased on the observation that these items are frequently bought together, it could be advantageous to offer them as bundles to customers. The firm could offer convenience and value to customers while potentially increasing sales and revenue. For example, a bundle might include both the Blue Polkadot Bowl and the Pink Polkadot Bowl, or the Dolly Girl Lunch Box and the Spaceboy Lunch Box. This strategy can be an effective way to meet Asian customers needs while boosting profits for the retailer.\n\n# Since we want to create bundle offers for single products, we filter for single items\nrules = rules[(rules['n_antecedents'] == 1) & (rules['n_consequents'] == 1)]\nrules.sort_values('support', ascending=False)\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nn_antecedents\nn_consequents\n\n\n\n\n6\n(Dolly Girl Lunch Box)\n(Spaceboy Lunch Box)\n0.097826\n0.108696\n0.097826\n1.0\n9.200000\n0.087193\ninf\n1\n1\n\n\n0\n(Alarm Clock Bakelike Red)\n(Alarm Clock Bakelike Green)\n0.065217\n0.065217\n0.065217\n1.0\n15.333333\n0.060964\ninf\n1\n1\n\n\n1\n(Alarm Clock Bakelike Green)\n(Alarm Clock Bakelike Red)\n0.065217\n0.065217\n0.065217\n1.0\n15.333333\n0.060964\ninf\n1\n1\n\n\n2\n(Basket Of Toadstools)\n(Set 3 Retrospot Tea/Coffee/Sugar)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n3\n(Blue Happy Birthday Bunting)\n(Pink Happy Birthday Bunting)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n4\n(Pink Polkadot Bowl)\n(Blue Polkadot Bowl)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n5\n(Blue Polkadot Bowl)\n(Pink Polkadot Bowl)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n7\n(Fairy Tale Cottage Night Light)\n(Red Toadstool Led Night Light)\n0.054348\n0.119565\n0.054348\n1.0\n8.363636\n0.047850\ninf\n1\n1\n\n\n8\n(Feltcraft Princess Lola Doll)\n(Feltcraft Princess Olivia Doll)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n9\n(Green Regency Teacup And Saucer)\n(Roses Regency Teacup And Saucer)\n0.054348\n0.097826\n0.054348\n1.0\n10.222222\n0.049031\ninf\n1\n1\n\n\n10\n(Set Of 4 Knick Knack Tins Leaf)\n(Set Of 4 Knick Knack Tins Doily)\n0.054348\n0.076087\n0.054348\n1.0\n13.142857\n0.050213\ninf\n1\n1\n\n\n11\n(Set Of 6 Tea Time Baking Cases)\n(Set Of 6 Snack Loaf Baking Cases)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n12\n(Set Of 6 Snack Loaf Baking Cases)\n(Set Of 6 Tea Time Baking Cases)\n0.054348\n0.054348\n0.054348\n1.0\n18.400000\n0.051394\ninf\n1\n1\n\n\n\n\n\n\n\n\nrules['antecedent'] = rules['antecedents'].apply(lambda x: list(x)[0])\nrules['consequent'] = rules['consequents'].apply(lambda x: list(x)[0])\nrules['rule'] = rules.index\n\ncoords = rules[['antecedent', 'consequent', 'rule']]\n\nparallel_coordinates(coords, 'rule', colormap='ocean')\nplt.title('Bundle offers for Asian / Pacific market')\nplt.show()\n\n\n\n\n\nThe parallel coordinates plot visually highlights the bundles that were put together for the Asian market, and that the firm should offer on their e-commerce.\n\nOffering bundles of products that are already sold together as frequent itemsets can be an effective marketing strategy for several reasons:\n\nConvenience: Bundling products that are frequently purchased together can provide customers with a convenient and streamlined shopping experience. Instead of having to search for each product individually, customers can purchase them together in a single transaction.\nValue proposition: Bundling products can create a compelling value proposition for customers. By offering a discount or special deal on a bundle of products, customers may be more likely to make a purchase than if they were buying each item individually.\nIncreased sales: Bundling can also lead to increased sales by encouraging customers to purchase additional products that they may not have otherwise considered. For example, a customer who only intended to buy coffee may be enticed to buy a bundle that includes coffee, a mug, and a bag of coffee beans.\nUpselling opportunities: Bundling can also provide opportunities for upselling by encouraging customers to purchase a higher-value bundle that includes additional products or features.\n\nIn summary, while some products may already be sold together as frequent itemsets, bundling can provide additional value and convenience for customers, as well as opportunities for increased sales and upselling. By offering bundles, businesses can differentiate themselves from competitors and create a more compelling value proposition for their customers."
  },
  {
    "objectID": "posts/titanic/index.html",
    "href": "posts/titanic/index.html",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "",
    "text": "The Titanic disaster remains one of the most infamous shipwrecks in history, with over 1500 lives lost. In this notebook, we will explore and analyze a dataset containing information on passengers who were aboard the ship and build a machine learning model to predict which passengers are likely to have survived the tragedy.\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nimport shap"
  },
  {
    "objectID": "posts/titanic/index.html#data",
    "href": "posts/titanic/index.html#data",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "Data",
    "text": "Data\nAfter having installed and imported the necessary libraries, we begin our work by accessing the dataset containing the passengers’ data.\n\ndf = pd.read_csv('titanic.csv')\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\nThe variables included in the dataset are: * PassengerId: An integer value representing the unique ID of each passenger on board. * Survived: A binary variable indicating whether the passenger survived the sinking of the Titanic or not, with “True” indicating survival and “False” indicating death. * Pclass: An integer variable representing the class of the passenger’s ticket, with values 1, 2, or 3 (1 being the highest class). * Name: A string variable representing the name of the passenger. * Sex: A string variable representing the gender of the passenger. * Age: A float variable representing the age of the passenger in years. * SibSp: An integer variable representing the number of siblings or spouses the passenger had on board. * Parch: An integer variable representing the number of parents or children the passenger had on board. * Ticket: A string variable representing the ticket number of the passenger. * Fare: A float variable representing the fare paid by the passenger for their ticket. * Cabin: A string variable representing the cabin number of the passenger, if applicable. * Embarked: A string variable representing the port of embarkation for the passenger, with possible values “S” (Southampton), “C” (Cherbourg), or “Q” (Queenstown)."
  },
  {
    "objectID": "posts/titanic/index.html#transformations-feature-engineering-preprocessing",
    "href": "posts/titanic/index.html#transformations-feature-engineering-preprocessing",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "Transformations, feature engineering, preprocessing",
    "text": "Transformations, feature engineering, preprocessing\nIn this first section we are going to explore and process the given dataset and prepare it for modeling. Let’s first take a general look at the variables from the structural point-of-view.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nPassengerId column is a useless variable in terms of prediction capabilities. We can set it as dataframe index (or drop it) to keep it out of the scope.\n\ndf = df.set_index('PassengerId')\n# df = df.drop(columns=['PassengerId'])\n\nTo give more weight to first class (and vice-versa) Pclass values must be inverted.\n\nconditions = [df['Pclass'] == 1, df['Pclass'] == 3]\nchoices = [3, 1]\ndf['Pclass'] = np.select(conditions, choices, 2)\n\nLet’s assess now the presence of null values. Percentage of null values per variable:\n\ndisplay((df.isnull().sum() / df.shape[0]).round(3).sort_values(ascending=False))\n\nCabin       0.771\nAge         0.199\nEmbarked    0.002\nSurvived    0.000\nPclass      0.000\nName        0.000\nSex         0.000\nSibSp       0.000\nParch       0.000\nTicket      0.000\nFare        0.000\ndtype: float64\n\n\nA part from a negligeable presence of null values in the Embarked variable, we can see that important columns such as Cabin and Age are full of empty observations. Before proceeding with the model development, we’ll have to deal with these inconsistencies.\n\ndf['Cabin'].unique()\n\narray([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n       'C148'], dtype=object)\n\n\n\ndisplay(df['Cabin'].value_counts().head(30))\n\nCabin\nB96 B98            4\nG6                 4\nC23 C25 C27        4\nC22 C26            3\nF33                3\nF2                 3\nE101               3\nD                  3\nC78                2\nC93                2\nE8                 2\nD36                2\nB77                2\nC123               2\nE121               2\nE44                2\nD35                2\nC125               2\nE67                2\nB35                2\nB18                2\nE24                2\nB49                2\nC65                2\nB20                2\nB5                 2\nB57 B59 B63 B66    2\nC126               2\nB51 B53 B55        2\nF4                 2\nName: count, dtype: int64\n\n\nThe majority of the cabins in the dataset seem to follow a pattern of a letter followed by a two or three digit number, suggesting that the letter represents the section or deck where the cabin is situated and the number represents the room number. It is reasonable to assume that knowing the section where a passenger’s cabin is located would provide valuable information about their likelihood of survival. Therefore, it would be beneficial to clean up the column containing cabin information to extract useful insights.\n\ntrans_df = df.copy()\n\n\n# Splitting Cabin variable into Deck and Room variables\ntrans_df['CabinData'] = trans_df['Cabin'].isnull().apply(lambda x: not x)\ntrans_df['Deck'] = trans_df['Cabin'].str.slice(0,1)\ntrans_df['Room'] = trans_df['Cabin'].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\n\n\ntrans_df[['Survived', 'CabinData']] = trans_df[['Survived', 'CabinData']].astype(int)\n\n\n# Label-encoding target variable\ntrans_df['Female'] = np.where(trans_df['Sex'] == 'female', 1, 0)\n\n# Drop useless variables for modeling\ntrans_df = trans_df.drop(columns=['Name', 'Ticket', 'Cabin', 'Sex', 'Room'])\n\n\n# Create one-hot encoding of the categorical variable\ndf_encoded = pd.get_dummies(trans_df, columns=['Deck', 'Embarked'])\n\n# Impute missing values with KNN imputer\nimputer = KNNImputer(n_neighbors=8)\nimputed_df = imputer.fit_transform(df_encoded)\nimputed_df = pd.DataFrame(imputed_df, columns=df_encoded.columns)\n\n\nimputed_df\n\n\n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nCabinData\nFemale\nDeck_A\nDeck_B\nDeck_C\nDeck_D\nDeck_E\nDeck_F\nDeck_G\nDeck_T\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n0.0\n1.0\n22.00\n1.0\n0.0\n7.2500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n3.0\n38.00\n1.0\n0.0\n71.2833\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n2\n1.0\n1.0\n26.00\n0.0\n0.0\n7.9250\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n1.0\n3.0\n35.00\n1.0\n0.0\n53.1000\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n0.0\n1.0\n35.00\n0.0\n0.0\n8.0500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0.0\n2.0\n27.00\n0.0\n0.0\n13.0000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n887\n1.0\n3.0\n19.00\n0.0\n0.0\n30.0000\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n888\n0.0\n1.0\n23.75\n1.0\n2.0\n23.4500\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n889\n1.0\n3.0\n26.00\n0.0\n0.0\n30.0000\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n890\n0.0\n1.0\n32.00\n0.0\n0.0\n7.7500\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n891 rows × 19 columns"
  },
  {
    "objectID": "posts/titanic/index.html#exploratory-analysis",
    "href": "posts/titanic/index.html#exploratory-analysis",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nWe have transformed our dataframe by imputing missing values with a KNNImputer algorythm and one-hot encoded categorical variables. Let’s now perform an exploratory analysis to look for potential correlations between our target variable and the others.\n\nax = sns.countplot(x=\"Pclass\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\nplt.title(\"Perished / Survived per `Pclass`\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n42% of perished people is coming from the third class.\n\nax = sns.countplot(x=\"Female\", data=imputed_df, hue='Survived', palette=['red', 'green'])\nax.set(xlabel='')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percentage = '{:.1f}%'.format(100 * count / len(imputed_df))\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_y() + p.get_height()\n    ax.annotate(f'{count}\\n{percentage}', (x, y), ha='center', va='center')\n    \nplt.xticks([0, 1], ['Men', 'Women'])\nplt.title(\"Perished / Survived per sex\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\nThe majority of perished people are men, while the majority of survived are women.\n\ncatplot = sns.catplot(data=imputed_df, y=\"Age\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Age` and `Pclass`', y=1.05)\nplt.show()\n\ncatplot = sns.catplot(data=imputed_df, y=\"Fare\", x=\"Pclass\", hue='Survived', col='Female', alpha=0.5, palette=['red', 'green'])\ncatplot.set(xlabel='')\nplt.xticks([0, 1, 2], ['Third class', 'Second class', 'First class'])\ncatplot.fig.suptitle('Distribution of \"Survival\" per `Fare` and `Pclass`', y=1.05)\nplt.show()\n\n\n\n\n\n\n\nAlready from these exploratory visualizations, we can easily see that the majority of survived people are women and people from the top classes. Will this be confirmed by the model?\n\nsurvived_corr = imputed_df.corr()['Survived'].sort_values(ascending=False)[1:]\ndisplay(survived_corr)\n\nFemale        0.543351\nPclass        0.338481\nCabinData     0.316912\nFare          0.257307\nDeck_B        0.175095\nEmbarked_C    0.168240\nDeck_D        0.150716\nDeck_E        0.145321\nDeck_C        0.114652\nParch         0.081629\nDeck_F        0.057935\nDeck_A        0.022287\nDeck_G        0.016040\nEmbarked_Q    0.003650\nDeck_T       -0.026456\nSibSp        -0.035322\nAge          -0.092717\nEmbarked_S   -0.155660\nName: Survived, dtype: float64\n\n\n\nfor i in range(0, len(survived_corr.index), 6):\n    chunk = survived_corr.index[i:i+6]\n    sns.pairplot(imputed_df, y_vars=['Survived'], x_vars=chunk, kind='reg')"
  },
  {
    "objectID": "posts/titanic/index.html#model-development",
    "href": "posts/titanic/index.html#model-development",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "Model development",
    "text": "Model development\nWe begin by splitting the dataset into train and test sets and instantiate a KFold object for cross validation to avoid overfitting. Several trial and errors suggested to set the test size to 0.4, giving the best balance between cross validation score and test set accuracy score for the models.\n\n# Split the data into training and test sets\nmodel_df = imputed_df.drop(columns=['CabinData'])\nX = model_df.drop(columns=['Survived'])\ny = model_df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n\n# instantiate a KFold object for cross validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\nModel selection\nThen we compare three classifiers and choose the best one in terms of accuracy.\n\n# Define sklearn pipelines for classifier comparison\n\nlogistic_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('logistic', LogisticRegression(max_iter=1000))\n])\n\nrf_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('rf', RandomForestClassifier(random_state=42))\n])\n\nsvm_pipeline = Pipeline([\n#     ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\npipelines = [logistic_pipeline, rf_pipeline, svm_pipeline]\nfor pipeline in pipelines:\n    scores = cross_val_score(pipeline, X_train, y_train, cv=kf)\n    print(f\"Pipeline: {pipeline.named_steps.keys()} Mean Cross-Validation Score: {scores.mean().round(3)}\")\n    \n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    print(f\"Test accuracy score: {round(accuracy_score(y_pred, y_test), 3)}\")\n    \n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    cmd = ConfusionMatrixDisplay(cm)\n    fig, ax = plt.subplots()\n    cmd.plot(ax=ax, cmap=plt.cm.Blues)\n    ax.set_title(f\"Confusion Matrix\\n{pipeline[0]}\")\n    plt.show()\n\nPipeline: dict_keys(['logistic']) Mean Cross-Validation Score: 0.792\nTest accuracy score: 0.821\nPipeline: dict_keys(['rf']) Mean Cross-Validation Score: 0.8\nTest accuracy score: 0.798\nPipeline: dict_keys(['svm']) Mean Cross-Validation Score: 0.695\nTest accuracy score: 0.65\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, the classification model with the highest accuracy score (mean cross-validation score) is the Random Forest Classifier. We are going to use this specific model to predict survival on the Titanic.\n\n\nPrediction with Random Forest Classifier\nAfter having chosen the right classifier for the job, we train it on the train set with GridSearchCV, that performs an exhaustive search over a specified hyperparameter space for an estimator. It takes as input an estimator, a dictionary of hyperparameters, and a cross-validation strategy. It then fits the estimator on all possible combinations of hyperparameters and evaluates the performance of each combination using cross-validation.\nThe GridSearchCV class is useful for finding the best hyperparameters for a machine learning model. By testing all possible combinations of hyperparameters, it can help to identify the combination that results in the best performance on a given dataset.\n\nparam_grid = {\n    'rf__n_estimators': [10, 50, 100],\n    'rf__max_depth': [None, 5, 10],\n    'rf__min_samples_split': [2, 5, 10],\n    'rf__min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=rf_pipeline, param_grid=param_grid, cv=kf)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\ny_pred = grid_search.predict(X_test)\nprint(\"Test set accuracy:\", grid_search.score(X_test, y_test))\n\nestimator = grid_search.best_estimator_.named_steps['rf']\n\n # Calculate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ncmd = ConfusionMatrixDisplay(cm)\nfig, ax = plt.subplots()\ncmd.plot(ax=ax, cmap=plt.cm.Blues)\nax.set_title(f\"Confusion Matrix\\n{estimator}\")\nplt.show()\n\nBest parameters: {'rf__max_depth': None, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 2, 'rf__n_estimators': 50}\nBest score: 0.8294657026979368\nTest set accuracy: 0.8207282913165266\n\n\n\n\n\nWe have taken the best performing algorithm and trained it on our dataset with a GridSearchCV and KFold cross validation. The perfomance on the test set suggests that the chosen classification model is able to predict Survival with an 82% of accuracy. As a final step, we are going to explain the variables which have been taken into consideration by the classifier in the prediction, and draw conclusions on the “Survived Persona”.\n\nshap_values = shap.TreeExplainer(estimator).shap_values(X_test)\nshap.summary_plot(shap_values[1], X_test)\n\nNo data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n\n\n\n\n\nThe SHAP (SHapley Additive exPlanations) summary plot is a powerful tool for interpreting the output of a tree-based model, such as a Random Forest Classifier, using a TreeExplainer. The plot displays the features in order of their importance, as measured by their mean absolute SHAP values. Here’s how to interpret the SHAP summary plot for a TreeExplainer:\n\nFeature importance: The features are listed along the y-axis of the plot, with the most important features at the top. The importance of each feature is represented by its mean absolute SHAP value, which is indicated by the horizontal bars.\nImpact on prediction: The color of the bars represents the direction and magnitude of the feature’s impact on the model’s prediction. Red bars indicate a positive impact on the prediction, while blue bars indicate a negative impact. The length of the bar represents the magnitude of the impact, with longer bars indicating a greater impact.\nRelationship between feature and prediction: The position of each bar relative to the center line indicates the direction and strength of the relationship between the feature and the prediction. Features that push the prediction towards the top of the plot are associated with higher predicted probabilities, while features that push the prediction towards the bottom are associated with lower predicted probabilities.\nInteractions between features: The spacing between the bars for each feature represents the degree of interaction between that feature and other features in the model. If the bars for two features are close together, it indicates that those features tend to interact with each other in the model."
  },
  {
    "objectID": "posts/titanic/index.html#conclusion",
    "href": "posts/titanic/index.html#conclusion",
    "title": "🚢 Understanding Survival on the Titanic",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest Classifier trained on our dataset was capable of predicting Survival on the Titanic with a 82% of accuracy. The best predictors identified by the SHAP Tree Explainer are: * Female: whether the person was female or not. The red dots (representing female persons) are all gathered on the right side of the plot, meaning that they have a strong positive impact on survival. * Pclass: passenger class. 1st class passengers (red dots) are located in the right size of the graph. Being a 1st class passenger would have meant sure survival. * Age: older persons, represented by red dots are all gathered in the left (death) side of the plot, inversely, younger people were more prone to be saved. * Fare: similar situation for fare. Higher fares (red dots) are all located in the right side, meaning that people who paid for more expensive tickets had better chances to survive.\nAn interesting insights that SHAP gives us is regarding the Embarked_S. This dummy variable tells us if the passenger was embarked in Southampton. We can see that these passengers (red dots) had practically no chance of being saved. Maybe because they were all 3rd class?\nTo conclude, we can say that the Survived Persona was a young girl, coming from the top classes (having paid a higher fare) and not embarked in Southampton."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytics Portfolio & Blog",
    "section": "",
    "text": "Welcome to my repository of analytics and data science notebooks.\nHere, you will find a collection of studies and analyses in which I make use of data visualization, statistical analysis, data mining, and machine learning to extract important insights from data and solve common case studies on various topics such as business, social sciences, sports, and much more.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n🛒 Sales & Market Basket Analysis\n\n\nBundled deals for an untapped market\n\n\n\n\nOnline Retail\n\n\nMarket Basket Analysis\n\n\n\n\nThrough the analysis of one year of sales data from an e-commerce website, I was able to identify for the firm a promising market with untapped potential, and implement a marketing strategy to target it using bundle deals tailored to the specific preferences of its consumers.\n\n\n\n\n\n\nApr 2, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\n  \n\n\n\n\n🚢 Understanding Survival on the Titanic\n\n\nUsing machine learning to explain phenomena and develop business personas\n\n\n\n\nBusiness Personas\n\n\nMachine Learning\n\n\n\n\nBusinesses can utilize machine learning not only to make predictions but also to unveil valuable insights that are deeply embedded in their data. Using one of the most commonly used datasets in the study of data science – the Titanic dataset –, I demonstrate how supervised learning can serve as a data mining technique to achieve a deep understanding of phenomena and generate Personas.\n\n\n\n\n\n\nMar 15, 2023\n\n\nAntonio Buzzelli\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Antonio Buzzelli",
    "section": "",
    "text": "Combining several international experiences in project/business management and a state-of-the-art expertise in data science, my goal is to put at the service of the executive management of a well-established company my capacity to inform data-driven business decisions and strategies."
  }
]